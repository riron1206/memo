<html>
<head>
  <title>Evernote Export</title>
  <basefont face="メイリオ" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="Evernote Windows/309091 (ja-JP, DDL); Windows/10.0.0 (Win64);"/>
  <style>
    body, td {
      font-family: メイリオ;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="1649"/>

<div>
<span><div><div><div><span style="font-size: 10pt;">20181210</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt; font-weight: bold;">H立用語</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■QA(Quality Assurance:品質保証): 製品やシステムの性能や機能を保証するための検証作業。第三者の立場で品質を見極め、最終的な品質を保証する責任を持つ。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■構成管理: 構成要素（仕様書、ソースコード、計画書など）に対する変更を管理・検証し、その状況を記録する活動。主に以下の3つ。</span></div><div><span style="font-size: 10pt;">①変更管理: 変更した履歴を残す</span></div><div><span style="font-size: 10pt;">②バージョン管理: ある時点に切り戻しできるようにする</span></div><div><span style="font-size: 10pt;">③不具合管理: 不具合の原因と状況を確認できるようにする</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■B票（不具合管理票）: テストで発生した不具合・問題を記録する帳票（業務に関連する書類）</span></div><div><span style="font-size: 10pt;">■C票: 仕様変更の発生内容を記録する帳票</span></div><div><span style="font-size: 10pt;">■P票: B票もしくはC票の起票後に、それを契機に行うプログラム修正内容を記録する帳票</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><hr/><div><span style="font-size: 10pt; font-weight: bold;">A社独自のもの</span></div><div><span style="font-size: 10pt;">■CPRD Viewer, PSSA Viewer：（F井）から引き継いだR/Shinyのwebアプリ</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■DR:マイクロアレイデータ取得元の部名。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■OCLN: マイクロアレイデータ の共同研究先。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■GAPFREE：産学官共同創薬研究プロジェクト</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■XCEED：A社の新サーバ</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■RWI（Real World Informatics and Analytics）：</span><span style="font-size: 10pt;">A社</span><span style="font-size: 10pt;">が設立したビックデータを活用するための専門機能。コンプライアンスや国家間の法律などを考慮して世界中のヘルスケアデータ（＝real world data）を活用して新たな価値を作りだす。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■BKL(BIOBASE Knowledge Library):遺伝子・タンパク質の発現・機能・疾患との関連などを100万以上の文献から専門家がマニュアルでキューレーションし纏め上げたDB。有償DB。</span></div><div><span style="font-size: 10pt;">　A社が購入しているタンパク質の発現・機能・疾患に関する有償データベース。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■DHL( Dispatch HyperLink ):URLを変換し転送するアプリケーション</span></div><div><font style="font-size: 10pt;"><br/></font></div><hr/><div><span style="font-size: 10pt; font-weight: bold;">医療系</span></div><div><span style="font-size: 10pt;">■CPRD:英国保健省のサービス。診療データ収集、診療データの結合および調査、診療データの提供などを行う。&lt;</span><a href="https://www.cprd.com/intro.asp" rev="en_rl_none" style="font-size: 10pt;">https://www.cprd.com/intro.asp</a><span style="font-size: 10pt;">&gt;</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■PSSA:薬物使用と診断の2つの事象間から薬物の副作用などを検出する方法&lt;</span><a href="http://www.ncbi.nlm.nih.gov/pubmed/8862977" rev="en_rl_none" style="font-size: 10pt;">http://www.ncbi.nlm.nih.gov/pubmed/8862977</a><span style="font-size: 10pt;">&gt;</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■NCBI(National Center for Biotechnology Information):&lt;</span><a href="https://www.ncbi.nlm.nih.gov/" rev="en_rl_none" style="font-size: 10pt;">https://www.ncbi.nlm.nih.gov/</a><span style="font-size: 10pt;">&gt;分子生物学やバイオインフォマティクスの研究に用いられるデータベースの構築及び運営や、研究に用いられるソフトウェアの開発を行っているアメリカの機関。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Entrez Gene：NCBIのデータベース内での遺伝子ID。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Open Target platform&lt;</span><a href="https://targetvalidation.org/" rev="en_rl_none" style="font-size: 10pt;">https://targetvalidation.org/</a><span style="font-size: 10pt;">&gt;：薬剤の標的（タンパク質、タンパク質複合体またはRNA分子）を特定するための調査支援を目的とした薬剤の標的と疾患の関連性、薬剤の標的情報および疾患の情報についての統合データ。</span></div><div><span style="font-size: 10pt;">　遺伝子名や疾患名などのキーワード検索でデータを取得でき、薬剤標的と疾患の関連性についてはバブルチャート/一覧表/ツリー形式で表示する。</span></div><div><span style="font-size: 10pt;">　データ取得はREST APIを利用。</span></div><div><span style="font-size: 10pt;">　内包しているデータ（20161226時点）</span></div><div><span style="font-size: 10pt;">　　薬剤の標的情報：31,071件</span></div><div><span style="font-size: 10pt;">　　薬剤の標的と疾患の関連情報：2,559,080件</span></div><div><span style="font-size: 10pt;">　　疾患情報：8,659件</span></div><div><span style="font-size: 10pt;">　　データソース：13個の public DB</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■PhenoDigm：動物モデルの表現型情報にもとづいて人の疾患遺伝子候補を順位付けするアルゴリズム。</span></div><div><span style="font-size: 10pt;">&lt;</span><a href="http://www.sanger.ac.uk/science/tools/phenodigm" rev="en_rl_none" style="font-size: 10pt;">http://www.sanger.ac.uk/science/tools/phenodigm</a><span style="font-size: 10pt;">&gt;</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■AKI（acute kidney injury: AKI）: 急性腎障害。何らかの原因で短期間に腎機能が急速に低下した状態の総称。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■SLE(systemic lupus erythematosus)：全身性エリテマトーデス。細胞の核成分に対する抗体を中心とした自己抗体が作られてしまうために、全身の諸臓器が侵されてしまう病気。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■PFAM：タンパク質ドメインの類似配列データベース</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■MOE(Molecular Operating Environment)： カナダCCG社が独自に開発したSVL（Scientific Vector Language）を搭載する統合計算化学システム。smilesを構造式画像に変換などができる。</span></div><div><a href="https://www.molsis.co.jp/lifescience/moe/" style="font-size: 10pt;">https://www.molsis.co.jp/lifescience/moe/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■脱塩（Wash）：NaやOHを取り除いて正式な構造式に戻すこと</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Largest Fragmet：分子量が大きい化合物の構造だけにしたもの</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■HCS(ハイコンテントスクリーニング)： </span></div><div><span style="font-size: 10pt;">電動顕微鏡、マルチパラメータの画像処理、および視覚化ツールを用いた一連の解析方法。</span></div><div><span style="font-size: 10pt;">新薬の候補となる膨大な数の化合物から，有用なものを迅速に高効率で選別する。</span></div><div><span style="font-size: 10pt;">１．培養した（薬剤などを投与多め/少なめ/なしにした）細胞をセットした蛍光プレートを電子顕微鏡で撮影して画像の形式でデータを取得する。</span></div><div><span style="font-size: 10pt;">画像として蛍光測定または発光測定した結果を保持=細胞内におけるターゲット分子の増減、局在の変化や細胞の運動性や形態情報などが含まれる（はず）</span></div><div><span style="font-size: 10pt;">２．取得したデータを解析するために画像処理を行いスクリーニング（</span><span style="font-size: 10pt; color: rgb(84, 84, 84); font-family: arial, sans-serif;">選別</span><span style="font-size: 10pt;">）を行う。</span></div><div><span style="font-size: 10pt;">複数のデータ（撮影する光の波長が違うデータとか）を得ることで多面的な解析ができる点に特徴がある。</span></div><div><span style="font-size: 10pt;">一言でいうと培養した細胞から取得した大量の画像データを解析して薬剤の効果判定をする手法。</span></div><div><span style="font-size: 10pt;">マイクロアレイのすごい版みたいな感じかなあ。。。</span></div><div><a href="https://www.yodosha.co.jp/jikkenigaku/keyword/3496.html" style="font-size: 10pt;">https://www.yodosha.co.jp/jikkenigaku/keyword/3496.html</a></div><div><a href="https://www.hamamatsu.com/jp/ja/applications/high-throughput-screening_high-content-screening/index.html" style="font-size: 10pt;">https://www.hamamatsu.com/jp/ja/applications/high-throughput-screening_high-content-screening/index.html</a></div><div><a href="https://www.thermofisher.com/jp/ja/home/life-science/cell-analysis/cellular-imaging/high-content-screening.html" style="font-size: 10pt;">https://www.thermofisher.com/jp/ja/home/life-science/cell-analysis/cellular-imaging/high-content-screening.html</a></div><div><a href="https://www.corning.com/jp/jp/products/life-sciences/applications/drug-discovery-applications/high-content-screening.html" style="font-size: 10pt;">https://www.corning.com/jp/jp/products/life-sciences/applications/drug-discovery-applications/high-content-screening.html</a></div><div><span style="font-size: 10pt;">処理フロー概要</span></div><div><span style="font-size: 10pt;"><img src="用語_files/image.png" type="image/png" data-filename="image.png" title="Attachment"/></span></div><div><a href="http://www.mext.go.jp/b_menu/shingi/gijyutu/gijyutu8/010/gijiroku/__icsFiles/afieldfile/2010/11/29/1296830_3_1.pdf" style="font-size: 10pt;">http://www.mext.go.jp/b_menu/shingi/gijyutu/gijyutu8/010/gijiroku/__icsFiles/afieldfile/2010/11/29/1296830_3_1.pdf</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ImageJ：Javaの仮想マシン上で動作するNIHで開発されたオープンソースでパブリックドメインの画像処理ソフト。</span></div><div><span style="font-size: 10pt;">　</span><a href="https://www.yodosha.co.jp/jikkenigaku/keyword/3496.html" rev="en_rl_none" style="font-size: 10pt;">https://www.yodosha.co.jp/jikkenigaku/keyword/3496.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■IC50：50%阻害濃度（50%そがいのうど、IC50）または半数阻害濃度。</span></div><div><span style="font-size: 10pt;">　化合物の生物学的または生化学的阻害作用の有効度を示す値。</span></div><div><span style="font-size: 10pt;">　どの濃度で、その薬物（毒など）が標的としている物の半数（50％）の働きを阻害できるかを示す。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Murcko scaffolds</span></div><div><span style="font-size: 10pt;">　化合物から余分な側鎖を取り払い、&quot;環構造&quot;とそれらをつないでいる&quot;Linker&quot;のみで表現することでより単純な化合物表現</span></div><div><span style="font-size: 10pt;">　環構造を全く持たない化合物についてはMurcko scaffoldsが定義されない</span></div><div><span style="font-size: 10pt;">　Murcko scaffoldsの単純な環構造から似ている化合物クラスタリングするとかに使う</span></div><div><span style="font-size: 10pt;">　化合物構造活性相関において機械学習を行いたい場合にも有用で、DeepChemの Scaffold spliter は、Murcko scaffolds に基づいて化合物データセットを分割している</span></div><div><span style="font-size: 10pt;">　</span><a href="https://horomary.hatenablog.com/entry/2018/12/01/220952" rev="en_rl_none" style="font-size: 10pt;">https://horomary.hatenablog.com/entry/2018/12/01/220952</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■プロテインキナーゼ（以下，キナーゼ:kinase）:</span></div><div><span style="font-size: 10pt;">　マグネシウム（またはマンガン）イオンを必須とし，基質となるタンパク質の水酸基にアデノシン三リン酸（ATP）のγリン酸基を転移する酵素群</span></div><div><span style="font-size: 10pt;">　キナーゼは役割分担しながら，外部刺激に応答するシグナルを伝達し，細胞の分化，増殖，アポトーシスなどを通じて複雑な生体機能を恒常的に制御している</span></div><div><span style="font-size: 10pt;">　キナーゼはどれも生命にとって必要不可欠であるだけに，絶妙に制御された活性バランスの崩壊はガンなどの重篤な疾患につながる</span></div><div><span style="font-size: 10pt;">　2000年代初頭頃から今日に至るまで，イマチニブやゲフィチニブをはじめとする，約30種類の低分子化合物がキナーゼ阻害薬としてアメリカ食品医薬品局（FDA）によって認可されている.2）これらの医薬品は，疾患の原因となるキナーゼを特定してから開発されたことから，分子標的医薬品と呼ばれる</span></div><div><span style="font-size: 10pt;">　</span><a href="https://www.jstage.jst.go.jp/article/jcrsj/59/4/59_174/_pdf" rev="en_rl_none" style="font-size: 10pt;">https://www.jstage.jst.go.jp/article/jcrsj/59/4/59_174/_pdf</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■活性：化学的に活発な性質をもっていること。</span></div><div><span style="font-size: 10pt;">　物質の原子・分子がエネルギーの高い状態にあり、化学反応を起こしやすくなっていること。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■分子標的薬：病気の原因となる遺伝子をもとにつくられる分子（おもにタンパク質）を標的とし、その働きを抑える薬（分子の働きによっては、促進する場合もあります）</span></div><div><a href="http://p-direct.jfcr.or.jp/knowledge/knowledge02.html" style="font-size: 10pt;">http://p-direct.jfcr.or.jp/knowledge/knowledge02.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■%inhibition(インヒビテーション): 阻害率</span></div><div><span style="font-size: 10pt;">　化合物の生物学的または生化学的阻害作用の有効度を示す値。</span></div><div><span style="font-size: 10pt;">　遺伝子などに投与した化合物の濃度について、その薬物（毒など）が標的としている物（機能）を阻害した%（割合）のこと。</span></div><div><span style="font-size: 10pt;">　xxxというアッセイ( yyyという遺伝子についての実験や測定)で、 ある化合物を濃度10μM投与した時、%inhibition=15%（化合物濃度 10μM で yyyという遺伝子の機能を15%阻害している） みたいな使い方</span></div><div><span style="font-size: 10pt;">　ic50は逆で遺伝子の機能を50%阻害している化合物濃度[μM]</span></div><div><span style="font-size: 10pt;">　また、阻害剤(inhibitors)は酵素と相互に作用し，その反応効率を低下させる合成物または天然に存在する化合物のこと</span></div><div><span style="font-size: 10pt;">　</span> <a href="http://www.sc.fukuoka-u.ac.jp/~bc1/Biochem/EnzInh.htm" rev="en_rl_none" style="font-size: 10pt;">http://www.sc.fukuoka-u.ac.jp/~bc1/Biochem/EnzInh.htm</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■SMILES（読み方：スマイルス）：化合物の構造を１行の文字列で表記するための「線形表記法」（分子の構造をある一定のルールに沿って，１行で表記するようにした表記法）の１つ</span></div><div><span style="font-size: 10pt;">　</span> <a href="https://future-chem.com/smiles-smarts/" rev="en_rl_none" style="font-size: 10pt;">https://future-chem.com/smiles-smarts/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■エームス試験（Ames試験）</span></div><div><span style="font-size: 10pt;">　細菌を用いて変異原性（遺伝子に突然変異を引き起こす力）を調べる評価法。</span></div><div><span style="font-size: 10pt;">　評価物質の存在下、突然変異を起こした細菌のみ増殖可能な条件で培養を行う。変異増殖した細菌を培養プレート上で観察します。</span></div><div><span style="font-size: 10pt;">　</span><a href="https://www.ube-ind.co.jp/usal/documents/se136_134.htm" rev="en_rl_none" style="font-size: 10pt;">https://www.ube-ind.co.jp/usal/documents/se136_134.htm</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■GWAS：遺伝子型（特にSNPs）と表現型との関連調査する1手法。</span></div><div><span style="font-size: 10pt;">特定の形質についての遺伝子データ（マイクロアレイや次世代シーケンサーで取ったデータかなあ）のTarget/Control群用意し、統計（t検定？）の結果に有意差あるかで遺伝子型と表現型の関係を調べる。</span></div><div><span style="font-size: 10pt;">EBIにGWASの公共DBであるGWAS Catalogがある。</span></div><div><a href="https://www.ebi.ac.uk/gwas/" style="font-size: 10pt;">https://www.ebi.ac.uk/gwas/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■遺伝子型（Genotype）：個々人の遺伝子が持つ塩基配列のバリエーション</span></div><div><span style="font-size: 10pt;">例. ALK遺伝子のある部分が人によりA もしくは G であるようなこと</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■表現型（Phenotype）:遺伝子型が形質（身体的特徴）に現れたもの</span></div><div><span style="font-size: 10pt;">例. 瞳の色/身長/遺伝病 etc...</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■一塩基多型（Single Nucleotide Polymorphism:SNP）：遺伝子の中での1塩基の置換。SNPが個々人の体質の違いや薬の効きやすさにも関係すると考えられている。</span></div><div><span style="font-size: 10pt;">例. ALK遺伝子のある部分が自分は A だが、母親は G のように1塩基だけ違うこと。</span></div><div><a href="http://delta2323.github.io/blog/2014/01/21/gwas-phewas/" style="font-size: 10pt;">http://delta2323.github.io/blog/2014/01/21/gwas-phewas/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■PheWAS：ある遺伝子型がどのような症例に影響するかを網羅的に調査する方法で、GWASの逆の解析手法になる（GWASは形質からSNP探す）。</span></div><div><span style="font-size: 10pt;">PheWAS catalog はVanderbilt大学で公開されているPheWASのデータベース。</span></div><div><span style="font-size: 10pt;">eMERGEという電子医療情報からアルゴリズムで患者を2群に分けて変異との関連解析を実施。</span></div><div><span style="font-size: 10pt;">疾患には ICD-9 コード (約19000)が付いており、PheWAS で907に集約。 変異はGWAScatalogで調べられているものに限定。</span></div><div><a href="http://xceedsys/pukiwiki/?GWAS%20DB%C4%B4%EF%BF%BD%EF%BF%BD#b1821a84" style="font-size: 10pt;">http://xceedsys/pukiwiki/?GWAS%20DB%C4%B4%BA%BA#b1821a84</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■新薬開発プロセス</span></div><div><span style="font-size: 10pt;">1.標的分子の決定＝どんな分子を標的とするかを決める</span></div><div><span style="font-size: 10pt;">　標的分子は、がんの発症や悪性化などに関わるものがほとんど。</span></div><div><span style="font-size: 10pt;">　標的分子を決めたら、標的分子の働きを抑えたときに予想した効果（例えば細胞の増殖が遅くなるなど）が現れるかどうかを、細胞や実験動物を使って検証します。</span></div><div><span style="font-size: 10pt;">　これをPOC（proof of concept、概念の実証という意味）といいます。</span></div><div><span style="font-size: 10pt;">　（多分、マイクロアレイ、次世代シーケンサーを利用した実験から標的分子（遺伝子やタンパク質）を見つけてる）</span></div><div><span style="font-size: 10pt;">2.シード/リード化合物の探索=標的分子の働きを抑える化合物を探索</span></div><div><span style="font-size: 10pt;">　ハイスループット・スクリーニングやHCS（ハイコンテントスクリーニング）で標的分子と結合し、その働きを抑える化合物を探索する。</span></div><div><span style="font-size: 10pt;">　これにより見つかった化合物を「シード化合物」と呼びます。</span></div><div><span style="font-size: 10pt;">　シード化合物は標的分子と結合するものですが、薬とするには、「水に溶ける」、「体内で安定である」、「細胞に取り込まれる」といった条件を満たさなければなりません。</span></div><div><span style="font-size: 10pt;">　そこで、シード化合物の構造を少しずつ変えて、このような条件に合う化合物をつくります。</span></div><div><span style="font-size: 10pt;">　これが「リード化合物」です。</span></div><div><span style="font-size: 10pt;">3.リード化合物の評価</span></div><div><span style="font-size: 10pt;">　リード化合物が狙った標的分子に結合し、その働きを抑えるかどうかは、やはりPOCを行って確かめます。</span></div><div><span style="font-size: 10pt;">　（ここでもHCSを利用した実験をしてたしかめるのかなあ）</span></div><div><span style="font-size: 10pt;">4.リード化合物の最適化</span></div><div><span style="font-size: 10pt;">　リード化合物が薬効を示すことが確かめられたら、より薬効が強く、安全性も高い化合物を求めて、さらに構造をブラッシュアップします。</span></div><div><span style="font-size: 10pt;">　たくさんの化合物を合成してテストしますが、その際には、標的分子の構造とリード化合物の構造がぴったり合うかどうかを、合成前にコンピュータの中で調べる「イン・シリコ解析」も行います。</span></div><div><span style="font-size: 10pt;">　（ここで、VAEで作った化合物、マルチタスクモデルで化合物のAssay分類予測とか試す？）</span></div><div><span style="font-size: 10pt;">5.前臨床実験＝動物実験</span></div><div><span style="font-size: 10pt;">6.臨床試験＝人でお薬試験</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image.png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="http://p-direct.jfcr.or.jp/knowledge/knowledge02.html" style="font-size: 10pt;">http://p-direct.jfcr.or.jp/knowledge/knowledge02.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [1].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://www.procommit.co.jp/mitou/pharma_fuji" style="font-size: 10pt;">https://www.procommit.co.jp/mitou/pharma_fuji</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■（医療用医薬品の標的）分子：遺伝子そのもの、または遺伝子により規定されるタンパク質またはそれらタンパク質の活性にともなう2次的な産物</span></div><div><a href="https://www.jstage.jst.go.jp/article/fpj/129/2/129_2_119/_pdf" style="font-size: 10pt;">https://www.jstage.jst.go.jp/article/fpj/129/2/129_2_119/_pdf</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■遺伝的アルゴリズム = 生物の進化の過程を真似て作られたアルゴリズムで、確率的探索(サンプル店を評価しながら探索する方法=ニューラルネットのlossとoptimaizerみたいなの)、学習、最適化の一手法</span></div><div><span style="font-size: 10pt;">遺伝的アルゴリズムの基本を構成している重要な処理プロセスは、以下の3つになります。</span></div><div><span style="font-size: 10pt;">　　●選択 (selection)</span></div><div><span style="font-size: 10pt;">　　●交叉 (crossover)</span></div><div><span style="font-size: 10pt;">　　●突然変異 (mutation)</span></div><div><span style="font-size: 10pt;">　そして、これらを繰り返し行うことで、人工的な進化を行い、最適解を発見していく</span></div><div><a href="http://ipr20.cs.ehime-u.ac.jp/column/ga/index.html" style="font-size: 10pt;">http://ipr20.cs.ehime-u.ac.jp/column/ga/index.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">→遺伝的アルゴリズムによる探索</span></div><div><span style="font-size: 10pt;">　ランダムサーチ、最急勾配法(山登り法)ともに問題があるのですが、遺伝的アルゴリズムはどうなのでしょうか？</span></div><div><span style="font-size: 10pt;">　遺伝的アルゴリズムによる探索は、初期集団から選択と交叉の組み合わせにより並列的に山登り探索をし、なおかつ突然変異びよりときどきランダムな変化を起こしています。</span></div><div><span style="font-size: 10pt;">　複数の解について並列的に調べていくため、最急勾配法のような局所安定には陥りにくく、それでも局所安定に近づいてしまっても、突然変異によってそこから抜け出すことができます。</span></div><div><span style="font-size: 10pt;">　遺伝的アルゴリズムにももちろん問題があります。</span></div><div><span style="font-size: 10pt;">　遺伝的アルゴリズムの問題は、個体数、交叉、突然変異の確率などのパラメータやコーディングの一般的手法が確立されていないことです。</span></div><div><span style="font-size: 10pt;">　そして、必ず最適解を求めなくてはならないという場合には使えません。</span></div><div><span style="font-size: 10pt;">　しかし、ある程度の基準以上の解をなるべく少ない計算量で求めたい場合には、良い手法だと云うことができます。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">→遺伝的アルゴリズムの特徴</span></div><div><span style="font-size: 10pt;">○長所</span></div><div><span style="font-size: 10pt;">　・実用時間内に比較的優れた解を求めることができる。</span></div><div><span style="font-size: 10pt;">　・幅広い応用範囲を持っており、さまざまな問題に適応できる。</span></div><div><span style="font-size: 10pt;">　・多点探索アルゴリズムのため、関数の連続性の影響を受けにくい。</span></div><div><span style="font-size: 10pt;">○短所</span></div><div><span style="font-size: 10pt;">　・パラメータやコーディングに対する一般的な規範がない。</span></div><div><span style="font-size: 10pt;">　・問題に適用する一般的な方法が存在しない。</span></div><div><a href="http://ipr20.cs.ehime-u.ac.jp/column/ga/chapter3.html" style="font-size: 10pt;">http://ipr20.cs.ehime-u.ac.jp/column/ga/chapter3.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■DAPI（ だぴ/ だーぴー、4',6-diamidino-2-phenylindole）: 染色に用いられる蛍光色素の一種で、DNAに対して強力に結合する物質。 蛍光顕微鏡観察に広く利用されている。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■FITC: 蛍光色素。細胞の情報解析等に汎用される。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■OpenEye:創薬のアプリケーションやツールキットを開発している会社。アメリカのボストンが本社の海外資本</span></div><div><span style="font-size: 10pt;">&lt;</span><a href="https://www.eyesopen.com/about" rev="en_rl_none" style="font-size: 10pt;">https://www.eyesopen.com/about</a><span style="font-size: 10pt;">&gt;</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■OEDepict TK:smilesから化学構造画像を作成するOpenEyeのソフトウエア。pythonライブラリとして利用できる</span></div><div><a href="https://www.eyesopen.com/oedepict-tk" style="font-size: 10pt;">https://www.eyesopen.com/oedepict-tk</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■有機化合物（organic compound）：炭素を含む化合物</span></div><div><span style="font-size: 10pt;">　炭素原子が共有結合で結びついた骨格を持ち、分子間力によって集まることで液体や固体となっている。</span></div><div><span style="font-size: 10pt;">　以下のメタンとか。</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [2].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://juken-mikata.net/how-to/chemistry/organic-compound.html" style="font-size: 10pt;">https://juken-mikata.net/how-to/chemistry/organic-compound.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■炭化水素: 炭素と水素とから成り立っている有機化合物。メタンとか。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■基（groupまたは原子団）：有機化合物の性質を特徴づける原子の集団（＝原子が相互に共有結合で連結された部分構造）</span></div><div><span style="font-size: 10pt;">　不対電子をもつ原子，分子やイオンを指す遊離基（ radical ）を意味する基（ radical ）の異なる意味をもつ。</span></div><div><span style="font-size: 10pt;">　原子団を意味する基は，概念の違いにより</span></div><div><span style="font-size: 10pt;">　置換基（ substituent group ），</span></div><div><span style="font-size: 10pt;">　特性基（ characteristic group ），</span></div><div><span style="font-size: 10pt;">　官能基（ functional group ）に分けられる。</span></div><div><span style="font-size: 10pt;">■置換基: 化合物の部分構造。炭化水素鎖やベンゼン環についている原子や原子群を置換基という。</span></div><div><span style="font-size: 10pt;">　ハロゲノ基や炭化水素基などいろいろ種類がある。</span></div><div><span style="font-size: 10pt;">　</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [3].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">　</span><a href="https://chem.mikuas.com/hc1-u4/c1/p8/" rev="en_rl_none" style="font-size: 10pt;">https://chem.mikuas.com/hc1-u4/c1/p8/</a></div><div><span style="font-size: 10pt;">■特性基: 化合物を特徴づける原子団。</span></div><div><span style="font-size: 10pt;">■官能基: 化学的属性や化学反応性に着目した原子団。</span></div><div><span style="font-size: 10pt;">　単一の特性基または複数の特性基の組み合わせで官能基が構成される</span></div><div><span style="font-size: 10pt;">　</span><a href="http://sekigin.jp/science/chem/chem_06_01_3.html" rev="en_rl_none" style="font-size: 10pt;">http://sekigin.jp/science/chem/chem_06_01_3.html</a></div><div><span style="font-size: 10pt;">　</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [4].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">　</span><a href="https://ja.wikipedia.org/wiki/%E5%9F%BA" rev="en_rl_none" style="font-size: 10pt;">https://ja.wikipedia.org/wiki/%E5%9F%BA</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">　</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [5].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">　</span><a href="http://sekigin.jp/science/chem/chem_06_01_3.html" rev="en_rl_none" style="font-size: 10pt;">http://sekigin.jp/science/chem/chem_06_01_3.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ヒドロキシ基（ヒドロキシル基、水酸基）:-OHで表される一価の基。置換基の一種。</span></div><div><span style="font-size: 10pt;">　H（ヒドロゲン）とO（オキシゲン）からなるのでヒドロキシ基。</span></div><div><span style="font-size: 10pt;">　</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [6].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">　</span><a href="https://chem.mikuas.com/hc1-u4/c1/p8/" rev="en_rl_none" style="font-size: 10pt;">https://chem.mikuas.com/hc1-u4/c1/p8/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■hERG(human Ether-a-go-go Related Gene)：ヒト心臓の心筋に発現しているhERG（human ether-a-go-go related gene）から発現するタンパク質（＝心筋活動電位の再分極を担うカリウムイオンチャネルをコードする遺伝子）</span></div><div><span style="font-size: 10pt;">　hERGに対するIC50値が10μM以下の薬物は，心不全につながるQT延長のリスクが高い．</span></div><div><span style="font-size: 10pt;">　</span><a href="https://www.yodosha.co.jp/jikkenigaku/keyword/2939.html" rev="en_rl_none" style="font-size: 10pt;">https://www.yodosha.co.jp/jikkenigaku/keyword/2939.html</a></div><div><span style="font-size: 10pt;">　hERGチャンネル（カリウムチャンネル） に対する阻害効果を調べるassayがある（心不全に関する薬探索のためのassay?）</span></div><div><span style="font-size: 10pt;">　</span><a href="https://www.thermofisher.com/jp/ja/home/japan/services-jp/selectscreen-herg.html" rev="en_rl_none" style="font-size: 10pt;">https://www.thermofisher.com/jp/ja/home/japan/services-jp/selectscreen-herg.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■PAMPA(Parallel ar£ificial membrane permeation assay:並行人工膜透過性試験)：</span></div><div><span style="font-size: 10pt;">　多くの製薬企業が創薬初期段階のスクリーニングに導入しているアッセイらしい</span></div><div><span style="font-size: 10pt;">　</span><a href="https://www.jstage.jst.go.jp/article/jpstj/64/1/64_40/_pdf/-char/ja" rev="en_rl_none" style="font-size: 10pt;">https://www.jstage.jst.go.jp/article/jpstj/64/1/64_40/_pdf/-char/ja</a></div><div><span style="font-size: 10pt;">　</span><a href="https://www.scas.co.jp/technical-informations/technical-news/pdf/tn450.pdf" rev="en_rl_none" style="font-size: 10pt;">https://www.scas.co.jp/technical-informations/technical-news/pdf/tn450.pdf</a></div><div><span style="font-size: 10pt;">　</span><a href="https://en.wikipedia.org/wiki/Parallel_artificial_membrane_permeability_assay" rev="en_rl_none" style="font-size: 10pt;">https://en.wikipedia.org/wiki/Parallel_artificial_membrane_permeability_assay</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■EC50(50%効果濃度:half maximal Effective Concentration)：活性値。薬などの作用が最大値の半分の効果を示すときの濃度。</span></div><div><span style="font-size: 10pt;">　</span><a href="https://ultrabem.jimdofree.com/other-topics/biochemistry/ec50-ic50/" rev="en_rl_none" style="font-size: 10pt;">https://ultrabem.jimdofree.com/other-topics/biochemistry/ec50-ic50/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■阻害剤(inhibitors)：酵素と相互に作用し，その反応効率を低下させる合成物または天然に存在する化合物</span></div><div><span style="font-size: 10pt;">　</span><a href="http://www.sc.fukuoka-u.ac.jp/~bc1/Biochem/pdf/DetKi.pdf" rev="en_rl_none" style="font-size: 10pt;">http://www.sc.fukuoka-u.ac.jp/~bc1/Biochem/pdf/DetKi.pdf</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Ki値：阻害定数。酵素—阻害剤複合体の解離定数に相当する。Kiは酵素と阻害剤の親和性の尺度であり、値が小さいほど酵素に対する親和性が強いことを示す。</span></div><div><span style="font-size: 10pt;">　</span><a href="https://bsd.neuroinf.jp/wiki/%E3%83%9F%E3%82%AB%E3%82%A8%E3%83%AA%E3%82%B9%E3%83%BB%E3%83%A1%E3%83%B3%E3%83%86%E3%83%B3%E3%81%AE%E5%BC%8F" rev="en_rl_none" style="font-size: 10pt;">https://bsd.neuroinf.jp/wiki/%E3%83%9F%E3%82%AB%E3%82%A8%E3%83%AA%E3%82%B9%E3%83%BB%E3%83%A1%E3%83%B3%E3%83%86%E3%83%B3%E3%81%AE%E5%BC%8F</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Kd値（dissociation constant）：解離平衡定数。</span></div><div><span style="font-size: 10pt;">　解離定数は物質がどれだけ強く結合しているかを説明する指標として有用らしい。EC50やIC50が物質の生物学的活性を説明するのと同じように。</span></div><div><span style="font-size: 10pt;">　</span><a href="https://ja.wikipedia.org/wiki/%E8%A7%A3%E9%9B%A2%E5%AE%9A%E6%95%B0" rev="en_rl_none" style="font-size: 10pt;">https://ja.wikipedia.org/wiki/%E8%A7%A3%E9%9B%A2%E5%AE%9A%E6%95%B0</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ADMET：吸収（absorption），分布（distribution），代謝（metabolism），排泄（excretion），毒性（toxicity）の英語表記の頭文字からなる略語</span></div><div><span style="font-size: 10pt;">　薬物が生体内に取り込まれてから体外に排泄されるまでの過程のこと</span></div><div><span style="font-size: 10pt;">　薬物の薬効と毒性に関係する重要な項目</span></div><div><span style="font-size: 10pt;">　</span><a href="https://www.yodosha.co.jp/jikkenigaku/keyword/887.html" rev="en_rl_none" style="font-size: 10pt;">https://www.yodosha.co.jp/jikkenigaku/keyword/887.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Kinase Panel Assay:Kinaseに関するアッセイ</span></div><div><span style="font-size: 10pt;">　</span><a href="https://www.discoverx.com/services/drug-discovery-development-services/kinase-profiling" rev="en_rl_none" style="font-size: 10pt;">https://www.discoverx.com/services/drug-discovery-development-services/kinase-profiling</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■バイオマーカー：</span></div><div><span style="font-size: 10pt;">血液や尿などの体液や組織に含まれる、タンパク質や遺伝子などの生体内の物質で、病気の変化や治療に対する反応に相関し、指標となるもの。</span></div><div><span style="font-size: 10pt;">バイオマーカーの量を測定することで、病気の存在や進行度、治療の効果の指標の1つとすることができます。</span></div><div><span style="font-size: 10pt;">腫瘍マーカーもバイオマーカーの一種です。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■メタボロミクス（metabolomics）：</span></div><div><span style="font-size: 10pt;">生体中の有機酸やアミノ酸などの低分子化合物をはじめとする代謝物（メタボローム：metabolome）の分析を行うこと</span></div><div><span style="font-size: 10pt;">ゲノミクス（genomics）が DNAを，</span></div><div><span style="font-size: 10pt;">トランスクリプトミクス（transcriptomics）が ｍRNAを，</span></div><div><span style="font-size: 10pt;">プロテオミクス（proteomics）がタンパク質を網羅的に解析することに対し，</span></div><div><span style="font-size: 10pt;">metabolomics は，代謝物を網羅的に解析するといえます。</span></div><div><span style="font-size: 10pt;">食事，薬物，運動，各種ストレスなど環境の影響を，DNAもタンパク質も受けます。</span></div><div><span style="font-size: 10pt;">これらの影響を受けた結果が，代謝物に反映されると考えられ，メタボロミクスは生体内におけるゲノムおよびタンパク質の活性，そして環境が及ぼす影響の総和を測定するといえます。</span></div><div><span style="font-size: 10pt;">メタボロミクスにより，生体機能に関する価値のある情報が得られるため，バイオマーカー発見のプロジェクト（薬物代謝・動態、薬物の安全性，毒性からの薬理など）から病気の診断，生活習慣や健康に関する研究に至るまで，広範囲な研究分野で応用が広がっています。</span></div><div><a href="https://www.an.shimadzu.co.jp/apl/lifescience/metabolomics.htm" style="font-size: 10pt;">https://www.an.shimadzu.co.jp/apl/lifescience/metabolomics.htm</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■代謝(metabolism)：生体内で行われる物質の変化</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■血漿（けっしょう:plasma(プラズマ)）</span></div><div><span style="font-size: 10pt;">血液から有形成分（赤血球、白血球、血小板）を除いた液体成分で、血液の約55％を占めます。栄養成分を各組織に運ぶことや組織呼吸の結果できた炭酸ガスや老廃物を排出する働きをします。血漿のpHはきわめて狭い範囲に一定に保たれ、体中の細胞を至適環境におくために重要な働きをしています。</span></div><div><font style="font-size: 10pt;"><br/></font></div><hr/><div><span style="font-size: 10pt; font-weight: bold;">マイクロアレイ系</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■（DNA）マイクロアレイ：数千〜数万種類といった非常に多くの既知遺伝子のDNA断片（これをDNAプローブといいます）を小さな基盤の上に並べたもののことで、DNAマイクロアレイ法を用いることにより、細胞における遺伝子発現の網羅的な解析を行うことができます。</span></div><div><span style="font-size: 10pt;">※DNAプローブとは、目的の DNAと相補的に結合するように人工的に設計された既知のDNA断片のことです。</span></div><div><span style="font-size: 10pt;">※遺伝子発現の網羅的解析とは、DNAから転写された全遺伝子におけるmRNAの発現レベルを一度に解析できる手法のことをいいます。</span></div><div><span style="font-size: 10pt;">ヒトの全遺伝子を貼り付けたチップやマウスの全遺伝子を貼り付けたチップなど実験の用途に合わせてチップを使い分ける。</span></div><div><span style="font-size: 10pt;">解析で用いるマイクロアレイはアジレント社（Agilent Technologies）、アフィメトリクス社（Affymetrix）製が一般的。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">DNAマイクロアレイの基盤の上には、対象となる生物（例えばマウス など）がもつあらゆる遺伝子がDNAプローブとして固定されていますので、細胞内のmRNAから合成したcDNA（蛍光標識したもの）をこのDNAマイクロアレイ上に流し込むことにより、細胞内で発現している遺伝子を網羅的に解析することができます。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">　細胞からmRNAを抽出し、逆転写酵素によってcDNAを合成する際、蛍光色素でcDNAを標識（ラベル）しておく必要があります。これをDNAマイクロアレイ上に流し込むと、基盤の上に固定したDNAプローブが蛍光色素で標識したcDNAとハイブリダイゼーションしますので、このときの蛍光を観察することによって、細胞内でどの遺伝子がどの程度発現しているかを網羅的に知ることができます。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">※ラベルには、1種類の蛍光色素でラベルする「1色法」と2種類の蛍光色素でラベルする「2色法」の二種類があります。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><a href="https://lifescience-study.com/2-comprehensive-analysis-of-gene-expression-microarray-method/" style="font-size: 10pt;">https://lifescience-study.com/2-comprehensive-analysis-of-gene-expression-microarray-method/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■マイクロアレイデータ：多数のDNA断片をプラスチックやガラス等の基板上に高密度に配置して測定した細胞の遺伝子発現量（計測されているのは光の量（シグナル）だが発現多いほど光るのでmRNAの発現量といえる）についてのデータ</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Affymetrix社&lt;</span><a href="http://www.affymetrix.com/jp/" rev="en_rl_none" style="font-size: 10pt;">http://www.affymetrix.com/jp/</a><span style="font-size: 10pt;">&gt;：マイクロアレイ製品の主流メーカーの１つ。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■GEO(NCBI Gene Expression Omnibus)&lt;</span><a href="https://www.ncbi.nlm.nih.gov/geo/" rev="en_rl_none" style="font-size: 10pt;">https://www.ncbi.nlm.nih.gov/geo/</a><span style="font-size: 10pt;">&gt;：NCBIが提供・維持管理している遺伝子発現情報のデータベース。主にマイクロアレイ実験で得られた生データが蓄積されており、データ検索しダウンロードできる。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■アノテーションファイル：マイクロアレイの生データであるシグナル値やフラグ以外に付加されている情報。NCBIなどのデータベースから取得する。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Illumina：全世界でバイオ関係の研究機関や企業に対してゲノム/DNAを分析する装置や試薬を提供している企業。 DNAマイクロアレイも開発販売してる。</span></div><div><span style="font-size: 10pt;">　&lt;</span><a href="https://jp.illumina.com/" rev="en_rl_none" style="font-size: 10pt;">https://jp.illumina.com/</a><span style="font-size: 10pt;">&gt;</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Bioconductor：バイオ関連データを扱うためのRパッケージ集。無償で利用可能。</span></div><div><span style="font-size: 10pt;">　&lt;</span><a href="http://www.bioconductor.org/" rev="en_rl_none" style="font-size: 10pt;">http://www.bioconductor.org/</a><span style="font-size: 10pt;">&gt;</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■fold change：サンプル検体とコントロール検体のシグナル値の比率。遺伝子の発現量を表す。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■p-value：統計用語。群間差が偶然生じる可能性を示す尺度。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■q-value of FDR(False Discovery Rate):統計用語。複数回検定を行う場合に発生する間違った結果を含む確率。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■DNA（デオキシリボ核酸）(Deoxyribonucleic Acid)：細胞核の染色体に存在する4種類（A, C, G, T）の塩基の二重螺旋の並びから構成される遺伝物質（生物の遺伝的形態を決定し、それを子孫に伝える物質）。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■（遺伝）形質：DNAの情報に基づいて子孫に受け継がれる特徴</span></div><div><span style="font-size: 10pt;">　</span><a href="https://mycode.jp/dna/dna-quick-description.html" rev="en_rl_none" style="font-size: 10pt;">https://mycode.jp/dna/dna-quick-description.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■遺伝子：遺伝形質を決める因子（DNAの塩基配列）のこと</span></div><div><span style="font-size: 10pt;">　(ヒトでは31億文字あるDNAの“意味を持つ単語”にあたる部分が“遺伝子”）</span></div><div><span style="font-size: 10pt;">　</span><a href="https://mycode.jp/dna/dna-quick-description.html" rev="en_rl_none" style="font-size: 10pt;">https://mycode.jp/dna/dna-quick-description.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■転写：DNAの塩基配列(遺伝子)を元にRNAが合成されること。</span></div><div><span style="font-size: 10pt;">　転写を行うのは、RNAポリメラーゼという酵素（DNAの塩基配列を鋳型として、それと相補的なRNAを合成する酵素）</span></div><div><span style="font-size: 10pt;">　転写では、DNA（遺伝子）中のプロモーターとよばれる領域にRNAポリメラーゼが結合します。</span></div><div><span style="font-size: 10pt;">　プロモーターは、転写を開始する位置と、転写の方向を決めている。</span></div><div><span style="font-size: 10pt;">　そして転写を開始する位置付近のDNAの２本鎖が解離し、そこから転写が開始します。</span></div><div><span style="font-size: 10pt;">　一方のDNA鎖を鋳型とし、その塩基配列と相補的なRNAを合成していきます。</span></div><div><span style="font-size: 10pt;">　つまり、転写はDNAの一方の鎖を鋳型とし、もう一方の鋳型のコピーを作成する。</span></div><div><span style="font-size: 10pt;">　ここで注意しなければならないのは、DNAではA・G・C・Tの４種類の塩基が使われていましたが、RNAではTの代わりにU（ウラシル）が使われる。</span></div><div><span style="font-size: 10pt;">　すなわち、合成されたRNA中では、DNA中でTであった部分がすべてUに置き換えられている。</span></div><div><span style="font-size: 10pt;">　</span><a href="http://www.mls.sci.hiroshima-u.ac.jp/smg/gene/gene5.html" rev="en_rl_none" style="font-size: 10pt;">http://www.mls.sci.hiroshima-u.ac.jp/smg/gene/gene5.html</a></div><div><span style="font-size: 10pt;">　</span><a href="http://www.spring8.or.jp/ja/news_publications/research_highlights/no_14/" rev="en_rl_none" style="font-size: 10pt;">http://www.spring8.or.jp/ja/news_publications/research_highlights/no_14/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■RNA（リボ核酸）：DNAの遺伝情報の伝達やタンパク質の合成などを担う4種類（A, C, G, U）の塩基が一本鎖に並んだ遺伝物質。</span></div><div><span style="font-size: 10pt;">　DNAが持つ遺伝情報のうち、個々のタンパク質を合成する際の鋳型となる塩基配列を写しとったものをmRNA（messenger RNA）といい、mRNAを鋳型として細胞内のタンパク質が合成される。</span></div><div><span style="font-size: 10pt;">　</span><a href="http://shiritanist.net/dna-rna-chigai/" rev="en_rl_none" style="font-size: 10pt;">http://shiritanist.net/dna-rna-chigai/</a></div><div><span style="font-size: 10pt;">　</span><a href="https://www.jst.go.jp/pr/info/info381/yougo.html" rev="en_rl_none" style="font-size: 10pt;">https://www.jst.go.jp/pr/info/info381/yougo.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■cDNA（相補的DNA：complementary DNAA)：</span></div><div><span style="font-size: 10pt;">様々なタンパクを定義しているDNAは、生体内でDNAと相補的な塩基配列をもったmRNAに変換される。</span></div><div><span style="font-size: 10pt;">そのmRNAを鋳型に、逆転写酵素によって合成されるDNAはmRNAとは相補的な塩基配列となるため、相補的DNA(complementary DNA;cDNA)と呼ばれる。</span></div><div><a href="https://www.jst.go.jp/pr/info/info74/yougo.html" style="font-size: 10pt;">https://www.jst.go.jp/pr/info/info74/yougo.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■翻訳：RNAを元にタンパク質が合成されること。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■遺伝子発現（発現）：遺伝情報に基づいてタンパク質が合成されること（DNA→（転写）→RNA→（翻訳）→タンパク質のサイクルをセントラルドグマという）。また発現される量（発現量）のことを発現ということもある。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ゲノム(genome)：染色体上の遺伝子（塩基配列）が持つすべての遺伝情報。遺伝子(gene)と染色体(chromosome)から合成された言葉。ゲノムを解析すればその生物の遺伝情報（親から子に引き継がれる情報）がわかる。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■PCR法：遺伝子増幅技術。</span></div><div><span style="font-size: 10pt;">増やしたい遺伝子のDNA配列にくっつくことができる短いDNA（プライマー）を用意し、酵素の働きと温度を上げ下げすることで、目的の遺伝子を増やす方法。</span></div><div><span style="font-size: 10pt;">増えたDNAを染め出す特殊な装置に入れる事で、増えた遺伝子を目で確認する事ができます。</span></div><div><span style="font-size: 10pt;">検体の中に増やしたい遺伝子があれば増えて目で確認することができ“陽性”と判定されます。</span></div><div><span style="font-size: 10pt;">しかし、検体の中に遺伝子がなければ増えないので、目で確認することはできず、 “陰性”と判定されます。</span></div><div><a href="http://www.biseibutu.co.jp/tabid/148/Default.aspx" style="font-size: 10pt;">http://www.biseibutu.co.jp/tabid/148/Default.aspx</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■in vito: インビトロ。「試験管内での」という意味。試験管や培養器などで体内と同様の環境を作り薬物などの反応を検出する試験のこと</span></div><div><span style="font-size: 10pt;">■in vivo: インビボ。「生体内での」を意味する。動物実験や薬の治験とか。</span></div><div><span style="font-size: 10pt;">■in silico: インシリコ。「コンピュータを用いて」を意味する。バイオインフォでよく使われる。機械学習モデルなどからバーチャルで作成した化合物はインシリコでの化合物。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■スクリーニング: 不特定多数の中から特定のものを選ぶこと。ふるい分け。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■アッセイ: 検査、試験のこと</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■コンタミ（コンタミネーション）: 検体やデータに他のデータや不純物が混じること</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■正規化: 数値の取りえる範囲をそろえる変換処理（前処置）</span></div><div><font style="font-size: 10pt;"><br/></font></div><hr/><div><span style="font-size: 10pt; font-weight: bold;">統計系</span></div><div><span style="font-size: 10pt;">■R: 統計解析向けのプログラミング言語</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ROracle &lt;</span><a href="https://cran.r-project.org/web/packages/ROracle/index.html" rev="en_rl_none" style="font-size: 10pt;">https://cran.r-project.org/web/packages/ROracle/index.html</a><span style="font-size: 10pt;">&gt;：Oracleドライバをサポートする、オープン・ソースのRパッケージ。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■dplyr：データフレームの操作に特化したRパッケージ。SQLとは文法が異なる。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Shiny: RのWeb アプリを作るためのフレームワーク。Shiny webサイト&lt;</span><a href="http://shiny.rstudio.com/articles/" rev="en_rl_none" style="font-size: 10pt;">http://shiny.rstudio.com/articles/</a><span style="font-size: 10pt;">&gt;</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■SAS（Statistical Analysis System）:統計計算ソフト。有償ライセンス。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ノンパラメトリック：解析対象のデータの分布を仮定しないこと。パラメトリックはデータの分布をガウス分布などに仮定する</span></div><div><font style="font-size: 10pt;"><br/></font></div><div>■質的変数: 性別、職業、配偶者の有無など、一般に数や量で測れない変数</div><div>身長など数で表せるものは量的変数という</div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■クロス集計：複数条件掛け合わせた集計表</span></div><div><span style="font-size: 10pt;">エクセルだとピボットテーブルのこと</span></div><div><span style="font-size: 10pt;">例. 各社員の性別と年齢情報があるとき、女性で10代はn人、男性で30代はm人みたいな集計表</span></div><div><span style="font-size: 10pt;">2つの質的変数を比較する（年齢と性別など）場合はクロス集計表で数をカウントしてカイ二乗検定や線形回帰モデルで比較する</span></div><div><img src="用語_files/Image [7].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div>ちなみに、量的変数の場合は共分散や相関係数で関係性を比較する</div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■単回帰：説明変数が1つだけの回帰（数値予測）。教師データからy=ax+b の式を作成すること。回帰はy:目的変数、x:説明変数どちらも数値データ。 バイアス+node1個の1層+lossがl2 loss+活性化関数なしのニューラルネット。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■最小二乗法：実際のyと予測値との差の合計が最小になる直線を導く方法。回帰の最適化方法。平均二乗誤差（MSE=l2 loss）と同じ式。</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [8].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt; font-weight: bold;">最小二乗法なら過学習しないということはない!!!</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [9].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">上記みたいな直線の式ではなく、下記のような</span><span style="font-size: 10pt; font-weight: bold;">サンプルでしかfitしないモデル（過学習）</span><span style="font-size: 10pt;">になる場合もある。</span></div><div><span style="font-size: 10pt;">下記の場合は、</span><span style="font-size: 10pt; font-weight: bold;">各説明変数の重みの値がめっちゃ大きいため、モデルはグニャグニャの式になっている</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [10].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt; font-weight: bold;">線形回帰モデルの過学習を抑える=各説明変数の重みの大きさを抑える手法を取り入れたのが、Lasso（L1正則化）回帰やRidge(L2正則化)</span></div><div><span style="font-size: 10pt; font-weight: bold;">各説明変数の重み=βn の絶対値の和や二乗したものの和を線形モデルの式に足し算することで過学習を抑える</span></div><div><span style="font-size: 10pt; font-weight: bold;">y =</span> <span style="font-size: 10pt; font-weight: bold;">β0 + </span><span style="font-size: 10pt; font-weight: bold;">β1x1 + β2x2 + p(β)</span></div><div><span style="font-size: 10pt; color: rgb(255, 0, 0); font-weight: bold;">→各重みの値βが大きいほど、線形モデルからの値は小さくしないと、正解のyにfitできなくなり、結果βの値は小さく抑えられる</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [11].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://www.youtube.com/watch?v=KNE-BUKGyDk" style="font-size: 10pt;">https://www.youtube.com/watch?v=KNE-BUKGyDk</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■決定係数R^2:回帰式の指標。定義は相関係数Rを二乗したもの。1に近いほど高精度。導出式は1-(残差（実際の値-予測値）の合計/偏差（実際の値-実際の値の平均値）の合計)。</span></div><div><span style="font-size: 10pt;">決定係数は説明変数が多いほど増加することが知られているため、重回帰の場合は自由度調整済み決定係数の方が望ましい</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">データ分析手法の理論と適用_3章 より</span></div><div><a href="https://drive.google.com/drive/u/1/folders/1MRedc-1z3WUopNNvCScL6qgfaVdyUVAz" style="font-size: 10pt;">https://drive.google.com/drive/u/1/folders/1MRedc-1z3WUopNNvCScL6qgfaVdyUVAz</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■自由度調整済み決定係数(adjusted R-square)：説明変数多いとき（=重回帰の時）に使う補正した決定係数。（決定係数は説明変数の数が増えるほど1に近づくという性質を持っているため）</span></div><div><span style="font-size: 10pt;">自由度(=説明変数の数-1)を考慮した決定係数。</span></div><div><span style="font-size: 10pt;">決定係数なので回帰式の当てはまりの良さを表すことは変わりない。</span></div><div><span style="font-size: 10pt;">自由度調整済み決定係数は次の式から求められます。</span></div><div><span style="font-size: 10pt;">nはサンプルサイズを、kは説明変数の数を表します。</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [12].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://bellcurve.jp/statistics/course/9706.html" style="font-size: 10pt;">https://bellcurve.jp/statistics/course/9706.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■重回帰：説明変数が複数の回帰。教師データからy=ax1+bx2+cx3+…+bb の式を作成すること。バイアス+nodeがn個の1層+lossがl2 loss+活性化関数なしのニューラルネット。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■数理化理論I類：説明変数xが数値でない回帰。xは質的データ。たとえば、「男、女」や「好き、嫌い」など。xをダミー変数に変換したら回帰で解ける。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ダミー変数：onehotにして1列削除した変数。1列削除する理由は1列以外のonehotラベル決まっているので消した列が他の列から自動でわかるため。たとえば、性別についてラベルが「男」を「1,0」とonehot化して1列削除し、ラベルを「1」だけにする。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■判別分析：目的変数yが数値でない回帰。yは質的データ。たとえば、「男、女」や「好き、嫌い」など。yをダミー変数に変換したら回帰で解ける。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ロジスティクス回帰：目的変数の値が確率値の回帰。回帰だがyが確率なので分類クラスの閾値を決めて分類のとき使う。y=ax1+bx2+cx3+…+bb の式を作成し、ロジスティクス関数（sigmid）:p(y)=1/(1+e^-y)をかけてyを確率値にする。1層+活性化関数sigmoid+lossがbinary cross entropyのニューラルネット。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■決定木：目的変数y、説明変数xどちらも数値でない（質的データ）でも使える分析手法。教師データから最適な条件分岐をツリー形式で作成するモデル。（決定木はモデルであってアルゴリズムの名前ではない）</span></div><div><span style="font-size: 10pt;">ちなみに、ランダムフォレストは決定木のバギング（教師データを重複ありでランダムサンプリングしたモデルの多数決）</span></div><div><span style="font-size: 10pt;">決定木は過学習になりやすい（教師データにフィットしやすい）ため木に制限をつける。</span></div><div><span style="font-size: 10pt;">方針としては「複雑にならないように木の深さを制限する」「木を生成した後に枝を剪定する」といったアプローチがある。</span></div><div><span style="font-size: 10pt;">例.各末端の枝に2つだけデータを持つ決定木</span></div><div><span style="font-size: 10pt;">　</span> <a href="https://qiita.com/woody_egg/items/232e982094cd3c80b3ee" style="font-size: 10pt;">https://qiita.com/woody_egg/items/232e982094cd3c80b3ee</a></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [13].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■決定木の作り方：</span></div><div><span style="font-size: 10pt;">①学習データが全てルートノードに集める</span></div><div><span style="font-size: 10pt;">②そのデータの持つ素性の中で集められたデータを一番よく分割する素性と閾値の組を選ぶ</span></div><div><span style="font-size: 10pt;">③その素性と閾値で分割後，またそれぞれのノードで分割を繰り返す</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■エントロピー：物事の乱雑さを測る指標</span></div><div><span style="font-size: 10pt;">不純度が最も低ければエントロピーの値は0，不純度が高くなればなるほどエントロピーの値が大きくなる</span></div><div><span style="font-size: 10pt;">（不純度が最も低い=ノード内のサンプルが全て同じクラスに属している(10個中10個Aクラス)、不純度が最も高い=ノード内のサンプルが全て異なるクラスに属している(10個中各1個ずつA,B,C,D…クラス)）</span></div><div><a href="https://qiita.com/3000manJPY/items/ef7495960f472ec14377" style="font-size: 10pt;">https://qiita.com/3000manJPY/items/ef7495960f472ec14377</a></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [14].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ジニ係数：「偏り」や「不均等さ」を数値で表したもの</span></div><div><span style="font-size: 10pt;">不純度が最も低ければジニ係数の値は0，不純度が高くなればなるほどジニ係数の値が大きくなる</span></div><div><span style="font-size: 10pt;">（不純度が最も低い=ノード内のサンプルが全て同じクラスに属している(10個中10個Aクラス)、不純度が最も高い=ノード内のサンプルが全て異なるクラスに属している(10個中各1個ずつA,B,C,D…クラス)）</span></div><div><span style="font-size: 10pt;">ジニ係数はもともと計量経済学の分野で社会における所得分配の均衡・不均衡を表すものとして使われているものらしい</span></div><div><a href="https://qiita.com/3000manJPY/items/ef7495960f472ec14377" style="font-size: 10pt;">https://qiita.com/3000manJPY/items/ef7495960f472ec14377</a></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [15].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■決定木の分割アルゴリズム</span></div><div><span style="font-size: 10pt;">①C4.5：エントロピーを分割指標として決定木を構築していく分類アルゴリズム</span></div><div><span style="font-size: 10pt;">分岐前のエントロピーと分岐後のエントロピーの合計との差が最大になるような素性と閾値の組（不純度を最大限減らす組）を探す</span></div><div><a href="https://qiita.com/3000manJPY/items/ef7495960f472ec14377" style="font-size: 10pt;">https://qiita.com/3000manJPY/items/ef7495960f472ec14377</a></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [16].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">②CART：ジニ係数を分割指標として決定木を構築していく回帰アルゴリズム</span></div><div><span style="font-size: 10pt;">分岐前のジニ係数と分岐後のジニ係数の合計との差が最大になるような素性と閾値の組（不純度を最大限減らす組）を探す</span></div><div><a href="https://qiita.com/3000manJPY/items/ef7495960f472ec14377" style="font-size: 10pt;">https://qiita.com/3000manJPY/items/ef7495960f472ec14377</a></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [17].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">③カイ二乗値で分ける</span></div><div><span style="font-size: 10pt;">■カイ二乗値：実測値と期待値の差。（実測値-期待度数）^2/期待度数。</span></div><div><span style="font-size: 10pt;">実際のクロス集計値と予測のクロス集計値（=期待度数）を比較して出す。</span></div><div><span style="font-size: 10pt;">カイ二乗値がでかい=実測値と期待値が大きく違う=そのパラメータの要因が大きい=p値が小さい。</span></div><div><span style="font-size: 10pt;">検定の考え方からp値が小さいパラメータの条件をツリーにしていく。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■k-means:教師なしのクラスター分析。n個の説明変数をn次元の空間plotして指定したクラスター数にデータ分類する。分け方はクラスタ内で重心計算→最も近い重心に分類を繰り返す。</span></div><div><span style="font-size: 10pt;"><br/></span></div><div><span style="font-size: 10pt;">■Pseudo F(</span><span style="font-size: 13px;">スゥードエフ</span><span style="font-size: 10pt;">): クラスタリングの評価指標。クラスタ内のデータの凝集性だけでなく、クラス間の凝集性も考慮した値。</span></div><div><span style="font-size: 10pt;">Psudo F = ( (&lt;全データの距離の２乗和&gt; - &lt;クラスタ内の距離の２乗和&gt;) / (&lt;クラスタ数&gt; - 1) ) / ( &lt;</span><span style="font-size: 10pt;">クラスタ内の距離の２乗和</span><span style="font-size: 10pt;">&gt; / (全データ数 - &lt;クラスタ数&gt;) )</span></div><div><span style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">→クラスタ間のばらつき / クラスタ内のばらつき の式になってる</span></span></div><div><span style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">良いクラスタリングは「クラスタ同士はばらついており、クラスタ内はまとまっている」と定義できるため、値が大きいと良いクラスタリングと言える</span></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■エルボー法: </span><span style="font-size: 10pt;">クラスタリングの評価指標。</span><span style="font-size: 13px;">クラスタ内の距離の２乗和をプロットした図（それだけ）</span></div><div><span style="font-size: 13px;">クラスタ内の距離の２乗和が&quot;ヒジ&quot;のようにガクンと曲がった点（値の低下がサチる場所）が最適なクラスター数とみなす</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■主成分分析(PCA):次元削減。分散が最大（各データ点の間が最大＝データの違いがはっきりしている）になるように軸を引き直して次元を1つ減らす。これを繰り返して指定の次元まで次元を減らす手法。次元減らすのはx,y空間でz=ax+byのz軸を引き直すこと。PCAではzが最もばらつくa,bを決めて軸を引いている。</span></div><div><span style="font-size: 10pt;">PCAの累積寄与率は新しい軸に元の情報を何%残すことができたかを示す。</span></div><div><span style="font-size: 10pt;">→モデルの精度向上には機能しない（本や経験上より）</span></div><div><span style="font-size: 10pt;">→</span><span style="font-size: 10pt; font-weight: bold;">主成分分析の目的は情報集約（</span><span style="font-size: 10pt;">して可視化）</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">データ分析手法の理論と適用_4章 より</span></div><div><a href="https://drive.google.com/drive/u/1/folders/1MRedc-1z3WUopNNvCScL6qgfaVdyUVAz" style="font-size: 10pt;">https://drive.google.com/drive/u/1/folders/1MRedc-1z3WUopNNvCScL6qgfaVdyUVAz</a></div><div><font style="font-size: 10pt;"><br/></font></div><div>■因子分析: 複数の変数間の関係性を探る際によく用いられる手法</div><div>「ある観測された変数（たとえば質問項目）が，どのような潜在的な因子から影響を受けているか」を探る手法</div><div><span style="font-weight: bold;">因子分析の目的は各因子に関連する共通因子を探ること</span></div><div>例.２つの因子（文系能力と理系能力）が見いだされる場合</div><div><img src="用語_files/Image [18].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div>数学の得点が「文系能力」と「理系能力」はどの教科にも影響を及ぼす因子である場合，「文系能力」と「理系能力」は「共通因子」</div><div>数学「だけ」に影響を及ぼす因子もある場合、このような因子を「独自因子」という</div><div><img src="用語_files/Image [19].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div><a href="http://psy.isc.chubu.ac.jp/~oshiolab/teaching_folder/datakaiseki_folder/08_folder/da08_01.html">http://psy.isc.chubu.ac.jp/~oshiolab/teaching_folder/datakaiseki_folder/08_folder/da08_01.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■確率変数：事象の確率によって定義された変数。さいころの出目など。さいころの出目はさいころを振ることで値が決まるため確率変数と言える。</span><span style="font-size: 10pt; font-weight: bold;">確率変数は平均値や分散を計算するために定義された概念でもあるため数値</span><span style="font-size: 10pt;">。コイントスの確率変数は表=1,裏=0のようにする</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■確率分布：各確率変数（さいころの出目など）の確率がどう分布するかを示したもの。y軸確率、x軸確率変数のグラフや表（表の場合は確率分布表ともいう）。</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [20].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■分散：平均値を中心にして確率変数の値がどれくらい広がっているかを表す数値。標準偏差σは分散のルート</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■確率密度関数：確率分布を表す関数。</span></div><div><span style="font-size: 10pt;">＜確率密度関数を導入する理由＞</span></div><div><span style="font-size: 10pt;">例えば工場で生産される部品が必ず100gか言い切れない。</span></div><div><span style="font-size: 10pt;">誤差が生じるはずなので、重さを測ることは確率的な行為。</span></div><div><span style="font-size: 10pt;">このため製品の重さは確率変数になる。</span></div><div><span style="font-size: 10pt;">重さは連続値なので、変数として表すと関数になる。</span></div><div><span style="font-size: 10pt;">それを確率密度関数と呼ぶ。</span></div><div><span style="font-size: 10pt; font-weight: bold;">関数なので積分すると確率値が取り出せることが確率密度関数のメリット</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Scannable の文書 (2020-05-18 21_39_38).png" type="image/png" data-filename="Scannable の文書 (2020-05-18 21_39_38).png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■正規分布：標本の平均の分布。総和が1、1σ=68%、2σ=95%、3σ=99.7%の確率分布</span></div><div><span style="font-size: 10pt;">正規分布は、異なる正規分布（平均や分散が違う正規分布）同士の足し算は正規分布になる特徴がある(「再生性」と呼ばれる性質)</span></div><div><span style="font-size: 10pt;">ただし、正規分布の2乗の足し算は正規分布にならない。別の分布に従う。2乗の場合はカイ二乗分布に従う</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Shapiro-Wilk（シャピロ-ウィルク）検定：データが正規分布しているか判断するための検定</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Q-Qプロット：データが正規分布かどうかを判断するためのグラフ</span></div><div><span style="font-size: 10pt;">実測値を対象に</span></div><div><span style="font-size: 10pt;">・実際に発生した確率の累積</span></div><div><span style="font-size: 10pt;">・正規分布に従って発生した場合の確率の累積</span></div><div><span style="font-size: 10pt;">を比較して、比例の関係になるかどうかを描画するグラフ</span></div><div><span style="font-size: 10pt;">理想(正規分布)と現実(実測値)の比例状態を見るので、</span><span style="font-size: 10pt; font-weight: bold;">Q-Qプロットが直線に近ければ正規分布であると言える</span></div><div><span style="font-size: 10pt;">ヒストグラムで正規分布かわかる気がするが、ヒストグラムは取るビンの大きさによって形が変わるため、ヒストグラムだけで正規分布かは言い切れない</span></div><div><span style="font-size: 10pt;">検定で正規分布か推定する場合はシャピロ-ウィルク検定を使う</span></div><div><span style="font-size: 10pt;">→シャピロ-ウィルク検定はQ-Qプロットが直線に従っているのかを見ている</span></div><div><a href="https://sigma-eye.com/2020/02/23/shapiro-wilk/" style="font-size: 10pt;">https://sigma-eye.com/2020/02/23/shapiro-wilk/</a></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [21].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■中心極限定理：「母集団の分布にかかわらずその標本平均は正規分布する」という定理。要するに無限回標本抽出して全標本の平均の確率分布を見ると必ず正規分布になるってこと。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■大数の法則：サンプルサイズを大きくすると標本平均は母平均に収束する定理</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ベルヌーイ分布：</span></div><div><span style="font-size: 10pt;">成功か失敗かのような 2 つの結果だけが生起する試行をベルヌーイの試行いう．この試行で，成功なら 1，失敗なら 0 の値をとる確率変数を X とする．</span></div><div><span style="font-size: 10pt;">成功する確率を p，失敗する確率を q = 1 − p とするとき，確率変数 X はベルヌーイ分布に従うという</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [22].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">ベルヌーイ分布は再生性無い。ベルヌーイ分布の足し合わせは二項分布になる</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■二項分布：ベルヌーイ試行をn回行って、成功する回数Xが従う確率分布</span></div><div><span style="font-size: 10pt;">コイントスなど。試行回数多いと正規分布になる。確率が小さいときはポアソン分布に近づく。</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [23].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■幾何分布：ベルヌーイ施行を繰り返して、初めて成功するまでの試行回数が従う確率分布。f(x) = p*(1-p)^(x-1) (x=1,2,3,…)</span></div><div><span style="font-size: 10pt;">例.コインを表出るまで投げる(p=1/2のベルヌーイ施行)回数</span></div><div><span style="font-size: 10pt;">→五回目で初めて表が出る確率は P(x=5) = (1/2)*(1 - 1/2)^(5-1) = 1/32</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ポアソン分布：まれに起こる現象の確率分布。ある期間に平均λ回起こる現象が、ある期間にX回起きる確率分布</span></div><div><span style="font-size: 10pt;">今日起こる車の事故件数を評価するときとかに使える</span></div><div><span style="font-size: 10pt;">ポアソン分布も正規分布と同じく再生性を持つ</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [24].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [25].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [26].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [27].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">ポアソン分布は平均λがわかれば使える（正規分布は平均と分散が必要だがポアソン分布は1つのパラメータでいい）</span></div><div><span style="font-size: 10pt;">λはポアソン分布の分散でもある</span></div><div><a href="https://www.youtube.com/watch?v=LTYmA_mTltM" style="font-size: 10pt;">https://www.youtube.com/watch?v=LTYmA_mTltM</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■</span><span style="font-size: 10pt; font-weight: bold;">p値</span><span style="font-size: 10pt;">:帰無仮説のもとで検定統計量が</span><span style="font-size: 10pt; font-weight: bold;">帰無仮説になる確率</span><span style="font-size: 10pt;">のこと。</span><span style="font-size: 10pt; font-weight: bold;">偶然かどうかを表す指標</span><span style="font-size: 10pt;">。</span></div><div><span style="font-size: 10pt;">例. 「コイントス16/20回表になった結果は偶然か？」を検定する場合</span></div><div><span style="font-size: 10pt;">帰無仮説=「コイントス16/20回表は偶然である（=16/20回表にならない）」</span></div><div><span style="font-size: 10pt;">帰無仮説になる確率であるp値は 1.0 -  (0.5)^16 = 0.9941</span></div><div><span style="font-size: 10pt;">※16/20回表になる確率=(0.5)^16 の逆が「16/20回表にならない」確率であるため1引いてる</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">p値が小さいほどその値は珍しいことを示す。</span></div><div><span style="font-size: 10pt;">t値から取った確率分布の面積に相当する。</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [28].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■t値：t分布の中心からp値のx座標までの長さ。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■t分布：サンプル数が少ないときや母集団の分散が未知のときに使う確率分布。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■カイ二乗分布：標本の分散の分布。自由度(n-1)が大きいと正規分布になる。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■F分布：標本の分散比の分布。多変数の検定である分散分析で使われる。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■区間推定：予測値を幅を持たせて推定する方法。予測値を一意に決めるのは難しいため、現象の確率（=当たる確率）とその確率が許容する予測値の範囲を求める。明日の1ドルは120円ぐらいとあいまい予測するところを、区間推定は99%の確率で明日の1ドルは100-140の間と推定する。100-140の間といった予測誤差の範囲を信頼区間という。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■標準偏差、標本誤差、標準誤差</span></div><div>→標準偏差：<span style="font-weight: bold;">標本内のサンプルのばらつき</span>．１群から計算される</div><div><br/></div><div>→標本誤差: 標本から母集団における数値を推定するときに伴う誤差</div><div><span style="font-weight: bold;">「標本誤差」＝「標本値」ー「母集団値」</span></div><div>(=母集団のすべてを調査しないで、一部の標本を無作為抽出して調査した結果にともなう誤差=<span style="font-weight: bold;">標本ごとのばらつき</span>)</div><div><img src="用語_files/Image [29].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div><a href="https://www.intage.co.jp/glossary/041/">https://www.intage.co.jp/glossary/041/</a></div><div><br/></div><div>→標準誤差：<span style="font-weight: bold;">各標本の平均値のばらつき</span>．同じ母集団から得られた（と想定される）<span style="font-weight: bold;">何度も標本抽出した場合にだけ計算される</span>．</div><div><img src="用語_files/Image [30].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div><a href="https://www.yodosha.co.jp/jikkenigaku/statistics/q2.html#:~:text=1%EF%BC%89%E6%A8%99%E6%BA%96%E5%81%8F%E5%B7%AE%E3%81%A8%E6%A8%99%E6%BA%96%E8%AA%A4%E5%B7%AE&amp;text=%E6%A8%99%E6%BA%96%E5%81%8F%E5%B7%AE%EF%BC%9A%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%81%AE%E3%81%B0%E3%82%89%E3%81%A4%E3%81%8D%2C%E3%81%AB%E3%81%A0%E3%81%91%E8%A8%88%E7%AE%97%E3%81%95%E3%82%8C%E3%82%8B%EF%BC%8E">https://www.yodosha.co.jp/jikkenigaku/statistics/q2.html#:~:text=1%EF%BC%89%E6%A8%99%E6%BA%96%E5%81%8F%E5%B7%AE%E3%81%A8%E6%A8%99%E6%BA%96%E8%AA%A4%E5%B7%AE&amp;text=%E6%A8%99%E6%BA%96%E5%81%8F%E5%B7%AE%EF%BC%9A%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%81%AE%E3%81%B0%E3%82%89%E3%81%A4%E3%81%8D,%E3%81%AB%E3%81%A0%E3%81%91%E8%A8%88%E7%AE%97%E3%81%95%E3%82%8C%E3%82%8B%EF%BC%8E</a></div><div><br/></div><div><br/></div><div><span style="font-size: 10pt;">■有意水準：検定において帰無仮説を設定したときにその帰無仮説を棄却する基準となる確率</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■検定：仮説を確率分布から棄却するか決める手法。たとえば、「カラスはすべて黒いか？」を（フェルミ）推定するようなの。帰無仮説を「99%のカラスは黒い」として1000羽サンプリング。1000羽すべて黒だった。帰無仮説の確率的には0.99^1000=0.00004(0.004%)なので非常に低確率といえる（推定ではこの値を使用する確率分布からだしたp値を使う）→帰無仮説間違ってそう というように帰無仮説を棄却する。</span></div><div><span style="font-size: 10pt;">※</span><span style="font-size: 10pt; font-weight: bold;">検定においては「母集団の平均や分散がある値に等しい」と仮定して、その標本の分布がどうなるかを調べる</span><span style="font-size: 10pt;">。</span></div><div><span style="font-size: 10pt;">母集団の平均や分散についての仮説から標本平均がある値となる確率を計算し、めったに起こらない確率になれば元の仮説が間違っているという論法をとる。</span></div><div><span style="font-size: 10pt;">検定の例：発生確率p の事象がN 回起きたことが偶然か調べる</span></div><div><span style="font-size: 10pt;">（「じゃんけん10連敗は偶然か？」「巨人が10連勝したのは偶然か？」みたいなケース）</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt; font-weight: bold;">①帰無仮説を決める。※帰無仮説は否定したいことにする</span></div><div><span style="font-size: 10pt;">　→N回起きたことは偶然でない（「じゃんけん10連敗は偶然でない」「巨人が10連勝したことは偶然でない」）</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt; font-weight: bold;">②P値を計算する。※P値は帰無仮説の確率</span></div><div><span style="font-size: 10pt;">　→P値 = p^N （今回の場合は確率p がN回連続で発生するからべき乗）</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt; font-weight: bold;">③P値がしきい値よりも小さいか比較して、帰無仮説棄却できるか判定する</span></div><div><span style="font-size: 10pt;">　帰無仮説を棄却するしきい値 = 5% としたとき</span></div><div><span style="font-size: 10pt;">　p^N &lt; 0.05 なら、帰無仮説棄却（= N回起きたことは偶然であると言える）</span></div><div><span style="font-size: 10pt;">　</span><span style="font-size: 10pt; font-weight: bold;">→じゃんけんの場合はp=0.5 なので、0.5^10 ～ 0.001 &lt; 0.05 となり、10連敗は偶然と言える</span></div><div><span style="font-size: 10pt; font-weight: bold;">　→巨人の場合の条件式は、p^10 &lt; 0.05 なので、</span></div><div><span style="font-size: 10pt; font-weight: bold;">　　p=0.4（=巨人の勝率が4割とした）の時は、0.4^10 ～ 0.1*10^3 &lt; 0.05 となり、10連勝は偶然と言える</span></div><div><span style="font-size: 10pt; font-weight: bold;">　　p=0.94（=巨人の勝率が9.4割とした）の時は、0.94^10 ～ 0.53 &gt; 0.05 となり、10連勝は偶然でないと言える</span></div><div><span style="font-size: 10pt; font-weight: bold;">　つまり、サンプル数=10の時は、発生確率p=94%以上ならば棄却できると言える</span></div><div><span style="font-size: 10pt; font-weight: bold;">　サンプル数=100の時は、発生確率p=99.4%以上ならば棄却できる</span></div><div><span style="font-size: 10pt; font-weight: bold;">　→明日も生きてる確率=99.3%と仮定して、調査した100人全員生きてる場合は、P値=0.993^100=0.495なので、帰無仮説（明日も生きてる確率=99.3%である）を棄却できる</span></div><div><span style="font-size: 10pt; font-weight: bold;">　　10人調査では、P値=0.993^10=0.93なので、帰無仮説を棄却できない</span></div><div><span style="font-size: 10pt; font-weight: bold;">　このようにサンプル数は検定の結果に大きく影響する</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■z検定：正規分布使った検定。自然現象はほぼ正規分布に従うため検定といえばほぼこれ。n人サンプリングしてコールセンターの許容待ち時間の区間推定とかで使える。標準正規分布（平均0,標準偏差1の正規分布）の値をz値という。</span></div><div><span style="font-size: 10pt;">z検定は標準化（平均を０、標準偏差を1になるように整形）した値を元に、</span></div><div><span style="font-size: 10pt;">z(標準正規)分布によって生起確率を求め、それが分布のどこに位置するのかを求め、標本平均から推定された値が母平均値に等しいかを判断するための検定。</span></div><div><span style="font-size: 10pt;">よって分布が正規分布性を持ち、母平均か母分散が既知でなければならない。</span></div><div><span style="font-size: 10pt;">しかし、標本数が多く(標本数 &gt; 30)あるのであれば、中心極限定理により母集団の分布が正規分布と仮定できるので、母分散が未知の場合であっても区間推定に利用することができる。</span></div><div><span style="font-size: 10pt;">→zが1.96より大きい時、あるいは-1.96より小さい時にμは、その標本から推定される平均 mとは有意差があると結論し、帰無仮説μ=mを棄却する（1.96より大きいと差があるってこと）</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [31].png" type="image/png" data-filename="Image.png" title="Attachment"/>m は標本平均、μ は母平均、σ は母標準偏差、n は標本数</span></div><div><span style="font-size: 10pt;">例：</span><span style="font-size: 10pt; font-weight: bold;"> </span><span style="font-size: 10pt;">(政策支持率)</span></div><div><span style="font-size: 10pt;">ある放送局が 1600 人を無作為に選び、ある税制改正案を支持するか否かを調べたところ 850 人 (53.1%) が支持すると答えた。この改正案は国民の半数を超える支持を得ているといえるかについて検定する。</span><span style="font-size: 10pt; font-weight: bold;">（サンプル数が多いので中心極限定理により正規分布にfitするはずだからz検定使える）</span></div><div><span style="font-size: 10pt;">H0 : p = 0.5 対 H1 : p &gt; 0.5</span></div><div><span style="font-size: 10pt;">を有意水準 α = 0.05 で検定する。</span></div><div><span style="font-size: 10pt;">Z =(0.531−0.50)/√(0.5x0.5x1600) = 2.50</span></div><div><span style="font-size: 10pt;">であり、z(0.05) = 1.645 であるから、帰無仮説 H0 は棄却される。</span></div><div><span style="font-size: 10pt;">すなわち、国民の半数を超える支持が得られていると判断される。</span></div><div><a href="https://lecture.ecc.u-tokyo.ac.jp/~chkurata/0709_July9.pdf" style="font-size: 10pt;">https://lecture.ecc.u-tokyo.ac.jp/~chkurata/0709_July9.pdf</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■t検定：母集団が正規分布と仮定したときのt分布を使った検定。</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [32].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://www.amed.go.jp/content/000034160.pdf" style="font-size: 10pt;">https://www.amed.go.jp/content/000034160.pdf</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">t検定は主に2群の母平均の差を検定する（差があるのは偶然ではないかを決める検定）</span></div><div><span style="font-size: 10pt;">3群以上でそれぞれの群同士で差があるかの検定はt検定ではできない。</span></div><div><span style="font-size: 10pt;">　→少なくとも1つの組み合わせに差がある確率が高まるため。</span></div><div><span style="font-size: 10pt;">　　たとえば、A,B,Cの3群の場合</span></div><div><span style="font-size: 10pt;">　　少なくとも1つの組み合わせに差がある確率=(A,B間に差がある確率)+(A,C間に差がある確率)+(B,C間に差がある確率)</span></div><div><span style="font-size: 10pt;">　　となり確率が高まる</span></div><div><span style="font-size: 10pt;">　3群以上は分散分析になる</span></div><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">t検定は対応のないt検定（</span><span style="font-size: 10pt; font-weight: bold;">スチューデントのt検定 or </span><span style="font-size: 10pt; font-weight: bold;">ウィルチのt検定</span><span style="font-size: 10pt; font-weight: bold;">）と対応のあるt検定に分けらる</span></font></div><div><span style="font-size: 10pt;">対応があるとは、同じ個体群で繰り返し測定したデータの場合。反復測定したデータ。例えば、ある10人について、A群は去年の体重, B群は今年の体重 だと対応ありとなる。</span></div><div><span style="font-size: 10pt;">t検定は以下の表に従って使い分ける</span></div><table style="border-collapse: collapse; min-width: 100%;"><colgroup><col style="width: 268px;"></col><col style="width: 268px;"></col></colgroup><tbody style="box-sizing: border-box;"><tr style="box-sizing: border-box; background-color: rgb(249, 249, 249);"><td style="box-sizing: border-box; text-align: left; vertical-align: top; white-space: nowrap; background-color: rgb(43, 75, 101); width: 268px; padding: 8px; border: 1px solid;"><div><span style="font-size: 10pt; color: rgb(85, 85, 85); font-family: Arial, &quot;ヒラギノ角ゴ Pro W3&quot;, &quot;Hiragino Kaku Gothic Pro&quot;, メイリオ, Meiryo, Osaka, &quot;MS PGothic&quot;, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 1.42857;">状況</span></div></td><td style="box-sizing: border-box; text-align: left; vertical-align: top; white-space: nowrap; background-color: rgb(43, 75, 101); width: 268px; padding: 8px; border: 1px solid;"><div><span style="font-size: 10pt; color: rgb(85, 85, 85); font-family: Arial, &quot;ヒラギノ角ゴ Pro W3&quot;, &quot;Hiragino Kaku Gothic Pro&quot;, メイリオ, Meiryo, Osaka, &quot;MS PGothic&quot;, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 1.42857;">適用すべきt検定</span></div></td></tr><tr style="box-sizing: border-box;"><td style="box-sizing: border-box; vertical-align: top; white-space: nowrap; width: 268px; padding: 8px; border: 1px solid;"><div style="background-color: rgb(255, 255, 255);"><span style="font-size: 10pt; color: rgb(85, 85, 85); font-family: Arial, &quot;ヒラギノ角ゴ Pro W3&quot;, &quot;Hiragino Kaku Gothic Pro&quot;, メイリオ, Meiryo, Osaka, &quot;MS PGothic&quot;, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 1.42857;">データに対応がある．</span></div></td><td style="box-sizing: border-box; vertical-align: top; white-space: nowrap; width: 268px; padding: 8px; border: 1px solid;"><div style="background-color: rgb(255, 255, 255);"><a href="https://data-science.gr.jp/implementation/ist_r_welch_t_test.html" style="font-size: 10pt; color: rgb(85, 85, 85); font-family: Arial, &quot;ヒラギノ角ゴ Pro W3&quot;, &quot;Hiragino Kaku Gothic Pro&quot;, メイリオ, Meiryo, Osaka, &quot;MS PGothic&quot;, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 1.42857; text-decoration: underline;">対応のあるt検定</a></div></td></tr><tr style="box-sizing: border-box; background-color: rgb(249, 249, 249);"><td style="box-sizing: border-box; vertical-align: top; white-space: nowrap; width: 268px; padding: 8px; border: 1px solid;"><div style="background-color: rgb(255, 255, 255);"><span style="font-size: 10pt; color: rgb(85, 85, 85); font-family: Arial, &quot;ヒラギノ角ゴ Pro W3&quot;, &quot;Hiragino Kaku Gothic Pro&quot;, メイリオ, Meiryo, Osaka, &quot;MS PGothic&quot;, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 1.42857;">データに対応がなく，2群間に等分散性が仮定できる．</span></div></td><td style="box-sizing: border-box; vertical-align: top; white-space: nowrap; width: 268px; padding: 8px; border: 1px solid;"><div style="background-color: rgb(255, 255, 255);"><a href="https://data-science.gr.jp/implementation/ist_r_student_t_test.html" style="font-size: 10pt; color: rgb(85, 85, 85); font-family: Arial, &quot;ヒラギノ角ゴ Pro W3&quot;, &quot;Hiragino Kaku Gothic Pro&quot;, メイリオ, Meiryo, Osaka, &quot;MS PGothic&quot;, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 1.42857; text-decoration: underline;">スチューデントのt検定</a></div></td></tr><tr style="box-sizing: border-box;"><td style="box-sizing: border-box; vertical-align: top; white-space: nowrap; width: 268px; padding: 8px; border: 1px solid;"><div style="background-color: rgb(255, 255, 255);"><span style="font-size: 10pt; color: rgb(85, 85, 85); font-family: Arial, &quot;ヒラギノ角ゴ Pro W3&quot;, &quot;Hiragino Kaku Gothic Pro&quot;, メイリオ, Meiryo, Osaka, &quot;MS PGothic&quot;, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 1.42857;">データに対応がなく，2群間に等分散性が仮定できない．</span></div></td><td style="box-sizing: border-box; vertical-align: top; white-space: nowrap; width: 268px; padding: 8px; border: 1px solid;"><div><span style="font-size: 10pt; color: rgb(85, 85, 85); font-family: Arial, &quot;ヒラギノ角ゴ Pro W3&quot;, &quot;Hiragino Kaku Gothic Pro&quot;, メイリオ, Meiryo, Osaka, &quot;MS PGothic&quot;, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 1.42857;">ウェルチのt検定</span></div></td></tr></tbody></table><div><font style="font-size: 10pt;"><br/></font></div><div><a href="https://data-science.gr.jp/implementation/ist_r_welch_t_test.html" style="font-size: 10pt;">https://data-science.gr.jp/implementation/ist_r_welch_t_test.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■等分散：2つのグループの分散が等しいこと</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■スチューデントのt検定：対応のないt検定の1種</span></div><div><span style="font-size: 10pt;">（分布が正規分布であり）2群の等分散が仮定された場合のt検定</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ウィルチのt検定：対応のないt検定の1種</span></div><div><span style="font-size: 10pt;">（分布が正規分布であり）2群の不等分散（分散が異なる）が仮定された場合のt検定</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■F検定： 等分散性の検定(=2群のデータの分散が等しいかの検定)</span></div><div><span style="font-size: 10pt;">2群の平均値の差のt検定で、2群の分散が同じか知るためにつかう。</span></div><div><span style="font-size: 10pt;">t検定は、2群が同じ分散 or 違う分散 かでやり方が変わるため。</span></div><div><span style="font-size: 10pt;">例. </span><a href="https://statistics.co.jp/reference/software_R/statR_4_test.pdf" style="font-size: 10pt;">https://statistics.co.jp/reference/software_R/statR_4_test.pdf</a></div><div><span style="font-size: 10pt;">２つの高校A、Bにおいて、３年生の数学の学力に差があるかどうかを調べるため、A高校から８人、B高校から７人を無作為に選んで、実力テストを行ったところ、次のような結果を得た。</span></div><div><span style="font-size: 10pt;">A = [70, 67, 81, 92, 78, 62, 85, 73]</span></div><div><span style="font-size: 10pt;">B = [66, 75, 48, 58, 80, 57, 50]</span></div><div><span style="font-size: 10pt;">等分散（2つのグループの分散が等しい）を仮定して、t検定を行いましたが、この仮定は妥当と言えるのか？</span></div><div><span style="font-size: 10pt;">→F検定でA,Bの分散に有意差があるか検定する</span></div><div><span style="font-size: 10pt;">&lt;R&gt;</span></div><div><span style="font-size: 10pt;">A &lt;- c(70, 67, 81, 92, 78, 62, 85, 73)</span></div><div><span style="font-size: 10pt;">B &lt;- c(66, 75, 48, 58, 80, 57, 50)</span></div><div><span style="font-size: 10pt;">var.test(x=A, y=B, conf.level=0.95)  # 信頼水準を0.95に設定</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [33].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">→p値が0.05以上だから帰無仮説（2群の分散に差がない）棄却できないので、A,Bは等分散と言える</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt; font-weight: bold;">F検定はF値を使った検定。</span></div><div><span style="font-size: 10pt; font-weight: bold;">F値は2群の分散の比=σ1^2/σ2^2(※σ1&gt;σ2)</span></div><div><span style="font-size: 10pt; font-weight: bold;">F値はF分布に従う。F分布のF値が有意水準(5%とか)未満なら帰無仮説(=分散同士に差がない)を棄却とする。</span></div><div><span style="font-size: 10pt; font-weight: bold;">F検定の統計量は分散の比を検定しているので、F検定は分散分析(analysis of variance, ANOVA)とも呼ばれる。F検定ではz検定やt検定では許されなかった3群以上の検定ができる</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■z検定/t検定の例</span></div><div><span style="font-size: 10pt;">①</span><span style="font-size: 10pt; font-weight: bold;">平均の検定:標本から推定される母集団の平均値が、ある値に等しいかどうかを判定</span></div><div><span style="font-size: 10pt; font-weight: bold;">→判定する母集団の平均値、標本の値があれば計算できる</span></div><div><span style="font-size: 10pt;">→母集団の分散がわからない場合は標本から母集団の分散を求める</span></div><div><span style="font-size: 10pt;">　①サンプル数&gt;=30なら、標本の分散は母集団の分散と見なして正規分布を用いる（z検定）</span></div><div><span style="font-size: 10pt;">　②サンプル数&lt;30なら、標本の分散は標本の不変分散と見なしてt分布を用いる（t検定）</span></div><div><span style="font-size: 10pt;"> </span><a href="https://www.geisya.or.jp/~mwm48961/linear_algebra/t_test2.htm" style="font-size: 10pt;">https://www.geisya.or.jp/~mwm48961/linear_algebra/t_test2.htm</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt; font-weight: bold;">例題1:平均のｚ検定</span></div><div><span style="font-size: 10pt;">ある規格の製品は重さの平均がμ=50(g)で標準偏差がσ=3(g)となるように作られている。</span></div><div><span style="font-size: 10pt;">12個の製品を抽出して重さを測定したところ，次のようなデータを得た。</span></div><div><span style="font-size: 10pt;">50.2, 46.4, 46.2, 51.0, 51.0, 47.9, 47.9, 46.5, 46.9, 49.0, 48.8, 49.0</span></div><div><span style="font-size: 10pt;">この製品の平均の重さが50(g)であるといえるかどうか有意水準5%で検定をせよ。</span></div><div><span style="font-size: 10pt;">&lt;R&gt;</span></div><div><span style="font-size: 10pt; font-style: italic;">u &lt;- mean(c(50.2, 46.4, 46.2, 51.0, 51.0, 47.9, 47.9, 46.5, 46.9, 49.0, 48.8, 49.0))  #</span> <span style="font-size: 10pt; font-style: italic;">48.4。</span><span style="font-size: 10pt; font-style: italic;">標本平均</span></div><div><span style="font-size: 10pt; font-style: italic;">m &lt;- 50  # 母集団の平均</span></div><div><span style="font-size: 10pt; font-style: italic;">sd &lt;- 3/sqrt(12)  #</span> <span style="font-size: 10pt; font-style: italic;">母集団の</span><span style="font-size: 10pt; font-style: italic;">標準偏差/標本数^(0.5)=z値の分母</span></div><div><span style="font-size: 10pt; font-style: italic;"># pnormは正規分布の累積確率を返す</span></div><div><span style="font-size: 10pt; font-style: italic;"># pnorm(1.96, mean=0, sd=1)は標準正規分布において，-∞ から 1.96 までの累積確率になる</span></div><div><span style="font-size: 10pt; font-style: italic;"># lower.tail=TRUE なら下側確率(-∞ からの累積確率) P[X &lt;= x]、FALSE なら上側確率(+∞ からの累積)  P[X &gt; x]</span></div><div><span style="font-size: 10pt; font-style: italic; font-weight: bold;">p &lt;- pnorm(u, mean=m, sd, lower.tail=TRUE)</span></div><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-style: italic;">if (p * 2 &lt; 0.05) print(&quot;</span><span style="font-size: 10pt; font-style: italic;">帰無仮説は</span><span style="font-size: 10pt; font-style: italic;">棄却&quot;)  # 両側検定にしてるのでp*2にしている</span></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt; font-weight: bold;">例題2:平均のｚ検定</span></div><div><span style="font-size: 10pt;">ある県で模擬試験を受けた生徒から50人を無作為抽出してしたところ，平均63点，標準偏差11点であった。</span></div><div><span style="font-size: 10pt;">このことから，この県の生徒の得点は全国平均60点よりも高いといえるか。有意水準5%で検定せよ。</span></div><div><span style="font-size: 10pt;">&lt;R&gt;</span></div><div><span style="font-size: 10pt; font-style: italic;">sd = 11/sqrt(50) # 1.555635。標本の大きさがn=50(&gt;30)だから，母集団の標準偏差は標本の標準偏差に等しいと見なせる</span></div><div><span style="font-size: 10pt; font-style: italic;"><span style="font-size: 10pt; font-style: italic; font-weight: bold;">p &lt;- pnorm(63, mean=60, sd, lower.tail=FALSE)</span>  #</span> <span style="font-size: 10pt; font-style: italic;">0.02689816</span><span style="font-size: 10pt; font-style: italic;">。高いかなので上側確率(lower.tail=FALSE)で計算</span></div><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-style: italic;">if (p &lt; 0.05) print(&quot;帰無仮説は棄却&quot;)  # </span><span style="font-size: 10pt; font-style: italic;">帰無仮説は棄却</span></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">②</span><span style="font-size: 10pt; font-weight: bold;">平均の差の検定:2組の標本から推定される各々の母集団の平均値が等しいかどうかを判断する（マイクロアレイでやってたt検定）</span></div><div><span style="font-size: 10pt; font-weight: bold;">→2群の標本値が必要（正確には2群の合計と平均があれば計算できる）</span></div><div><span style="font-size: 10pt;">平均値の差の検定についても標本の個体数が多いか少ないかによって，ｚ検定，ｔ検定に分かれる。</span></div><div><span style="font-size: 10pt;">標本が大きい（概ね30以上）ときは「差が正規分布」になる</span></div><div><span style="font-size: 10pt;">標本が小さい（概ね30未満）ときは「差がｔ分布」になる</span></div><div><span style="font-size: 10pt;">正規分布の形は決まっているが、ｔ分布は標本の大きさｎ（正確には自由度n-1）によって形が変わるので，各自由度に応じたｔ分布を用いる。</span></div><div><a href="https://www.geisya.or.jp/~mwm48961/linear_algebra/t_test2.htm" style="font-size: 10pt;">https://www.geisya.or.jp/~mwm48961/linear_algebra/t_test2.htm</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt; font-weight: bold;">例題1:平均の差のｚ検定</span></div><div><span style="font-size: 10pt;">２つの銘柄のおのおのから選ばれた100個の電球をテストしてその平均寿命について、</span></div><div><span style="font-size: 10pt;">x1_avg=1160 , s1=90 , x2_avg=1140 , s2=80を得たとする．</span></div><div><span style="font-size: 10pt;">これら２つの銘柄の電球の平均寿命に有意差が認められるか？</span></div><div><span style="font-size: 10pt;"># 標本の大きさがいずれも30よりも大きいからｚ検定</span></div><div><span style="font-size: 10pt;"># 等しいか等しくないかに関心があるから両側検定</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [34].png" type="image/png" data-filename="Image.png" title="Attachment"/>より</span></div><div><span style="font-size: 10pt;">&lt;R&gt;</span></div><div><span style="font-size: 10pt; font-style: italic;">z = (1160 - 1140) / sqrt((90^2)/100 + (80^2)/100)  # 1.66091</span></div><div><span style="font-size: 10pt; font-style: italic;">if (z &gt; 1.96) print(&quot;有意差あり&quot;)  # 有意差なし。1.96は正規分布表により，両側確率0.05の境界値</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">例題2:平均の差のt検定</span><span style="font-size: 10pt; font-weight: bold;">(2群に対応ない場合)</span></font></div><div><span style="font-size: 10pt;">20匹のネズミについて、A群の10匹は生の落花生から、B群の10匹は炒った落花生からタンパク質をとらせたときの摂取量は以下</span></div><div><span style="font-size: 10pt;">A = [61, 60, 56, 63, 56, 63, 59, 56, 44, 61]</span></div><div><span style="font-size: 10pt;">B = [55, 54, 47, 59, 51, 61, 57, 54, 62, 58]</span></div><div><span style="font-size: 10pt;">落花生を炒ることがタンパク質の価値に影響を与えるか?（A,Bで有意差があるか）</span></div><div><span style="font-size: 10pt;">&lt;R&gt;</span></div><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-style: italic; font-weight: bold;"># ※20匹のネズミがたまたま10匹ずつに分かれただけであり「生のもの」と「炒ったもの」に対応があるわけではない。</span><span style="font-size: 10pt; font-style: italic; font-weight: bold;">したがって，対応のないｔ検定を行う</span></font></div><div><span style="font-size: 10pt; font-weight: bold;"># まず、A,B群が等分散か調べるためにF検定（より正確には2群の母集団が正規分布か調べる必要があるが、今回は正規分布であるとする）</span></div><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;"># ※</span><span style="font-size: 10pt; font-weight: bold;">F検定の後にt検定を用いることは検定の繰り返し行為に該当するため，正しくないと考える見方もある</span></font></div><div><span style="font-size: 10pt; font-style: italic;">a &lt;- c(61, 60, 56, 63, 56, 63, 59, 56, 44, 61)</span></div><div><span style="font-size: 10pt; font-style: italic;">b &lt;- c(55, 54, 47, 59, 51, 61, 57, 54, 62, 58)</span></div><div><span style="font-size: 10pt; font-style: italic; font-weight: bold;">var.test(x=a, y=b, conf.level=0.95) </span> <span style="font-size: 10pt; font-style: italic;"># </span><span style="font-size: 10pt; font-style: italic;">p値が0.05以上だから帰無仮説（2群の分散に差がない）棄却できないので、A,Bは等分散と言える</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [35].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt; color: rgb(255, 0, 0); font-weight: bold;"># </span> <span style="font-size: 10pt; color: rgb(255, 0, 0); font-style: italic; font-weight: bold;">A,Bは</span><span style="font-size: 10pt; color: rgb(255, 0, 0); font-style: italic; font-weight: bold;">対応ない+</span><span style="font-size: 10pt; color: rgb(255, 0, 0); font-style: italic; font-weight: bold;">等分散なので、</span><span style="font-size: 10pt; color: rgb(255, 0, 0); font-style: italic; font-weight: bold;">スチューデントのt検定を行う</span></div><div><span style="font-size: 10pt; font-style: italic;"># var.equal=TRUEは等分散性の仮定</span></div><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-style: italic;"># </span><span style="font-size: 10pt; font-style: italic;">paired=F は対応なしの仮定</span></font></div><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-style: italic; font-weight: bold;">t.test(x=a, y=b, conf.level=0.95, var.equal=TRUE, paired=F)</span><span style="font-size: 10pt; font-style: italic;">  # </span><span style="font-size: 10pt; font-style: italic;">p値が0.05以上だから帰無仮説（2群の平均に差がない）棄却できないので、A,Bは差がない（有意差なし）と言える</span></font></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [36].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt; color: rgb(255, 0, 0); font-weight: bold;"># 仮にA,Bが不等分散だった場合はウィルチのt検定を行う</span></div><div><span style="font-size: 10pt; font-style: italic; font-weight: bold;">t.test(x=a, y=b, conf.level=0.95, var.equal=F, paired=F)</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt; font-weight: bold;">例題3:平均の差のt検定(2群に対応のある場合)</span></div><div><span style="font-size: 10pt;">あるダイエット法が体重の減量に効果があるかどうかを調べる実験に，10人の女性が参加した。</span></div><div><span style="font-size: 10pt;">この療法に入る前と，２ヶ月間試みた後の体重（ｋｇ）を測定して次の結果を得た。</span></div><div><span style="font-size: 10pt;">前 = [51.0, 55.2, 52.6, 61.2, 55.4, 57.1, 58.6, 61.4, 57.8, 67.6]</span></div><div><span style="font-size: 10pt;">後 = [51.8, 55.6, 50.9, 59.6, 54.5, 56.4, 58.0, 60.3, 56.9, 66.0]</span></div><div><span style="font-size: 10pt;">体重は正規分布に従うとして，このダイエット法は減量に効果があるかどうかを有意水準5％で検定せよ。</span></div><div><span style="font-size: 10pt;">&lt;R&gt;</span></div><div><span style="font-size: 10pt; color: rgb(255, 0, 0); font-style: italic; font-weight: bold;"># ※個体番号ごとに前後のデータが同一人物のデータを表しているから「対応のある場合」とする．</span></div><div><span style="font-size: 10pt; font-style: italic; font-weight: bold;"># ※対応のある場合のｔ検定では標本の個数がそろっていなければならない．</span></div><div><span style="font-size: 10pt; font-style: italic; font-weight: bold;"># ※対応のある場合のｔ検定では，分散が等しいかどうかは問題にならない．</span></div><div><span style="font-size: 10pt; font-style: italic; font-weight: bold;"># ※両側検定にするか片側検定にするかは，標本から得られる情報ではなく分析者の関心によって決めなければならないとされている．ここでは「減量に効果があるか」に関心があるのだから，</span></div><div><span style="font-size: 10pt; font-style: italic; font-weight: bold;"># 帰無仮説　H0：平均の差が0</span></div><div><span style="font-size: 10pt; font-style: italic; font-weight: bold;"># 対立仮説　H1：前の平均&gt;後の平均（前後で体重減った）</span></div><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-style: italic; font-weight: bold;"># とする片側検定(</span><span style="font-size: 10pt; font-style: italic; font-weight: bold;">後の平均だけp値超えていたらいいので</span><span style="font-size: 10pt; font-style: italic; font-weight: bold;">右側検定)を行う</span></font></div><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-style: italic;"># ※</span><span style="font-size: 10pt; font-style: italic;">対立仮説を 前後で体重が増減したかにするなら(前の平均&gt;後の平均 or </span><span style="font-size: 10pt; font-style: italic;">前の平均&lt;後の平均 になるため両側検定になる</span></font></div><div><span style="font-size: 10pt; font-style: italic;">before &lt;- c(51.0, 55.2, 52.6, 61.2, 55.4, 57.1, 58.6, 61.4, 57.8, 67.6)</span></div><div><span style="font-size: 10pt; font-style: italic;">after &lt;- c(51.8, 55.6, 50.9, 59.6, 54.5, 56.4, 58.0, 60.3, 56.9, 66.0)</span></div><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-style: italic;"># </span><span style="font-size: 10pt; font-style: italic;">alternative='l'で右側検定指定。デフォルトは両側検定になる</span></font></div><div><span style="font-size: 10pt; font-style: italic; font-weight: bold;">t.test(x=before, y=after, conf.level=0.95, paired=T, alternative='l') </span> <span style="font-size: 10pt; font-style: italic;"># p</span><span style="font-size: 10pt; font-style: italic;">値が0.05よりも小さいから有意差あり</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [37].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ウィルコクソンの符号付き順位検定(Wilcoxon signed-rank test)：</span></div><div><span style="font-size: 10pt; font-weight: bold;">データに対応があり、差に正規分布を仮定できない場合の中央値の差の検定</span><span style="font-size: 10pt;">。</span></div><div><span style="font-size: 10pt;">データに対応あるt検定と異なり、平均ではなく中央値の差の検定</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■マン・ホイットニーのU検定(Mann–Whitney U test)：</span></div><div><span style="font-size: 10pt; font-weight: bold;">データに対応がなく、2標本の母集団に正規分布を</span><span style="font-size: 10pt; font-weight: bold;">仮定できない場合の中央値の差の検定</span><span style="font-size: 10pt;">。ウィルコクソンの順位和検定とも呼ばれている</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■カイ二乗検定：カイ二乗分布を使った検定。AサイトとBサイトでクリック回数に差があるが偶然かどうか調べるときに使う。</span></div><div><span style="font-size: 10pt;">A,Bでのカイ二乗（実測値と期待値の差）を計算して帰無仮説「クリック回数に差がない」を棄却できるか調べる。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">カイ二乗検定は「あり/なし」のような二値のカテゴリ変数</span><span style="font-size: 10pt; font-weight: bold;">の集計表から、群間で関連が言えるのか否か（相関があるのかそれとも独立なのか）を判断するための検定</span></font></div><div><span style="font-size: 10pt;">&lt;R&gt;</span></div><div><span style="font-size: 10pt; font-style: italic;">&gt;a = matrix(c(10,47,18,15),2,2) #aという行列を作成</span></div><div><span style="font-size: 10pt; font-style: italic;"># A/Bサイトの1日目/2日目のクリック数とする。1列目がAサイト、2列目がBサイト</span></div><div><span style="font-size: 10pt; font-style: italic;">&gt;a</span></div><div><span style="font-size: 10pt; font-style: italic;">     [,1] [,2]</span></div><div><span style="font-size: 10pt; font-style: italic;">[1,]   10   18</span></div><div><span style="font-size: 10pt; font-style: italic;">[2,]   47   15</span></div><div><span style="font-size: 10pt; font-style: italic;">&gt;chisq.test(a) #カイ二乗検定を実施</span></div><div><span style="font-size: 10pt; font-style: italic;"><img src="用語_files/Image [38].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-style: italic;"># p値&lt;0.05なので、</span><span style="font-size: 10pt; font-style: italic;">帰無仮説（=</span><span style="font-size: 10pt; font-style: italic;">A/Bサイトのクリック数は独立である</span><span style="font-size: 10pt; font-style: italic;">）を棄却し、対立仮説である「二つの変数は独立ではない」という仮説を採択</span></font></div><div><a href="https://ai-trend.jp/programming/r-beginner/chi-test/" style="font-size: 10pt; font-style: italic;">https://ai-trend.jp/programming/r-beginner/chi-test/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ABテスト：内容一部変えたAパターン、Bパターンを見てもらいどちらが良いか調べるテスト。</span></div><div><span style="font-size: 10pt;">ABテストは母集団からランダムサンプリングした標本にAパターン（施策あり）、Bパターン（施策なし）をかけて効果検証するRCT（無作為化比較試験）と同じこと。</span></div><div><span style="font-size: 10pt; font-weight: bold;">母集団から完全にランダムで標本選ぶことがとても重要！！！</span></div><div><span style="font-size: 10pt; font-weight: bold;">ランダムでなければ、効果検証したい施策以外のバイアス（セレクションバイアス）が入り、正確な施策の効果を測定できないので注意！！！</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">データ分析手法の理論と適用_5章 より</span></div><div><a href="https://drive.google.com/drive/u/1/folders/1MRedc-1z3WUopNNvCScL6qgfaVdyUVAz" style="font-size: 10pt;">https://drive.google.com/drive/u/1/folders/1MRedc-1z3WUopNNvCScL6qgfaVdyUVAz</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt; font-weight: bold;">■検定を繰り返しては（多重検定）いけない理由</span></div><div><span style="font-size: 10pt; font-weight: bold;">→繰り返すと有意水準が5%ではなくなるため</span></div><div><span style="font-size: 10pt;">n回検定した場合の有意水準=1-0.95^n</span></div><div><span style="font-size: 10pt;">ダメな例: 「A,B,C群の平均に差があるか」を調べるために、A,B,C群に対して総当たりでt検定した</span></div><div><span style="font-size: 10pt;">帰無仮説=「A,B,C群の平均に差がない」</span></div><div><span style="font-size: 10pt;">1回目:A,B群でt検定したときの有意水準=1-0.95^1=0.05</span></div><div><span style="font-size: 10pt;">2回目:A,C群でt検定したときの有意水準=1-0.95^2~0.10</span></div><div><span style="font-size: 10pt;">3回目:B,C群でt検定したときの有意水準=1-0.95^3~0.14</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt; font-weight: bold;">■多重検定の1回1回の有意水準=α/n</span></div><div><span style="font-size: 10pt; font-weight: bold;">α:全体の優位水準、n:検定回数</span></div><div><span style="font-size: 10pt;">→上のダメな例のA,B,C群なら、0.05/3~0.016となる</span></div><div><span style="font-size: 10pt;">多重検定の簡単な対策は、n回検定した時の優位水準が1-0.95^n=0.05に近づくように1回1回の検定の優位水準を小さくしておけばいい</span></div><div><span style="font-size: 10pt;">ただし、この方法は検定回数が増えると、</span><span style="font-size: 10pt; font-weight: bold;">1回1回の有意水準が非常に小さくなり現実的でなくなる</span></div><div><span style="font-size: 10pt;">なので、実際の多重検定では専用の検定（Williamsの検定とか）使うべき</span></div><div><a href="https://www.youtube.com/watch?v=godq1BhHj7w" style="font-size: 10pt;">https://www.youtube.com/watch?v=godq1BhHj7w</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■同時確率:事象Bかつ事象Aが起こる確率: P(A∧B)</span></div><div><span style="font-size: 10pt;">例. バスの乗客を1名選ぶとき、男性で眼鏡をかけている確率</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [39].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [40].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■条件付き確率(conditional probability):事象Bが起ったときに、事象Bと事象Aが同時に起こる確率（Bが与えられた時のAの条件付き確率）: P(A|B)=P(A∧B)/P(B)</span></div><div><span style="font-size: 10pt;">→Bの確率に対するAかつBの確率比。要は分母の母集団をBだけにしたときの同時確率</span></div><div><span style="font-size: 10pt;"><img src="用語_files/写真 (2020-05-13 16_25_50).jpg" type="image/jpeg" data-filename="写真 (2020-05-13 16_25_50).jpg" title="Attachment"/></span></div><div><span style="font-size: 10pt;">例. バスの乗客を1名選ぶとき、男性を選んだ場合、その乗客が眼鏡をかけている確率</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [41].png" type="image/png" data-filename="Image.png" title="Attachment"/><img src="用語_files/Image [42].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;"><br/></span></div><div><span style="font-size: 10pt;">■</span><span style="font-size: 10pt; font-weight: bold;">乗法定理：同時確率は各々の事象の確率の積になるという定理</span><span style="font-size: 10pt;">:</span></div><div><span style="font-size: 10pt;">P(A∧B)=P(A|B)*P(B)</span></div><div><span style="font-size: 10pt;">例. バスの乗客を1名選ぶとき、男性で眼鏡をかけている確率(12/35) = 男性を選んだ場合、その乗客が眼鏡をかけている確率(12/20) * バスから男性選ぶ確率(20/32)</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■加法定理：AまたはBが起こる確率 = Aが起こる確率 + Bが起こる確率 - AとBの同時確率(AかつBの確立)</span></div><div><span style="font-size: 10pt;">P(A∨B) = P(A) + P(B) - P(A∧B) </span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt; font-weight: bold;">■尤度：その確率になるもっともらしさを表す。</span><span style="font-size: 10pt;">たとえば、さいころの出目の確率P=1/6だが、実際に振ってみて4が多めにでたとする。それでもなお「出目の確率P=1/6」であることがどれだけ尤もらしいか（どれぐらいの確率で出目の確率P=1/6か）を示した値や数式。</span></div><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">ベイズの定理では条件付き確率P(A|B)と同じもの=</span><span style="font-size: 10pt; font-weight: bold;">もし仮説Bが成立するときの事象Aの発生確率。現在仮定法で表現された概念</span></font></div><div><span style="font-size: 10pt;">→A:さいころの出目の確率=1/6、B:さいころの出目はどの面も同じ確率</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ベイズの定理：乗法の定理を繋げただけの式</span></div><div><span style="font-size: 10pt; font-weight: bold;">P(B|A) = P(A|B)*P(B) / P(A)</span></div><div><span style="font-size: 10pt;">Aが得られた時にBが成立する確率</span><span style="font-size: 10pt; font-weight: bold;">(事後確率)</span> <span style="font-size: 10pt;">= BのもとでAが生じる確率</span><span style="font-size: 10pt; font-weight: bold;">(尤度)</span><span style="font-size: 10pt;">*Bが成立する確率</span><span style="font-size: 10pt; font-weight: bold;">(事前確率)</span> <span style="font-size: 10pt;">/ Aが得られた確率</span></div><div><span style="font-size: 10pt;">※事後確率は直接過去法で表現された概念。過去に起きたことの確率っこと</span></div><div><span style="font-size: 10pt;">※ベイズの定理は右辺の仮説の掛け算により左辺の事後確率がでるので、左辺を原因の確率とも言う</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■理由不十分の原則：ベイズの理論の概念。不確かな事前確率に適当な確率を設定すること。この原則のおかげで人間の常識や直観を許容できるため、ベイズの理論では問題設定しやすい</span></div><div><span style="font-size: 10pt;">例.コイントスは何も条件が無いだろうからとりあえず表裏の確率=0.5とする</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ベイズ更新：ベイズの理論の概念。以前のデータから算出した事後確率を、次のデータの事前確率として利用する技法。要は前回計算した値（事後確率）を次の結果の入力（事前確率）として扱えるってこと</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ナイブベイズフィルター：「ベイズの定理」「理由不十分の原則」「ベイズ更新」を組み合わせて迷惑メールを除外するときなどに使われるデータフィルター</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Scannable の文書 (2020-05-18 20_58_29).png" type="image/png" data-filename="Scannable の文書 (2020-05-18 20_58_29).png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ベイズ統計学は確率分布が主役。サンプリングしたデータから確率分布を推定する</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Scannable の文書 (2020-05-19 0_14_15).png" type="image/png" data-filename="Scannable の文書 (2020-05-19 0_14_15).png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">■</span><span style="font-size: 10pt; font-weight: bold;">最尤法(method of maximum likelihood)、あるいは最尤推定(maximum likelihood estimation)法：</span><span style="font-size: 10pt; font-weight: bold;">　</span></font></div><div><span style="font-size: 10pt; font-weight: bold;">尤度（尤度関数）最大になるパラメータを求める方法</span></div><div><span style="font-size: 10pt;">最尤法はデータが実現する確率を利用して推定値（尤度関数）を構成する</span></div><div><span style="font-size: 10pt;">例. コインの裏表の確率をパラメータとした最尤法</span></div><div><span style="font-size: 10pt;">　コイン10回振って表が7回、裏が3回出た事象の表裏の出現確率を推定する</span></div><div><span style="font-size: 10pt;">　推定値（尤度関数）=(p^7)*(1-p)^3</span></div><div><span style="font-size: 10pt;">　p=0.5の時は(0.5^7)*(1-0.5)^3 = 0.000977:確率が0.000977の事象が起きたということ</span></div><div><span style="font-size: 10pt;">　p=0.6の時は(0.6^7)*(1-0.6)^3 = 0.001792:確率が0.001792の事象が起きたということ</span></div><div><span style="font-size: 10pt;">　確率が高いp=0.6の方が良い推定値</span></div><div><span style="font-size: 10pt;">　この事象ではp=0.7のとき推定値が最大（最尤推定値）になる</span></div><div><span style="font-size: 10pt;">→コイントスでの最尤法は尤度関数が最大になる確率pを求めるということ</span></div><div><span style="font-size: 10pt; font-weight: bold;">→最尤法は尤度関数から特定のパラメータを最大化する方法なので、関数をパラメータで偏微分して極値求めることと同じ</span></div><div><span style="font-size: 10pt; font-weight: bold;">　微分が楽になるように対数をとってから微分するのがセオリーみたい。（尤度は確率の掛け算だからどんどん小さい値になるため、対数とった方がいやすいという理由もある）</span></div><div><span style="font-size: 10pt; font-weight: bold;">　対数とった尤度関数を対数尤度関数と呼ぶ</span></div><div><span style="font-size: 10pt;">　コイントスのような2値分類の尤度関数:</span></div><div><span style="font-size: 10pt;">　l(p) = (p^y)*(1-p)^(n-y)</span></div><div><span style="font-size: 10pt;">　↓l(p) = logL(p)とする</span></div><div><span style="font-size: 10pt;">　l(p) = y*log(p) + (n-y)*log(1-p)  # cross entropy系のlossの式と同じ!!!!</span></div><div><span style="font-size: 10pt;">→ニューラルネットだとcross entropy系のlossは（yが確率値なので）最尤法でパラメータ（重み）最適化している。</span></div><div><span style="font-size: 10pt;">参考:</span> <a href="https://medium.com/music-and-technology/%E5%B0%A4%E5%BA%A6%E9%96%A2%E6%95%B0%E3%81%A8%E3%82%AF%E3%83%AD%E3%82%B9%E3%82%A8%E3%83%B3%E3%83%88%E3%83%AD%E3%83%94%E3%83%BC%E8%AA%A4%E5%B7%AE%E9%96%A2%E6%95%B0-d8376a4d3ac" rev="en_rl_none" style="font-size: 10pt;">https://medium.com/music-and-technology/%E5%B0%A4%E5%BA%A6%E9%96%A2%E6%95%B0%E3%81%A8%E3%82%AF%E3%83%AD%E3%82%B9%E3%82%A8%E3%83%B3%E3%83%88%E3%83%AD%E3%83%94%E3%83%BC%E8%AA%A4%E5%B7%AE%E9%96%A2%E6%95%B0-d8376a4d3ac</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■逸脱度:対数尤度を-2倍したもの。あてはまりの悪さを表す。ニューラルネットのloss。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■オッズ比：説明変数が１変化したときに目的変数が起こる確率が何倍になるかを示す。宝くじ1枚買うのと10枚買うのでは確率変わる。</span></div><div><span style="font-size: 10pt;">　オッズはある事象が発生する確率をpとしたときp / (1-p)で表されます。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■不偏分散：母集団の分散。不偏がつくと母集団のこと。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■自由度：ある変数において自由な値をとることができるデータの数。基本はn（サンプルサイズ）-1</span></div><div><span style="font-size: 10pt;">　たとえば、n個のデータx1,x2,…があり、平均値aである場合、平均値が変わらないようにするためにn-1個のデータは自由な値とれるがn番目のデータは自由な値とれない。ことのき自由度はn-1</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■標準誤差:標本平均の分布の標準偏差。標本平均のばらつき具合。標準偏差/データ数^1/2</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■標本平均：母集団から標本を抽出して求めた平均</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■変数選択法：回帰式に入れる説明変数の選択方法。基本、説明変数のp値が小さいものを採用する。ステップワイズ法とかが有名。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">データ分析手法の理論と適用_付録 より</span></div><div><a href="https://drive.google.com/drive/u/1/folders/1MRedc-1z3WUopNNvCScL6qgfaVdyUVAz" style="font-size: 10pt;">https://drive.google.com/drive/u/1/folders/1MRedc-1z3WUopNNvCScL6qgfaVdyUVAz</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■多重共線性(multicollinearity:マルチコ)：重回帰モデルで説明変数同士で相関係数が高いこと</span></div><div><span style="font-size: 10pt;">例えば下の式なら、X1とX2で相関あるなら、X1とX2は多重共変性があるという</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [43].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">多重共変性は、回帰モデルで未知データ予測するだけなら問題にはならないが、</span><span style="font-size: 10pt; font-weight: bold;">重みからどの説明変数が目的変数変数に寄与するのか分析する場合は問題になる</span></div><div><span style="font-size: 10pt; font-weight: bold;">理由は、多重共変性があるとサンプル（train set）によって重みの値が大きく変更してしまうため</span><span style="font-size: 10pt;">。</span></div><div><span style="font-size: 10pt;">母集団から100サンプル取って多重共変性がある重回帰モデル作ることを繰り返した場合、</span></div><div><span style="font-size: 10pt;">1回目はβ2=10</span></div><div><span style="font-size: 10pt;">2回目はβ2=-0.001</span></div><div><span style="font-size: 10pt;">3回目はβ2=100000</span></div><div><span style="font-size: 10pt;">みたいに重みの値が安定しない。</span></div><div><span style="font-size: 10pt;">よって、多重共変性があると説明変数の寄与を分析することができない</span></div><div><a href="https://www.youtube.com/watch?v=AF5z3adXGeI" style="font-size: 10pt;">https://www.youtube.com/watch?v=AF5z3adXGeI</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■カーネル密度推定：未知の母集団からのサンプリングと見なせるデータ点から、母集団の確率密度を推定する方法</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■マシューズ相関係数（Matthews Correlation Coefficient、MCC）:</span></div><div><span style="font-size: 10pt;">真陽性(TP),偽陽性(FP),真陰性(TN),偽陰性(FN)を全部考慮したうえで、クラスのデータ数に不均衡であっても使用できる2値分類の評価指標。</span></div><div><span style="font-size: 10pt;">MCCが大きいほど良い分類ができているとみなす。最大値は 1、最小値は -1。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■階層型クラスタリング：k-meanなどで近いサンプルをクラスタに分けて、距離が近いクラスタから順番に線でつないでいき階層構造を表すクラスタリング</span></div><div><span style="font-size: 10pt;">クラスター間の距離測定方法が色々ある</span></div><div><span style="font-size: 10pt;">途中過程が階層のように表せ、最終的に樹形図（デンドログラム）ができる</span></div><div><a href="https://www.albert2005.co.jp/knowledge/data_mining/cluster/hierarchical_clustering" style="font-size: 10pt;">https://www.albert2005.co.jp/knowledge/data_mining/cluster/hierarchical_clustering</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt; font-weight: bold;"># 階層型クラスタリングの種類（分類が階層的になる階層的クラスター分析はクラスター間の距離を決める方法にいくつかの種類がある）</span></div><div><span style="font-size: 10pt;">■最近隣法：2つのクラスターの中から、最も近いデータ間の距離を2つのクラスターの距離とする方法</span></div><div><span style="font-size: 10pt;">距離短いサンプル同士をくっつけていく</span></div><div><span style="font-size: 10pt;">  scipy.cluster.hierarchy.linkage(method='</span><span style="font-size: 10pt; font-weight: bold;">single</span><span style="font-size: 10pt;">')</span></div><div><span style="font-size: 10pt;">■最遠隣法：2つのクラスターの中から、最も遠いデータ間の距離を2つのクラスターの距離とする方法</span></div><div><span style="font-size: 10pt;">距離長いサンプル同士をくっつけていく</span></div><div><span style="font-size: 10pt;">  scipy.cluster.hierarchy.linkage(method='</span><span style="font-size: 10pt; font-weight: bold;">complete</span><span style="font-size: 10pt;">')</span></div><div><span style="font-size: 10pt;">■群平均法：2つのクラスターの中から、それぞれデータを一つずつ選び距離を求め、それらの距離の平均値を2つのクラスターの距離とする方法</span></div><div><span style="font-size: 10pt;">  scipy.cluster.hierarchy.linkage(method='</span><span style="font-size: 10pt; font-weight: bold;">average</span><span style="font-size: 10pt;">')</span></div><div><span style="font-size: 10pt;">■ウォード法：２つのクラスターを融合した際に、群内の分散と群間の分散の比を最大化する基準でクラスターを形成していく方法。</span></div><div><span style="font-size: 10pt;">分散小さいサンプルを同じ群にしていく</span></div><div><span style="font-size: 10pt;">  scipy.cluster.hierarchy.linkage(method='</span><span style="font-size: 10pt; font-weight: bold;">ward</span><span style="font-size: 10pt;">')</span></div><div><span style="font-size: 10pt;">→とりあえずウォード法を選択しておき、もし何か理由があるならばそれ以外の手法を選ぶという方針で良いらしい</span></div><div><a href="https://analysis-navi.com/?p=1805" style="font-size: 10pt;">https://analysis-navi.com/?p=1805</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">最近隣法と最遠隣法にはそれぞれチェーンと拡散現象という性質があるので、次にその説明を示す。</span></div><div><span style="font-size: 10pt;">■チェーン：</span></div><div><span style="font-size: 10pt;">クラスターが大きくなるにつれ、他のデータと最短距離を多く持つようになり、次のクラスターの形成の候補に選ばれやすくなる現象。</span></div><div><span style="font-size: 10pt;">■拡散現象：</span></div><div><span style="font-size: 10pt;">クラスターが大きくなるにつれ、他のデータと最長距離を多く持つようになり、次のクラスターの形成の候補に選ばれにくくなる現象。</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [44].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="http://www.gifu-nct.ac.jp/elec/deguchi/sotsuron/hattori/node20.html" style="font-size: 10pt;">http://www.gifu-nct.ac.jp/elec/deguchi/sotsuron/hattori/node20.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■独立同一分布：互いに独立だが同じ確率分布に従う多次元の確率変数</span></div><div><span style="font-size: 10pt;">= 同じ条件下で行われる実験や観測を複数回繰り返すことでデータを得ることを数学の言葉で表したもの</span></div><div><span style="font-size: 10pt;">例. 20人の数学のテストの点数</span></div><div><span style="font-size: 10pt;">(x1, x2, …, x20)と20次元であり、各xの値は異なるが確率分布は同じ</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■回帰分析:y=ax+bのような回帰モデルの変数x,yの関係を調べること</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■一般線形モデル: y=a1x1+a2x2+…+bの関数で予測するモデル</span></div><div><span style="font-size: 10pt; font-weight: bold;">一般線形回帰モデルでは、正解と予測の誤差の傾向には正規分布（normal distribution）が仮定されている</span></div><div><span style="font-size: 10pt;">- 単回帰（線形回帰）分析: y=ax+bの直線の関数（モデル）で連続値yを予測する分析</span></div><div><span style="font-size: 10pt;">- 重回帰分析: y=a1x1+a2x2+…+bの関数（モデル）で連続値yを予測する分析</span></div><div><span style="font-size: 10pt;">- t検定: 2群の母平均の差の検定（差があるのは偶然ではないかを決める検定）</span></div><div><span style="font-size: 10pt;">- 分散分析（ANOVA）: 3群以上の検定</span></div><div><span style="font-size: 10pt;">も一般線形モデルで表せる</span></div><div><a href="http://cogpsy.educ.kyoto-u.ac.jp/personal/Kusumi/datasem13/shrasuna1.pdf" style="font-size: 10pt;">http://cogpsy.educ.kyoto-u.ac.jp/personal/Kusumi/datasem13/shrasuna1.pdf</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■</span><span style="font-size: 10pt; font-weight: bold;">一般化線形モデル(generalized liner model; GLM): 正規分布以外の、指数型分布族(exponential family)の分布を仮定したモデル</span></div><div><span style="font-size: 10pt; font-weight: bold;">端的に言えば、t検定とかいろいろあるけどそれら検定をまとめた広義の回帰分析のこと</span></div><div><span style="font-size: 10pt;">一般化線形モデルは確率分布とlink関数（式を変換して線形にする関数。分布によってlink関数は大体決まっている）が必要</span></div><div><span style="font-size: 10pt;">- ロジスティック回帰モデル（logistic regression model）: 正解と予測の誤差の傾向に二項分布（binomial distribution）を仮定したモデル</span></div><div><span style="font-size: 10pt;">- ポアソン回帰モデル（Poisson regression model）:出力ラベルyの分布に対してポアソン分布（Poisson distribution）を利用したモデル</span></div><div><a href="https://www.bigdata-navi.com/aidrops/2925/" style="font-size: 10pt;">https://www.bigdata-navi.com/aidrops/2925/</a></div><div><a href="http://cogpsy.educ.kyoto-u.ac.jp/personal/Kusumi/datasem13/shrasuna1.pdf" style="font-size: 10pt;">http://cogpsy.educ.kyoto-u.ac.jp/personal/Kusumi/datasem13/shrasuna1.pdf</a></div><div><br/></div><div><img src="用語_files/Image [45].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div>本「統計学が最強の学問である」より</div><div><br/></div><div><br/></div><div>■一般化線形モデルでp値も計算される理由</div><div>t検定やカイ二乗検定などは回帰分析の1種であるため</div><div>下記はt検定と2変数の線形回帰モデルの比較であり、本質的に同じことをしているとわかる</div><div><img src="用語_files/Image [46].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div><img src="用語_files/Image [47].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div>本「統計学が最強の学問である」より</div><div><br/></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■機械学習モデリング</span></div><div><span style="font-size: 10pt;">交差検証などでモデル出力結果を評価し、ハイパーパラメータチューニングをすることで未知のデータに対しても精度の良い推論ができるモデルを設計する</span></div><div><span style="font-size: 10pt;">※TrainデータとTestデータで確率分布に偏りがあると適切な評価ができない</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■統計モデリング</span></div><div><span style="font-size: 10pt;">サンプルの確率分布から真の確率分布を推定し、その推定を考慮したモデル設計により、未知のデータに対しても真の確率分布に”あてはめた”推論ができる</span></div><div><span style="font-size: 10pt;">※統計モデルでは「あてはまりの良さ」を評価することが重要</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■対数化: xをlog(x)に変換すること。</span></div><div><span style="font-size: 10pt;">線形モデルでの前処理などで使う（線形モデルはy=ax+bみたいなシンプルなモデルなので、生のデータでは適切に表現できない場合がある。入力データの表現を変えるために使ったりする）</span></div><div><span style="font-size: 10pt;">→値が小さいほど差が大きく、値が大きいほど差が小さくなる。対数グラフ思い出せ</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Box-Cox変換: 対数化（スケール変更）して正規分布に近づける変換方法</span></div><div>パラメータλによって分布の形調整する</div><div>パラメータλ=0のときはただの対数化と同じこと</div><div>λをマイナスにすると分布を右側に集める。λをプラスにすると分布を左側に集める</div><div><img src="用語_files/Image [48].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div><a href="https://qiita.com/dyamaguc/items/b468ae66f9ce6ee89724">https://qiita.com/dyamaguc/items/b468ae66f9ce6ee89724</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■外れ値: 極端に大きな値や小さな値</span></div><div><span style="font-size: 10pt;">外れ値は統計指標を歪める可能性があり、モデル作成時に悪影響を及ぼす可能性があるので前処理で除去するのがセオリー（除くことで精度上がる場合もある）</span></div><div><span style="font-size: 10pt;">特殊な状況の分析では外れ値あえて残す場合もあり</span></div><div><span style="font-weight: bold;">外れ値のうち、測定ミス・記入ミス等原因が分かっているものを「異常値」とよぶ場合がある</span></div><div><span style="font-size: 10pt;"><br/></span></div><div><img src="用語_files/Image [49].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div><a href="https://bellcurve.jp/statistics/course/12929.html">https://bellcurve.jp/statistics/course/12929.html</a></div><div><br/></div><div><span style="font-weight: bold;">箱ひげ図を描いた場合、ひげの範囲から外れた値は外れ値とみなされます</span>。</div><div>ひげの上端は「第三四分位数+1.5×IQRより小さい最大値」を、ひげの下端は「第一四分位数-1.5×IQRより大きい最小値」を表しています</div><div><img src="用語_files/Image [50].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div><a href="https://bellcurve.jp/statistics/course/12929.html">https://bellcurve.jp/statistics/course/12929.html</a></div><div><br/></div><div><span style="font-size: 10pt;"><br/></span></div><div><span style="font-size: 10pt;">■(ピアソンの)相関係数: </span>2つの確率変数の間の相関（類似性の度合い）を示す統計学的指標</div><div>単位は無く、−1 から 1 の間の実数値をとり、</div><div>1 に近いときは2 つの確率変数には正の相関があるといい、</div><div>−1 に近ければ負の相関があるという</div><div>0 に近いときはもとの確率変数の相関は弱い</div><div><img src="用語_files/Image [51].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div><img src="用語_files/Image [52].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div>相関係数は、x と y の間の直線的な関係性の強さを表す指標</div><div>→年齢 x が高いほうが、年収 y も高い傾向があるみたいな</div><div><a href="https://atarimae.biz/archives/7966">https://atarimae.biz/archives/7966</a></div><div><br/></div><div><br/></div><div>■共分散: 2つの確率変数の関係性。<span style="font-weight: bold;">相関係数の式の分子</span>に当たる</div><div>共分散には「2つの変数の関係の強さ」と「単位」の両方の影響を受けてしまう欠点がある</div><div><span style="font-weight: bold;">相関係数は単位の欠点を持つ共分散の単位を打ち消すために、2変数の標準偏差の積で割り算している</span></div><div><br/></div><div>■因果関係と相関関係の違い</div><div><span style="font-weight: bold;">因果関係は「原因」→「結果」の関係</span></div><div>（原因とそれによって生ずる結果との関係）</div><div>例えば「いっぱい寝たから、元気になった」は睡眠時間と健康に因果関係あることを示す</div><div><img src="用語_files/Image [53].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div>因果関係は相関関係の一部</div><div>両者の関係性は「因果関係があるから、相関関係にある」と表現することができます</div><div>ということで、相関関係はあるけど因果関係はなかった…というケースもあり得る</div><div>→アイスクリームの消費量と森林火災の面積はどちらも夏になると増える=相関関係あり</div><div>しかし、アイスクリームの消費量が増えたことが原因で森林火災の面積が増えた結果とはいえない=因果関係なし</div><div><img src="用語_files/Image [54].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div><a href="https://note.com/shunkonya/n/n9eb149a54bc9">https://note.com/shunkonya/n/n9eb149a54bc9</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■傾向スコア: 興味のある2値の説明変数について「どちらに該当するか」という確率</span></div><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">→「どちらに該当するか」という傾向を示す値だから傾向スコアという</span></font></div><div><span style="font-weight: bold;">→傾向スコア自体はロジスティック回帰で簡単に出せる</span></div><div><span style="font-weight: bold;">傾向スコアはランダム化比較試験ができないとき、ランダム化比較試験と同等のフェアな比較をするために使われる</span></div><div><span style="font-weight: bold;">→傾向スコアが同じサンプルはランダムサンプリングに等しいので</span></div><div>例えば、喫煙することで肺がんになるという因果関係があるか調べたいとき</div><div>（喫煙の有無以外は同じ条件の2群でランダム化比較試験ができない前提）</div><div>→年齢、性別、年収などの説明変数から計算した「喫煙するであろう」という傾向スコアが同じ人をサンプリングして、その中で喫煙者、非喫煙者の2群を比較する</div><div><span style="font-size: 10pt;">説明変数からタバコ吸うはずの喫煙者、タバコ吸うはずなので非喫煙者の2群の比較になる</span></div><div><span style="font-size: 10pt;"><br/></span></div><div><span style="font-size: 10pt;">■統計学での変数の尺度</span></div><div>1.名義尺度: <span style="font-weight: bold;">他と区別し分類</span>するための名称のようなもの</div><div>例：男女、血液型、郵便番号、住所、本籍地、所属学部、学籍番号</div><div><br/></div><div>2.順序尺度:<span style="font-weight: bold;"> 順序や大小には意味がある</span>が間隔には意味がないもの</div><div>例：1位＋2位≠3位のように、足し算引き算ができないもの</div><div><br/></div><div><span style="font-size: 10pt;">3.間隔尺度: </span>目盛が等間隔になっているもので、その<span style="font-weight: bold;">間隔に意味があるもの</span></div><div>例：気温（摂氏）、西暦、テストの点数</div><div><br/></div><div>4.比例尺度: 0が原点であり、間隔と<span style="font-weight: bold;">比率に意味があるもの</span></div><div>例：身長、速度、睡眠時間、値段、給料、幅跳びの記録</div><div><br/></div><div><br/></div><div>■固有ベクトル: <span style="font-weight: bold;">行列 A を掛けても</span>、λ 倍されるだけで<span style="font-weight: bold;">方向が変わらないベクトル</span></div><div><span style="font-weight: bold;">この時のλをAの固有値という</span></div><div><span style="font-weight: bold;"><img src="用語_files/Image [55].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-weight: bold;"><img src="用語_files/Image [56].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://atarimae.biz/archives/24166">https://atarimae.biz/archives/24166</a></div><div><br/></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■箱ひげグラフの見方</span></div><div><img src="用語_files/Image [57].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div><a href="https://bellcurve.jp/statistics/course/5220.html">https://bellcurve.jp/statistics/course/5220.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■欠損値の種類</span></div><div><span style="font-size: 10pt;">1.MCAR (Missing Completely At Random): 完全ランダムに生じた欠損。「なぜかその日の株価が欠損になった」はランダム要因なのでMCAR。</span></div><div><span style="font-size: 10pt;">→MCARはランダムなので消していいい欠損</span></div><div><span style="font-size: 10pt;">2.MAR (Missing At Random): 他の変数によって生じた欠損。「証券会社がエラーになって、その日の株価が欠損になった」は別の要因で欠損なのでMAR</span></div><div><span style="font-size: 10pt;">→MARは本来特定の値があるはずなので置換すべき欠損</span></div><div><span style="font-size: 10pt;">3.MNAR (Missing Not At Random): 欠損値を持つ変数自身によって生じた欠損。「株価のレンジがマイナスの値を許容していないため、欠損になった」は自分の性で欠損なのでMNAR</span></div><div><span style="font-size: 10pt;">→MNARは本来特定の値があるはずなので置換すべき欠損</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [58].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://koumurayama.com/koujapanese/missing_data.pdf" style="font-size: 10pt;">https://koumurayama.com/koujapanese/missing_data.pdf</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■単一代入法: 一つの欠測値に一つの値を代入する欠損値の置換方法</span></div><div><span style="font-size: 10pt;">中央値とかを入れる代表値代入、(確率的)回帰代入、など色々ある</span></div><div><a href="https://techblog.nhn-techorus.com/archives/6309" style="font-size: 10pt;">https://techblog.nhn-techorus.com/archives/6309</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■多重代入法： 欠損値を置換するの方法</span></div><div><span style="font-size: 10pt;">欠測データの分布から独立かつ無作為に抽出されたM個のシミュレーション値によって欠測値を置き換えるもの。</span></div><div><span style="font-size: 10pt;">この代入法の目的は個々の値を完全に復元させることではなく、母集団のパラメータの推定。</span></div><div><span style="font-size: 10pt;">MARであると仮定して、観測データを条件とする欠測データの事後予測確率を構築する。</span></div><div><span style="font-size: 10pt;">→N個の代入済みのデータを生成し、それぞれのデータでモデル作成や分析を行う。その後にその結果を統合する。</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [59].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://qiita.com/saltcooky/items/2e39c4bc099d20f20d59" style="font-size: 10pt;">https://qiita.com/saltcooky/items/2e39c4bc099d20f20d59</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■数理モデル(mathematical model)：対象データ生成ルールを模擬したもの</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■確率過程：時間変化していく確率変数のこと</span></div><div><span style="font-size: 10pt;">何度もさいころ振り、毎回出目（確率変数:x）を記録する（x1=1, x2=6, x3=4, …）ようなときは、さいころの出目が確率過程といえる</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt; font-weight: bold;">■マルコフ過程：現在の値だけで次の値を決める確率過程。1つ前の状況からしか次の状況を決めない</span></div><div><span style="font-size: 10pt;">式にするなら x(t+1) = a*x(t) のようなイメージ</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■マルコフ連鎖：離散的にマルコフ過程が進展していくもの</span></div><div><span style="font-size: 10pt;">例. 毎日のランチを前日食べたメニューをもとに決める</span></div><div><span style="font-size: 10pt;">状態が変化していくことを「状態遷移」（この場合メニューが状態）</span></div><div><span style="font-size: 10pt;">それぞれの状態遷移が起こる確率を「遷移確率」という</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Scannable の文書 (2020-05-25 9_06_36).png" type="image/png" data-filename="Scannable の文書 (2020-05-25 9_06_36).png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">今回の場合、十分時間が経過したら、状態確率は時間変化しない一定の値（時間変化しない定常状態）に収束していくはず、</span></div><div><span style="font-size: 10pt; font-weight: bold;">→定常状態を考えると確率や期待値が求まる</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [60].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ポアソン過程：ランダムに生起する事象を表す基本的な確率過程。客の到着や故障の発生, 個体の出生など様々な現象のモデル化に使われる。</span></div><div><span style="font-size: 10pt;">ポアソン過程はマルコフ過程の一種。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■</span><span style="font-size: 10pt; font-weight: bold;">定常性：確率過程が時間変化しても変わらないこと。どの時点の値も同じ確率分布に従うということ</span></div><div><span style="font-size: 10pt;">時刻t=1の時、平均1,分散2の正規分布 であり 時刻t=nの時も同じ分布になることを定常性という</span></div><div><span style="font-size: 10pt;">時刻t=3の時、平均5,分散41の正規分布になるなら、定常性ではない</span></div><div><span style="font-size: 10pt;">飛行機乗客数は非定常（過程）のデータ。右肩上がりのトレンドや夏場に増加傾向の季節性もあり、時期によって確率分布が変わるといえる</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ARモデル（自己回帰モデル:AutoRegression model）：時系列データのモデル。マルコフ過程のように</span><span style="font-size: 10pt; font-weight: bold;">1つ前のデータ+</span><span style="font-size: 10pt; font-weight: bold;">定常性もつ</span><span style="font-size: 10pt; font-weight: bold;">ノイズで次の値を決めるモデル。</span><span style="font-size: 10pt; font-weight: bold;">自分自身の過去の値を使って次の値を予測するから自己回帰</span></div><div><span style="font-size: 10pt;">x(t) = c + Φ*x(t-1) + ε(t)</span></div><div><span style="font-size: 10pt;">c, Φはパラメータ。ε(t)は時刻tでのホワイトノイズ、平均=0,分散σ^2の確率分布からの値。</span></div><div><span style="font-size: 10pt; font-weight: bold;">ε(t)は定常性を持つ確率分布なので、ARモデルが使えるのは定常性が満たされている時系列データ。</span></div><div><span style="font-size: 10pt;">※株価は定常性持つかはわからないので、ARモデルで予測できるとは限らない</span></div><div><span style="font-size: 10pt;">p時点過去までの値を使うARモデルをAR(p)と書く</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ベクトル自己回帰モデル：ARモデルをp時点まで遡って変数に加えたモデル</span></div><div><span style="font-size: 10pt;">x(t) = c + Φ1*x(t-1) + Φ2*x(t-2) + Φ3*x(t-3) + … + ε(t)</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [61].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">p時点過去までの値を使うARモデルをAR(p)モデルと言います</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■MA（移動平均： </span><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; font-size: 10pt; color: rgb(0, 0, 0); font-family: Meiryo; font-variant-caps: normal; font-variant-ligatures: normal;">Moving Average</span><span style="font-size: 10pt;">）モデル：時間の変化に対し不規則に値が変化するけど、ある区間での変動は一定であるようなモデル。</span></div><div><span style="font-size: 10pt;">MAモデルは過去の誤差に影響される。直前q個の値誤差の影響を受けるモデルをMA(q)と表現</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [62].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">AR(p)の式と似ていますが、過去の値 yt−i で回帰しているのではなく、</span><span style="font-size: 10pt; font-weight: bold;">過去のノイズ εt−j で回帰</span><span style="font-size: 10pt;">しています。</span></div><div><span style="font-size: 10pt;">このように、 q 時点過去までのノイズを使うMAモデルをMA( q )モデルといいます。</span></div><div><span style="font-size: 10pt;">過去のデータで回帰するARモデルに比べ、観測不可能なノイズで回帰するMAモデルは直感的に理解しにくいですが、</span></div><div><span style="font-size: 10pt;">たとえば「今日の売上が予想より多ければ明日の売上は少なくなる」というような現象をモデリングすることができます。</span></div><div><span style="font-size: 10pt;">ARモデルと異なり、MAモデルは常に定常になります。</span></div><div><a href="https://to-kei.net/basic-study/time-series-analysis/sarima_model/" style="font-size: 10pt;">https://to-kei.net/basic-study/time-series-analysis/sarima_model/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ARMAモデル（自己回帰移動平均モデル:AutoRegression Moving Average model）：ベクトル自己回帰モデルに、過去のノイズの総和項を加えたモデル。</span><span style="font-size: 10pt; font-weight: bold;">過去のノイズの総和が移動平均に相当するので、移動平均モデルと呼ばれる</span></div><div><span style="font-size: 10pt;">MAモデルは常に定常性を有していますので、ARMAモデルが定常過程かどうかはARモデルが定常過程かどうかに依存</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [63].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">※株価予測するARモデルに移動平均線（5日線や25日線）の変数を足し算するイメージ</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ARIMAモデル（自己回帰和分移動平均モデル:AutoRegression Integrated and Moving Average model）：</span><span style="font-size: 10pt; font-weight: bold;">定常性が無い時系列データでも適応できるモデル</span></div><div><span style="font-size: 10pt;">各時刻の前後の値の差分で時系列を作ると、近似的に定常と見なせることがある（株価など）</span></div><div><span style="font-size: 10pt;">→この手続きはトレンドを除くこと。</span><span style="font-size: 10pt; font-weight: bold;">データにトレンドを除くことでモデルにトレンドの効果を追加している</span></div><div><span style="font-size: 10pt; font-weight: bold;">前後の差分をとってARMAモデルを適用する方法をARIMAモデルという</span></div><div><span style="font-size: 10pt;">d階差分をとった系列に対してARMA(p,q)を考えるモデルをARIMA(p,d,q)モデルといいます</span></div><div><span style="font-size: 10pt; font-weight: bold;">一言で述べると、データの差分を取ってからARMAを適用したモデルが、ARIMAモデルということ</span></div><div><span style="font-size: 10pt;">時系列が(d次式のトレンド) + (定常部分) といえる場合に有効なモデル</span></div><div><a href="https://to-kei.net/basic-study/time-series-analysis/sarima_model/" style="font-size: 10pt;">https://to-kei.net/basic-study/time-series-analysis/sarima_model/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■SARIMAモデル（Seasonal ARIMA model）：一年の中で起きる周期的な</span><span style="font-size: 10pt; font-weight: bold;">季節変動の効果を追加したARIMAモデル</span></div><div><span style="font-size: 10pt;">ARIMAモデルと同じように、データに季節変動を除くために前年の同じ時期の値との差分をとった時系列を作る</span></div><div><span style="font-size: 10pt;">SARIMAモデルのアイデアは、時系列方向の説明にARIMA( p,d,q ) モデルを使うだけでなく、周期方向の説明にもARIMA( P,D,Q )モデルを使おう、というもの</span></div><div><span style="font-size: 10pt;">SARIMAモデルでは合計7個の次数があります。 時系列方向のARIMA( p,d,q )に加え季節差分方向のARIMA( P,D,Q )、さらには周期 s があるため</span></div><div><a href="https://to-kei.net/basic-study/time-series-analysis/sarima_model/" style="font-size: 10pt;">https://to-kei.net/basic-study/time-series-analysis/sarima_model/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [64].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■階差系列:時系列で隣り合うデータとの差をとったもの（今日の値－昨日の値みたいに）</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■自己相関 (Auto Correlation)：元データ（時刻tのとき）と時間をずらしたデータ（時刻t-nのとき。時刻nの時ならn階差分やn次遅れともいう）との相関（相関係数の場合なら二変量がどのぐらい直線の関係にあるか）のこと。</span><span style="font-size: 10pt; font-weight: bold;">過去の自分のデータと似ているかだから自己相関</span></div><div><span style="font-size: 10pt;">■ラグ：元データからずらした量や差分。次数とも言う</span></div><div><span style="font-size: 10pt;">■コレログラム：ラグと自己相関を表したグラフ。データが周期性をもつかどうかを調べることができる</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [65].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://bellcurve.jp/statistics/course/12935.html" style="font-size: 10pt;">https://bellcurve.jp/statistics/course/12935.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■自己相関関数 (Auto Correlation Function: ACF) ：ラグをx軸に取った自己相関の相関係数（自己相関係数：時刻tとt-nとの相関係数）についての式のこと。過去の値が現在のデータにどれくらい影響しているか</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [66].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">以下のような、自己相関関数のplotを自己相関図という。要は、自己相関関数のコレログラム</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [67].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt; font-weight: bold;">ピンクの領域は95%信頼区間なので、この領域内なら0時点との自己相関なし、領域外なら自己相関なしを棄却（相関ありと解釈）</span></div><div><a href="https://to-kei.net/basic-study/time-series-analysis/auto_correlation/" style="font-size: 10pt;">https://to-kei.net/basic-study/time-series-analysis/auto_correlation/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■偏自己相関係数：注目している時以外の要因を無視して計算された自己相関係数。ある時点同士だけの関係性</span></div><div><span style="font-size: 10pt;">一日前と今日の関係だけが知りたくて、一昨日の要因無視したいときは自己相関係数ではなくこちらを使う。</span></div><div><span style="font-size: 10pt;">純粋に「一日前と今日の関係」を調べることができます</span></div><div><a href="https://logics-of-blue.com/%E6%99%82%E7%B3%BB%E5%88%97%E8%A7%A3%E6%9E%90_%E7%90%86%E8%AB%96%E7%B7%A8/" style="font-size: 10pt;">https://logics-of-blue.com/%E6%99%82%E7%B3%BB%E5%88%97%E8%A7%A3%E6%9E%90_%E7%90%86%E8%AB%96%E7%B7%A8/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■季節性(Seasonality)：周期的に繰り返す変動。四季とは直接関係なくても周期的な変動は季節性と表現する</span></div><div><span style="font-size: 10pt;">■トレンド(Trend)：右方上がりに増えているみたいなデータの傾向のこと。実データから季節性を引き算したデータに対応する。要は移動平均的な線のこと</span></div><div><span style="font-size: 10pt;">■残差(Residual)：トレンドと季節性を除いたその他変動成分</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ADF検定（拡張ディッキー-フラー検定: Augmented Dickey-Fuller test, ADF test）：時系列のサンプルデータが、単位根過程（非定常過程）であるかどうかを調べる</span></div><div><span style="font-size: 10pt; font-weight: bold;">ADF検定は帰無仮説『単位根過程（非定常過程）である』を棄却(p値&lt;0.1とか)することで、そのデータが定常過程とみなす</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■単位根過程：非定常だけど、差分をとると（弱）定常になるようなデータ</span></div><div><span style="font-size: 10pt; font-weight: bold;">元の時系列ytが非定常過程である一方で差分系列Δyt=yt−yt−1が定常過程である時、これを単位根過程（もしくは和分過程）と呼びます</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■共和分：非定常のデータの足し算は定常過程になる性質</span></div><div><span style="font-size: 10pt;">二つの単位根過程 xt, ytの線形和 xt+βyt=ztが定常過程に従う時、この二つは共和分の関係を持つ</span></div><div><span style="font-size: 10pt;">線形和 xt+βyt=ztが定常過程に従うようなβが存在するとき、二つは共和分の関係にあるという</span></div><div><a href="https://ai-trend.jp/basic-study/time-series-analysis/cointegration/" style="font-size: 10pt;">https://ai-trend.jp/basic-study/time-series-analysis/cointegration/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">共和分となる、二つの単位根過程 xt, ytの線形和 xt+βyt (βは定数)は定常なので、</span><span style="font-size: 10pt; font-weight: bold;">「平均回帰性」</span><span style="font-size: 10pt;">という性質を持つ</span></div><div><span style="font-size: 10pt;">平均回帰性は、時間経過すれば平均値に戻っていくという性質</span></div><div><span style="font-size: 10pt;">つまり、</span><span style="font-size: 10pt; font-weight: bold;">線形和 xt+βyt は時間により変動するが、ある値（平均）にいつかは収束するということ</span></div><div><span style="font-size: 10pt;">この性質を利用したのが株のペアトレード戦略</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■AIC（赤池情報量規準：Akaike's information criterion）：統計的モデルの予測性の良さを、観測値と理論値の差（残差）を用いて評価する統計量。値が小さいほど当てはまりが良いと言える。</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [68].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [69].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt; font-weight: bold;">単純に対数尤度がモデルのあてはなりの良さとしてはいけない理由は、対数尤度は説明変数を増やすと値が大きくなる特徴があるため。AICは説明変数の数（=モデルの複雑さ）とデータの適合度のバランスをとった指標になっている</span></div><div><span style="font-size: 10pt;">※AICは統計モデルの絶対的な基準ではない！！！AICが小さい方がよいモデルであるが、あくまでモデル評価の目安となる指標。</span></div><div><span style="font-size: 10pt;">→モデルのtrainデータでその値になっただけなので、testデータの予測精度が必ず高いというわけではない（過学習）</span></div><div><span style="font-size: 10pt;">→ARモデルなどの時系列モデルの場合、ノイズの分散がパラメータにあるので、分散が大きすぎると過学習になりやすい。</span></div><div><span style="font-size: 10pt; font-weight: bold;">ARモデルなどの時系列モデルはAICだけでなく、Ljung-Box検定（全ての自己相関係数が0かどうかを判断）などで、作成したモデルから出力される結果と実際の値との残差が、平均0、自己相関係数が全て0のホワイトノイズになっているかどうかで判断する必要がある</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■BIC（ベイズ情報量規準：Bayesian information criterion）：AICと類似のモデルの指標。BICは説明変数の数だけでなくサンプルサイズnもペナルティに加えた指標</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [70].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://research.miidas.jp/2019/04/aic%E3%81%A8bic%EF%BC%9F%E6%83%85%E5%A0%B1%E9%87%8F%E5%9F%BA%E6%BA%96%E3%81%A8%E3%81%AF%EF%BC%9F/" style="font-size: 10pt;">https://research.miidas.jp/2019/04/aic%E3%81%A8bic%EF%BC%9F%E6%83%85%E5%A0%B1%E9%87%8F%E5%9F%BA%E6%BA%96%E3%81%A8%E3%81%AF%EF%BC%9F/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■弱定常性：弱い定常性。背筋、分散、共分散が時点tに依存せず等しい</span></div><div><span style="font-size: 10pt;">■強定常性：強い定常性。各時点の確率分布が等しい</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ランダムウォーク：確率的に変動するデータ系列のこと</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ARCHモデル(Autoregressive Conditional Heteroskedasticity model:自己回帰条件付き分散不均一モデル)：</span></div><div><span style="font-size: 10pt;">分散不均一構造モデル（時刻tごとに分散が異なる確率分布から値が出るモデル）の代表例</span></div><div><span style="font-size: 10pt;">ホワイトノイズの分散についても非定常性を加味したARモデルともいえる</span></div><div><span style="font-size: 10pt; font-weight: bold;">前回絶対値の大きなノイズが来たのであれば今回の分散は大きくなるだろうとしたモデル</span></div><div><span style="font-size: 10pt;">株価の例：</span></div><div><span style="font-size: 10pt;">昨日A社の株がすごく下がった(=大きなノイズが来た)</span></div><div><span style="font-size: 10pt;">1.今日はみんなA社の株をたくさん買った（買い時だと思った）</span></div><div><span style="font-size: 10pt;">→株価は反発し上がった</span></div><div><span style="font-size: 10pt;">2.今日はみんなA社の株をたくさん売った（これ以上下がったら嫌だ）</span></div><div><span style="font-size: 10pt;">→株価はさらに下がった</span></div><div><span style="font-size: 10pt;">2つに共通しているのが大きなノイズがきたので株価がまた大きく動いたということ</span></div><div><span style="font-size: 10pt; font-weight: bold;">こういった株価に代表される動き（前日の値により次の日の値は分散変化した確率分散から生成される）はARモデルのようなホワイトノイズでは表すことはできませんがARCHモデルによって表すことができる</span></div><div><a href="https://qiita.com/knatsch/items/75c4cf115671081c5874" style="font-size: 10pt;">https://qiita.com/knatsch/items/75c4cf115671081c5874</a></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [71].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">→分散σ(t)^2が時刻により変動</span></div><div><span style="font-size: 10pt;">ε(t):ノイズ</span></div><div><span style="font-size: 10pt;">r(t):値</span></div><div><span style="font-size: 10pt;">μ:値の平均</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■GARCHモデル(Generalized ARCH model)：ARCHモデルを一般化したモデル</span></div><div><span style="font-size: 10pt;">ARCHモデルに条件付分散のラグ（σ2(t−i, i) = 1, 2, ...）を追加</span></div><div><span style="font-size: 10pt;">要は、t時点の分散（ボラティリティ）は過去の誤差の2乗と過去の分散（ボラティリティ）によって決まる構造</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [72].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="http://www.ier.hit-u.ac.jp/~iwaisako/class/Efinance2008/Efinance04_08dist.pdf" style="font-size: 10pt;">http://www.ier.hit-u.ac.jp/~iwaisako/class/Efinance2008/Efinance04_08dist.pdf</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div>■<span style="font-weight: bold;">CRISP-ML: CRISP-DMを拡張したもの</span></div><div><span style="font-weight: bold;">モデル導入後の結果のモニタリングとメンテナンスも含めた保守の工程を含めたCRISP-DM</span></div><div><a href="http://www.arxiv-vanity.com/papers/2003.05155/">http://www.arxiv-vanity.com/papers/2003.05155/</a></div><div><br/></div><div>データマイニングのための業界標準プロセス（<span style="font-weight: bold;">CRISP-DM=ビジネスの理解、データの理解、データの準備、モデリング、評価、デプロイの6つのフェーズで構成されたプロセス</span>）</div><div>CRISP-DMの欠点</div><div>・CRISP-DMは、MLモデルがアプリケーションとして維持されるアプリケーションシナリオをカバーしていません。</div><div>・第二に、さらに心配なことに、CRISP-DMには品質保証方法論に関するガイダンスが欠けています。</div><div><br/></div><div>モデルを維持しないリスクは、時間の経過に伴うパフォーマンスの低下であり、誤った予測につながり、後続のシステムでエラーを引き起こす可能性があります</div><div>→データの分布が変わる。株などはめっちゃ変わるが、象の画像分類なんかは変わらない</div><div>→ハードウェアの劣化</div><div>→システムの更新</div><div>モデル導入後のデータも監視してモデル再学習必要かやシステムやハード更新必要かどうかを検討することも忘れるなということ</div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div>■Sound event detection (SED)=音響イベント検出</div><div>ある期間（30秒間とか）に存在する人の声や楽音，環境音などを分類するタスク</div><div>アノテーションは各瞬間に動的な（時間変化する）ラベル付けをする必要がある</div><div>時間間隔が短いアノテーションが理想だが荒い（10秒間とか）ことが多い</div><div><br/></div><div><br/></div><div>■アクティブラーニング=能動学習</div><div>モデルの精度に関連するデータだけ学習させる学習手法。半教師学習や転移学習と同じレベルの話</div><div>代表的なgithub: <a href="https://github.com/modAL-python/modAL">https://github.com/modAL-python/modAL</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div>■Prophet：FacebookのCore Data Scienceチームが公開したオープンソースソフトウェア</div><div>    • 時系列データを予測するために、RまたはPythonで利用可能なフレームワーク</div><div>    • 特に毎年、毎週ごとの周期性や、休日などの影響を加味したモデルである点が特徴</div><div><a href="https://qiita.com/Umaremin/items/9cd463708d1a24a465ae">https://qiita.com/Umaremin/items/9cd463708d1a24a465ae</a></div><div><br/></div><div><img src="用語_files/Image [73].png" type="image/png" data-filename="Image.png" title="タップしてダウンロード"/></div><div>y(t)は時点tにおける予測値，</div><div>g(t)は時点tにおけるトレンド成分，</div><div>s(t)は時点tにおける周期成分，</div><div>h(t)は時点tにおける週末などのイレギュラーな成分，</div><div>そしてϵ(t)は時点tにおける誤差を表す．</div><div>各成分のパラメータをStanで推定し，モデルを学習する</div><div><a href="https://haltaro.github.io/2018/07/22/summer-prophet">https://haltaro.github.io/2018/07/22/summer-prophet</a></div><div><br/></div><div>→ARやARMAのように自己回帰や差分とるやり方とは別物みたい</div><div>→あくまでトレンドや季節性の関係に注目する時系列モデルみたい</div><div><br/></div><div>→入力する時系列データは、定常的(stationary)である必要はありますか？</div><div>prophetはトレンドや季節性といった、データの非定常性を検出して予測に利用しています。</div><div>定常的なデータには、トレンドも季節性もありませんので、定常的なデータをprophetに与えても、常に一定値を返す予測となるだけであり、これはprophetが力を発揮するようなデータではないとも言えます。</div><div><a href="https://exploratory.io/note/hideaki/Prophet-FAQ-3433248588798649">https://exploratory.io/note/hideaki/Prophet-FAQ-3433248588798649</a></div><div><br/></div><div>→欠損のある時系列データで予測を行うことはできますか？</div><div>はい、できます。欠損のある時系列データを扱えることは、他の多くの時系列予測アルゴリズムとの比較において、Prophetの特徴的な強みの一つです。</div><div><a href="https://exploratory.io/note/hideaki/Prophet-FAQ-3433248588798649">https://exploratory.io/note/hideaki/Prophet-FAQ-3433248588798649</a></div><div><br/></div><hr/><div><span style="font-size: 10pt; font-weight: bold;">DB系</span></div><div><span style="font-size: 10pt;">■SI Object Browser：Oracle、SQLServer、DB2、HiRDB、Symfoware、PostgreSQLのデータベース開発に必要な機能をオールインワンでサポートした「データベース開発支援ツール」。有償ライセンス。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Oracle SQL*Loader：データファイルからOracle DBのテーブルにデータを流し込むツール。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Materialized View：作成されたビューをメモリのキャッシュ領域などに保存し、再検索せずに何度も参照できるようにしたもの。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Amazon EMR(Elastic MapReduce)：AWS(Amazon Web Services)内部でHadoopを動かせる環境を提供してくれるサービス。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Hadoop：大量データを高速に分散処理するためのJavaフレームワーク。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Hive：Hadoop上でSQLを扱うアプリケーション。バッチ処理での利用を想定したつくり。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Presto：Hadoop上でSQLを扱うアプリケーション。大規模なデータセットに対してインタラクティブに結果を返せるよう開発されてるためHiveよりレスポンスが早い。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Spark:巨大なデータに対して高速に分散処理を行うオープンソースのフレームワーク。Scala、Java、R、Pythonなどに対応(APIが用意されてる)。</span></div><div><span style="font-size: 10pt;">　&lt;</span><a href="http://spark.apache.org/" rev="en_rl_none" style="font-size: 10pt;">http://spark.apache.org/</a><span style="font-size: 10pt;">&gt;</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■SparklyR：Apache Spark用のRインターフェース。</span></div><div><span style="font-size: 10pt;">　SparklyRパッケージはR からSparkに接続し、dplyr(or SQL)を使用してSparkデータにアクセスできる。</span></div><div><span style="font-size: 10pt;">　&lt;</span><a href="http://spark.rstudio.com/" rev="en_rl_none" style="font-size: 10pt;">http://spark.rstudio.com/</a><span style="font-size: 10pt;">&gt;</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■DB Browser for SQLite：SQLiteの管理ツール</span></div><div><span style="font-size: 10pt;">インストーラー:</span> <a href="http://sqlitebrowser.org/" style="font-size: 10pt;">http://sqlitebrowser.org/</a></div><div><span style="font-size: 10pt;">使い方:</span> <a href="https://www.dbonline.jp/sqlite-db-browser/database/index1.html" style="font-size: 10pt;">https://www.dbonline.jp/sqlite-db-browser/database/index1.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><hr/><div><span style="font-size: 10pt; font-weight: bold;">機械学習系</span></div><div><span style="font-size: 10pt;">■機械学習：人間が自然に行っている学習能力と同様の機能をコンピュータで実現しようとする技術。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Deep Learning：脳神経回路にヒントを得たニュートラルネットワークをベースにした機械学習の手法。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Tensorflow：Googleの機械学習ライブラリ。無償で利用可能。2017/2/15に「TensorFlow 1.0」をリリースした。</span></div><div><span style="font-size: 10pt;">　&lt;</span><a href="https://www.tensorflow.org/" rev="en_rl_none" style="font-size: 10pt;">https://www.tensorflow.org/</a><span style="font-size: 10pt;">&gt;</span></div><div><span style="font-size: 10pt;">　Google、機械学習ライブラリ「TensorFlow 1.0」をリリース</span></div><div><span style="font-size: 10pt;">　　&lt;</span><a href="http://internet.watch.impress.co.jp/docs/news/1044554.html" rev="en_rl_none" style="font-size: 10pt;">http://internet.watch.impress.co.jp/docs/news/1044554.html</a><span style="font-size: 10pt;">&gt;</span></div><div><span style="font-size: 10pt;">　Google Cloud Vision API：画像認識API。画像に写っている物、顔、企業ロゴなどを検知できる。</span></div><div><span style="font-size: 10pt;">　　&lt;</span><a href="https://cloud.google.com/vision/" rev="en_rl_none" style="font-size: 10pt;">https://cloud.google.com/vision/</a><span style="font-size: 10pt;">&gt;</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■TensorBoard：TensorFlowのデータを可視化するデバッグツール。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Tensorflow detection model zoo：COCOデータセット, Kittiデータセット, Open Imagesデータセットで事前に訓練された検出モデル</span></div><div><span style="font-size: 10pt;">　</span><a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md" rev="en_rl_none" style="font-size: 10pt;">https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Jupyter：Webブラウザ上でプログラム言語の記述・実行・表示・共有を行うツール。ノートブックと呼ばれる形式で作成したプログラムを実行しデータの分析などに利用する。</span></div><div><span style="font-size: 10pt;">　　&lt;</span><a href="http://jupyter.org/" rev="en_rl_none" style="font-size: 10pt;">http://jupyter.org/</a><span style="font-size: 10pt;">&gt;</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■JupyterHub：ユーザーごとのJupyter環境を管理するためのツール。</span> <span style="font-size: 10pt; color: rgb(0, 0, 0); font-family: verdana, arial, helvetica, sans-serif;">Jupyterを複数人で利用することができる。</span><span style="font-size: 10pt;">ブラウザ上で稼働する対話型実行環境Jupyterをリモートサーバー上で公開出来るサービス。</span></div><div><span style="font-size: 10pt;">　&lt;</span><a href="https://jupyterhub.readthedocs.io/en/latest/" rev="en_rl_none" style="font-size: 10pt;">https://jupyterhub.readthedocs.io/en/latest/</a><span style="font-size: 10pt;">&gt;</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■GPU(Graphics Processing Unit):リアルタイム画像処理に特化した演算装置ないしプロセッサ。ディープラーニングの計算を並列で高速に行うために利用する。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■CUDA(Compute Unified Device Architecture)：NVIDIA 社が提供する並列コンピューティング アーキテクチャ。GPUを利用して汎用演算を行うにはCUDAが必要。</span></div><div><span style="font-size: 10pt;">　&lt;</span><a href="http://www.nvidia.co.jp/object/cuda-parallel-computing-platform-jp.html" rev="en_rl_none" style="font-size: 10pt;">http://www.nvidia.co.jp/object/cuda-parallel-computing-platform-jp.html</a><span style="font-size: 10pt;">&gt;</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■cuDNN(NVIDIA CUDA Deep Neural Network library):CUDAのDeep Neural Network(DNN)開発用のライブラリ。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■nvidiaドライバ: NVIDIA製のGPUを使用できるようにするドライバ（ソフトウェア）。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Anaconda：データサイエンス向けのPythonパッケージなどを提供するプラットフォーム。</span> <span style="font-size: 10pt; color: rgb(84, 84, 84); font-family: arial, sans-serif;">モジュールやツールのコンパイル済みバイナリファイルを提供しており、簡単にPythonを利用する環境を構築</span><span style="font-size: 10pt;">できる。</span></div><div><span style="font-size: 10pt;">　&lt;</span><a href="https://www.anaconda.com/" rev="en_rl_none" style="font-size: 10pt;">https://www.anaconda.com/</a><span style="font-size: 10pt;">&gt;</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Pythonモジュール：Pythonのファイル（.py）の事</span></div><div><span style="font-size: 10pt;">■Pythonパッケージ：Pythonモジュールをいくつか集めてまとめたもの</span></div><div><span style="font-size: 10pt;">■Pythonライブラリ：いくつかのパッケージをまとめて一つのライブラリとしてインストールできるようにしたもの。NumPyなどはライブラリ</span></div><div><a href="https://ai-inter1.com/python-module_package_library/" style="font-size: 10pt;">https://ai-inter1.com/python-module_package_library/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■conda：Anacondaのパッケージ管理システム。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■pip：Pythonのパッケージ管理システム。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■numpy：Pythonの数値計算拡張モジュール。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Keras：Pythonのディープラーニングライブラリ。TensorFlowを使いやすくするためのライブラリ。</span></div><div><span style="font-size: 10pt;">　&lt;</span><a href="https://keras.io/ja/" rev="en_rl_none" style="font-size: 10pt;">https://keras.io/ja/</a><span style="font-size: 10pt;">&gt;</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■OpenCV（Open Source Computer Vision Library）：画像処理を行うためのライブラリ。JavaやPythonなどに対応したクロスプラットフォームなライブラリ。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■転移学習：学習済みモデルを別モデルの学習に適応させて学習効率を上げる手法。データ量が少ない場合でも精度の高いモデルできやすくなる。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Meta-Learning: 複数のタスクの学習結果や学習過程を利用し新しいタスクの学習効率を上げるような手法をメタ学習という。「学習の仕方を学習する」ような学習手法といってもよい。</span></div><div><span style="font-size: 10pt;">例えば、ImageNetのような大きなデータセットで学習された画像認識モデルを特定のタスク向けの学習データを使ってfinetune（微調整）するテクニックは広く使われているが、これもメタ学習。</span></div><div><a href="https://www.google.co.jp/amp/amazarashi.me/%25E3%2580%2590%25E8%25AB%2596%25E6%2596%2587%25E3%2583%25A1%25E3%2583%25A2%25E3%2580%2591meta-transfer-learning-for-few-shot-learning/?amp=1" style="font-size: 10pt;">https://www.google.co.jp/amp/amazarashi.me/%25E3%2580%2590%25E8%25AB%2596%25E6%2596%2587%25E3%2583%25A1%25E3%2583%25A2%25E3%2580%2591meta-transfer-learning-for-few-shot-learning/%3famp=1</a></div><div><a href="https://tech.nikkeibp.co.jp/atcl/nxt/mag/rob/18/00007/00009/" style="font-size: 10pt;">https://tech.nikkeibp.co.jp/atcl/nxt/mag/rob/18/00007/00009/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Fine-tuning：既存の学習済みモデルを再利用して、新しいモデルを構築する手法。他の画像データで学習したモデルを使うことで学習時間を短縮でき、少ない訓練データでも良いモデルができる。fine-tunningは元のモデルに何か手を加える（転移学習は元のモデルそのまま使う）</span></div><div><span style="font-size: 10pt;">具体的には、特徴抽出に使用される凍結された畳み込みベースの出力側の層をいくつか解凍し、モデルの新しく追加された部分（この場合は全結合分類器）と解凍した層の両方で訓練を行う</span></div><div><span style="font-size: 10pt;">Fine-tuningが有効なのはtrain setに各クラス数十〜数百データあるとき。1〜5枚程度しかないときは過学習を起こしてしまうため不適、この場合はFew-Shot learning が適している。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Few-Shot Learning: 新しいクラスの画像が極端に制限されている場合のモデル構築手法。新しいクラスの画像が1〜5枚程度の場合での認識精度を競うタスク。</span></div><div><span style="font-size: 10pt;">Few-Shot Learningの問題設定について述べます。まず大きく分けて以下の2つのデータセットが与えられます。</span></div><div><span style="font-size: 10pt;">各クラスにつき潤沢に正解ラベルが使用できるデータ（訓練データ）</span></div><div><span style="font-size: 10pt;">各クラス1〜5枚数程度の正解ラベルが使用できるデータ（サポートデータ）</span></div><div><span style="font-size: 10pt;">1と2のデータセットのラベルの積集合は空集合、つまり共有しているラベルは1つもありません。</span></div><div><span style="font-size: 10pt;">基本的には訓練データで特徴抽出器を学習し、サポートデータのクラスの認識精度を競います。</span></div><div><a href="https://qiita.com/minux302/items/d596785a321eca72c9a1" style="font-size: 10pt;">https://qiita.com/minux302/items/d596785a321eca72c9a1</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■順伝播: ニューラルネットワークの入力層から出力層に向けた伝播</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■逆伝播: 順伝播とは逆方向にデータ（勾配）を伝播すること</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■損失関数：正解データとモデルからの予測値(出力値)の誤差を計算する関数。機械学習の目標はこの関数の値を0に近づけること。</span></div><div><span style="font-size: 10pt;">　目的関数とかコスト関数とか呼ばれたりもする。</span></div><div><span style="font-size: 10pt;">　損失関数は、クロスエントロピーなど、問題に応じて何種類かの代表的な関数を使うことが多い。</span></div><div><span style="font-size: 10pt;">　正しい（期待する）出力に近ければ近いほど損失関数の値は小さくなっていく。</span></div><div><span style="font-size: 10pt;">　損失関数の値が小さくなるような出力となるように、重みやバイアスなどのパラメータを調整すれば、正しい判断のできるニューラルネットワークになる。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■クロスエントロピー誤差（交差エントロピー誤差:Cross Entropy Error）: 多クラス分類の損失関数</span></div><div><span style="font-size: 10pt;">　交差エントロピー誤差は、ニューラルネットワークが出力する各クラスの「確率」と「正解ラベル」から求められる。</span></div><div><span style="font-size: 10pt;">　式は E(w)=-(y1*log(x1(w))+y2*log(x2(w))+…)。wが重み。yが正解値。x(w)が出力値（確率）。出力は重みに依存するので、出力値xを変数wの関数として表現している。</span></div><div><span style="font-size: 10pt;">　</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [74].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">　</span> <a href="https://qiita.com/celaeno42/items/7efdbb1491406f4bde96" rev="en_rl_none" style="font-size: 10pt;">https://qiita.com/celaeno42/items/7efdbb1491406f4bde96</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■シグモイド関数(sigmoid): 1要素（1クラス）の入力値を確率に変換する関数</span></div><div><span style="font-size: 10pt;">　入力として任意の実数を受け取り、0.0～1.0の間の実数を出力する。入力が0.0～1.0に変換されるので、（2クラスの内の）ある1クラスの「確率」として解釈できる。</span></div><div><span style="font-size: 10pt;">　式: y = 1/(1+exp(-x))</span></div><div><span style="font-size: 10pt;">　</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [75].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">　</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [76].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ソフトマックス関数(softmax): 各要素（複数クラス）の入力値を確率に変換する関数</span></div><div><span style="font-size: 10pt;">　出力の各要素は 0.0 以上 1.0 以下の実数であり、全要素の出力をすべて足し合わせると 1.0 になるため、ソフトマックスの出力は「確率」として解釈できる。</span></div><div><span style="font-size: 10pt;">　式: y1 = exp(x1)/(exp(x1)+exp(x2)+…), y2 = exp(x2)/(exp(x1)+exp(x2)+…), …</span></div><div><span style="font-size: 10pt;">　※y_sum = y1 + y2 + … = 1.0</span></div><div><span style="font-size: 10pt;">　</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [77].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■勾配（gradient）: 各要素（各ベクトル）の偏微分（を行列形式でまとめたもの）</span></div><div><span style="font-size: 10pt;">　例）L = f(x,y,z)(=x^2+y+3z+9 みたいな式)の勾配は ∂L/∂X = (∂L/∂x, ∂L/∂y, ∂L/∂z)</span></div><div><span style="font-size: 10pt;">　数学で「勾配」と言うと、それはベクトルに対しての微分に限定されます。</span></div><div><span style="font-size: 10pt;">　一方、ディープラーニングの分野では、行列やテンソルについても微分を定義し、それを「勾配」と呼ぶのが一般的。</span></div><div><span style="font-size: 10pt;">　※偏微分∂y/∂xの意味:</span></div><div><span style="font-size: 10pt;">　　xの値を“少しだけ”(dxだけ)変化させたときに、yの値がどれだけ変化するか、という「変化の度合い」であり、関数yの「傾き」に相当する</span></div><div><span style="font-size: 10pt;">　</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [78].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■勾配法：関数の勾配（微分）を最適化に用いる手法の総称</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■勾配降下法: 損失関数の最小化に用いられる重みの最適化手法</span></div><div><span style="font-size: 10pt;">　関数f(x)の傾き(勾配)からパラメタxの値を更新して、関数の最小値を求める</span></div><div><span style="font-size: 10pt;">　→（損失関数の）傾き(勾配)を計算（微分）してその傾きの大きさとは逆方向にパラメータ（重み）を調整し、関数の値を最小化する</span></div><div><span style="font-size: 10pt;">　　式: w = w -ρ(∂L(w)/∂w)</span></div><div><span style="font-size: 10pt;">　　w:重み。L(w): 損失関数。ρ: 学習率。傾きの大きさとは逆方向に重みを調整していく大きさ。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [79].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://www.imagazine.co.jp/%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E3%82%92%E7%90%86%E8%A7%A3%E3%81%99%E3%82%8B%E3%80%80%EF%BD%9E%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97/" style="font-size: 10pt;">https://www.imagazine.co.jp/%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E3%82%92%E7%90%86%E8%A7%A3%E3%81%99%E3%82%8B%E3%80%80%EF%BD%9E%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■最急降下法(Gradient Descent): すべての誤差の合計を取ってからパラメタを更新する勾配降下法</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■確率的勾配降下法（Stochastic Gradient Descent - SDG）: 学習データの中からランダムに1つを取り出して誤差を計算してパラメタを更新する勾配降下法</span></div><div><span style="font-size: 10pt;">　＜確率的勾配降下法のメリット＞</span></div><div><span style="font-size: 10pt;">　・1つの学習データだけで更新するため、計算が早い</span></div><div><span style="font-size: 10pt;">　・ランダムにデータを取り出して勾配を下ろうとするため、最急降下法よりも局所解に陥る可能性が小さい</span></div><div><span style="font-size: 10pt;">　・1つの学習データだけで更新するため、新しいデータだけを学習させて、学習結果を更新することができる。このような学習方法をオンライン学習という</span></div><div><span style="font-size: 10pt;">　＜確率的勾配降下法のデメリット＞</span></div><div><span style="font-size: 10pt;">　・ランダムにデータを取り出しているので、最短で最適解にたどりつかない</span></div><div><span style="font-size: 10pt;">　・1つの学習データだけで更新するため、例外（異常）データに引っ張られやすい</span></div><div><span style="font-size: 10pt;">　</span><a href="https://shironeko.hateblo.jp/entry/2016/10/29/173634" rev="en_rl_none" style="font-size: 10pt;">https://shironeko.hateblo.jp/entry/2016/10/29/173634</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ミニバッチ確率的勾配降下法（Minibatch SGD - MSGD）: 学習データの中からランダムにいくつかのデータを取り出して誤差を計算してパラメタを更新する勾配降下法。このときの一回に取り出すデータの数をバッチサイズと呼ぶ。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■誤差逆伝播法</span></div><div><span style="font-size: 10pt;">　出力側に近い勾配の値を算出し、順々に入力側の勾配を求めていく逆伝播の方法</span></div><div><span style="font-size: 10pt;">　誤差逆伝播法の重みの更新は以下の手順で行われる</span></div><div><span style="font-size: 10pt;">　1.ミニバッチ</span></div><div><span style="font-size: 10pt;">　→訓練データの中からランダムに複数のデータを選び出す</span></div><div><span style="font-size: 10pt;">　2. 勾配の算出</span></div><div><span style="font-size: 10pt;">　→誤差逆伝播法により、各重みパラメータに関する損失関数の勾配を求める</span></div><div><span style="font-size: 10pt;">　3. パラメータの更新</span></div><div><span style="font-size: 10pt;">　→勾配を使って重みパラメータを更新する</span></div><div><span style="font-size: 10pt;">　4. 繰り返す</span></div><div><span style="font-size: 10pt;">　→Step-1、Step-2、Step-3 を必要な回数だけ繰り返す</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [80].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://www.imagazine.co.jp/%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E3%82%92%E7%90%86%E8%A7%A3%E3%81%99%E3%82%8B%E3%80%80%EF%BD%9E%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97/" style="font-size: 10pt;">https://www.imagazine.co.jp/%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E3%82%92%E7%90%86%E8%A7%A3%E3%81%99%E3%82%8B%E3%80%80%EF%BD%9E%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■正則化: 機械学習モデルが学習する際に過学習が発生するのを防ぐための仕組み。過学習が発生しているときは、特定の入力値に対する機械学習モデルの係数が大きくなる傾向がある。正則化はその傾向を利用して、モデルの係数が多いほど学習時にペナルティを与えて機械学習モデルの係数をなるべく小さくしようとする。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■</span>weight decay: deep learningの正則化手法。重みの更新にl2正則化を入れること。パラメータはl2正則化の定数ラムダ。sgdと相性がよくadamだと学習率を自動調整するためかうまくいかないらしい</div><div><br/></div><div>■AdamW:</div><div><a href="http://www.arxiv-vanity.com/papers/1711.05101/">http://www.arxiv-vanity.com/papers/1711.05101/</a></div><div>Adamの基本のアルゴリズムからWeight Decayに関する式を変更した。</div><div>自動調整された学習率の場合は、もともと期待していたWeight Decayの結果が得られず、精度が下がる事象が得られるよう。</div><div>その事象を回避するために式を変更している。</div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Lasso回帰: 正則化された線形回帰。線形回帰に「学習した重みの合計（L1正則化項）を加えたもの。Lasso回帰はリッジ回帰と違って不要と判断される説明変数の係数（重み）が0になる性質があり、つまりモデル構築においていくつかの特徴量（説明変数）が完全に無視される</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■リッジ回帰: 正則化された線形回帰。線形回帰に「学習した重みの二乗（L2正則化項）」を加えたもの。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■共変量シフト: データの分布が訓練時と推定時で異なるような状態のこと</span></div><div><span style="font-size: 10pt;">訓練中にネットワーク内の各層の間で起きる共変量シフトを内部共変量シフトと言う</span></div><div><a href="https://qiita.com/cfiken/items/b477c7878828ebdb0387" style="font-size: 10pt;">https://qiita.com/cfiken/items/b477c7878828ebdb0387</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■バギング: 弱学習器を並列に学習して組み合わせる手法</span></div><div><span style="font-size: 10pt;">　<span style="font-size: 10pt; font-weight: bold;">入力データを重複許してランダムサンプリング（=ブートストラップ:bootstrap）してモデルを複数作り、最後に全てのモデルで多数決（回帰なら平均）する（=集約:aggregating）アンサンブル学習</span></span></div><div><span style="font-size: 10pt;">　複数のモデルを並列に学習でき、過学習しにくいのが利点</span></div><div><img src="用語_files/Image [81].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ランダムフォレスト: 決定木を使ったバギング</span></div><div><span style="font-size: 10pt;">　決定木の説明変数（入力）である特徴量ベクトルの次元をランダムに決めてバギングする</span></div><div><span style="font-size: 10pt;"><br/></span></div><div><span style="font-size: 10pt;">■</span>Extra-Trees(Extremely randomized trees): 決定木ベースのアンサンブルモデル</div><div><span style="font-weight: bold;">特徴量ランダムサンプリング + 訓練データ全部使う（ランダムフォレストみたいにランダムサンプリングしない） + 木の分類の基準をジニ係数、エントロピーのどちらかをランダムに決める。基準ランダムに変更するのが肝</span></div><div>上記のルールで作成した複数の決定木を多数決する</div><div>ランダムフォレストと同じくらいの精度出るみたい。手法の違いは以下</div><div><img src="用語_files/Image [82].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ブースティング: 弱学習器を順番に学習して組み合わせる手法</span></div><div><span style="font-size: 10pt;">　弱学習器に誤分類されたデータを次の弱学習器に学習させて誤分類された値の重みを大きくして間違ったデータを優先的に正解できるようにするのを繰り返す。最後に全てのモデルで多数決（回帰なら平均）する（=集約:aggregating）アンサンブル学習</span></div><div><span style="font-size: 10pt;">　バギングより精度が良いが、弱学習器増やしすぎると過学習しやすい。順番に学習するので学習に時間が掛かる。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■アダブースティング（適応的ブースティング:Adaptive Boosting）: 重み付きのブースティング</span></div><div><span style="font-size: 10pt;">　多数決は sigmoid(t1*model1 + t2*model2 + …) で決める。</span></div><div><span style="font-size: 10pt;">　学習は順番に作っていく弱学習器:model_j、各model信頼率:t_j、各サンプルの重み:w_iを決める</span></div><div><span style="font-size: 10pt;">　AdaBoostでは損失関数に指数誤差関数を使用</span></div><div><span style="font-size: 10pt;">　＜AdaBoostの処理概要＞</span></div><div><span style="font-size: 10pt;">　１．サンプル（特徴ベクトルと教師信号であるクラスの組）への重みは，最初は1/N (Nはサンプル数）で初期化します．</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [83].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">　２．T個弱識別器を作るとします．</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [84].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">　３．（まずは，１つ目の）弱識別器tを作ります．弱識別器tの作り方は，重み付きのサンプルを利用して，一般的な機械学習アルゴリズムを利用して学習します．</span></div><div><span style="font-size: 10pt;">　４．弱識別器tの誤り率を求めます．誤り率は，各サンプルを見たときに，サンプルのクラスと弱識別器の出力クラスが一致しないサンプルの重みを足し合わせたものになります．</span></div><div><span style="font-size: 10pt;">　５．誤り率から弱識別器tの信頼度を求めます．具体的には，誤り率が小さいほど，信頼度が大きくなるようにします．</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [85].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">　６．サンプルへの重みを更新します．サンプルへの重みは，弱識別器tが正しく識別できたサンプルは重みが低くなるように更新します．弱識別器tが間違って識別したサンプルは重みが高くなるように更新します．</span></div><div><span style="font-size: 10pt;">　７．サンプルの重みの和が１になるように正規化します．</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [86].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">　８．３．に戻って，t+1個目の弱識別器を作ります．T個作れたら，アルゴリズム終了です．</span></div><div><span style="font-size: 10pt;">　９．最終的な識別は，全ての弱識別器を信頼度で重みづけして多数決を取ります．</span></div><div><span style="font-size: 10pt;">　　　→sigmoid(t1*model1 + t2*model2 + …)</span></div><div><span style="font-size: 10pt;">　</span> <a href="http://e-biz.cocolog-nifty.com/blog/2010/09/adaboost-b724.html" rev="en_rl_none" style="font-size: 10pt;">http://e-biz.cocolog-nifty.com/blog/2010/09/adaboost-b724.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■勾配ブースティング（Gradient Boosting）: 残差誤差にフィットするように（損失関数(f=y-xみたいなの)を最小化するように）弱学習器を修正するブースティング</span></div><div><span style="font-size: 10pt;">Gradient Boostingは勾配降下法（損失関数の微分である勾配を計算→勾配が負になる方向に学習率分少し移動→パラメータ更新→勾配を計算することを繰り返してパラメータ最小化していく）を使うため損失関数は微分可能である必要がある</span></div><div><span style="font-size: 10pt; font-weight: bold;">勾配ブースティングはそれぞれの弱学習器の誤差を学習することに最大の特徴がある</span></div><div><span style="font-size: 10pt;">XGBoostもLightGBMもこの「勾配ブースティング」を扱いやすくまとめたフレームワーク</span></div><div><span style="font-size: 10pt;">＜勾配ブースティングの処理の流れ＞</span></div><div><span style="font-size: 10pt;">　仮に最初に訓練を行う決定木を1号、次に訓練を行う決定木を2号としましょう。</span></div><ol><li><div><span style="font-size: 10pt;">　決定木（1号）でモデル訓練を行い推測結果を評価します。F1(x)=y</span></div></li><li><div><span style="font-size: 10pt;">　決定木（1号）の</span><span style="font-size: 10pt; font-weight: bold;">推測結果と実際の値の「誤差」(h1(x)=y-F1(x))を訓練データとして、決定木（2号）の訓練を行います。</span></div></li><li><div><span style="font-size: 10pt;">　N号の決定木はN-1号の決定木の誤差（Residuals）を学習する。</span></div></li></ol><div><span style="font-size: 10pt;">　</span><a href="https://www.codexa.net/lightgbm-beginner/" rev="en_rl_none" style="font-size: 10pt;">https://www.codexa.net/lightgbm-beginner/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■GBDT(Gradient Boosting Decision Tree):</span></div><div><span style="font-size: 10pt;">GBDTとは「勾配降下法(Gradient)」と「Boosting(アンサンブル)」、「決定木(Decision Tree)」を組み合わせた手法</span></div><div><span style="font-size: 10pt;">&lt;GBDTの手順&gt;</span></div><div><a href="https://www.acceluniverse.com/blog/developers/2019/12/gbdt.html" style="font-size: 10pt;">https://www.acceluniverse.com/blog/developers/2019/12/gbdt.html</a></div><div><span style="font-size: 10pt;">1.最初の予測として目的変数の平均を計算する（予測値の初期値は教師データの平均からスタート）</span></div><div><span style="font-size: 10pt;"> (900+500+400+800+700+600)/6=650</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [87].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">2.目的変数と予測の誤差を計算する（例.「誤差１」＝「給料の値（目的変数）」ー「予測１」）</span></div><div><span style="font-size: 10pt;"> 0番目の誤差 = 900 - 650 = 250</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [88].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">3.</span><span style="font-size: 10pt; font-weight: bold;">説明変数に</span><span style="font-size: 10pt; font-weight: bold;">予測の</span><span style="font-size: 10pt; font-weight: bold;">誤差を追加して、末端の葉の条件が</span><span style="font-size: 10pt; font-weight: bold;">誤差である</span><span style="font-size: 10pt; font-weight: bold;">決定木を</span><span style="font-size: 10pt; font-weight: bold;">作る</span></div><div><span style="font-size: 10pt; font-weight: bold;"><img src="用語_files/Image [89].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">4.予測値、学習率、構築した決定木の末端の葉を使って、新たな予測値を求める（例.予測２ = 予測１(ステップ１) + 学習率 * 誤差）</span></div><div><span style="font-size: 10pt;"> 0番目の予測 = 650 + 0.1 * 200 = 670</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [90].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">5.再び目的変数と予測の誤差を計算する（例.「誤差２」＝「給料の値（目的変数）」ー「予測2」）</span></div><div><span style="font-size: 10pt;"> 0番目の誤差 ＝ 900 - 670 = 230</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [91].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">6.3~5を繰り返す</span></div><div><span style="font-size: 10pt;">7.最終予測を行う（アンサンブル内のすべての決定木を使用して、給料の最終的な予測を行います。最終的な予測は、最初に計算した平均に、学習率を掛けた決定木をすべて足した値）</span></div><div><span style="font-size: 10pt;"> 0番目の最終予測 = 650 + 0.1*250 + 0.1*230 + 0.1*&lt;誤差3&gt;</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [92].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■スタッキング(stacking): 初段の学習器の出力結果を次段の入力結果とするアンサンブル学習</span></div><div><a href="http://segafreder.hatenablog.com/entry/2016/05/24/235822" style="font-size: 10pt;">http://segafreder.hatenablog.com/entry/2016/05/24/235822</a></div><div><span style="font-size: 10pt;">＜スタッキングの手順＞</span></div><div><span style="font-size: 10pt;">Out-of-Fold Predictions</span></div><div><span style="font-size: 10pt;">1. CV作成</span></div><div><span style="font-size: 10pt;">　スタッキングでは、第二モデルの学習データに、第一のベースモデルの予測が使用されます。</span></div><div><span style="font-size: 10pt;">　しかし、全ての学習・テストデータを一度に使ってしまうと、ベースモデルが既にテストデータを見た状態にあるため、第二モデルでオーバーフィットするリスクがあります。</span></div><div><span style="font-size: 10pt;">　そのため、交差検証を施します。</span></div><div><span style="font-size: 10pt;">2. 第一のベースモデルとして、複数モデルを準備</span></div><div><span style="font-size: 10pt;">3. ベースモデルの予測値を使って、第二モデルを学習</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Batch Normalization: </span></div><div><span style="font-size: 10pt; font-weight: bold;">1ミニバッチ内の全データの</span><span style="font-size: 10pt; color: rgb(253, 4, 4); font-weight: bold;">同一チャネル</span><span style="font-size: 10pt; font-weight: bold;">が平均0,分散1に正規化する</span><span style="font-size: 10pt; font-weight: bold;">正則化手法</span><span style="font-size: 10pt;">。</span></div><div><span style="font-size: 10pt;">　ミニバッチ毎に平均と分散を計算して入力を標準化(x-平均/標準偏差)し、それに</span><span style="font-size: 10pt; font-weight: bold;">係数γ</span><span style="font-size: 10pt;">とバイアスβを加えたものを出力(x_j = x_i-平均/(分散+ε)^0.5 出して(εは標準偏差が0になってしまうのを防ぐ、1e−8のような小さな値)、 y_j = γ*x_j + β でy_jがBatch Normalizationの出力)するという正則化手法。</span><span style="font-size: 10pt; font-weight: bold;">係数γがバッチノーマライゼーションで学習するパラメータ</span></div><div><span style="font-size: 10pt;">　このおかげでニューラルネットワークを多層にしても勾配消失・爆発が避けれている。</span></div><div><span style="font-size: 10pt;">　全結合のニューラルネットワークの場合、Affinの後、活性化(例:ReLU)の前にBN入れる。</span></div><div><span style="font-size: 10pt;">　CNNの場合、Convolutionの後、活性化(例:ReLU)の前にBN入れる。</span></div><div><span style="font-size: 10pt;">　</span><a href="https://qiita.com/t-tkd3a/items/14950dbf55f7a3095600" rev="en_rl_none" style="font-size: 10pt;">https://qiita.com/t-tkd3a/items/14950dbf55f7a3095600</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■バッチノーマライゼーションの弱点など</span></div><div><span style="font-size: 10pt;">・バッチサイズが小さいと使えない </span></div><div><span style="font-size: 10pt;">→例えば極端な話バッチサイズが1だったら、もちろん正規化なんて無理。 </span></div><div><span style="font-size: 10pt;">エッジコンピュータなどリソースが限られているときはバッチサイズ大きくできないから問題になる。 </span></div><div><span style="font-size: 10pt;">・RNNなどの時系列データに使えない RNNが時系列データに対して広く使われているのを考えると致命的！</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">・データの統計情報を使うバッチノームは、異なるデータを使う転移学習に向かないらしい（言われてみたら、バッチノーマライゼーションの係数γは転移元のデータセットの統計情報から学習した値だからそんな気する）</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">・ちなみにバッチノーマライゼーションがなんで効果的なのかの理論的な証明は出来ていないらしい（201912時点）</span></div><div><a href="https://qiita.com/omiita/items/01855ff13cc6d3720ea4#3-%E3%83%90%E3%83%83%E3%83%81%E3%83%8E%E3%83%BC%E3%83%9E%E3%83%A9%E3%82%A4%E3%82%BC%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3%E3%81%AE%E4%BB%B2%E9%96%93%E3%81%9F%E3%81%A1" style="font-size: 10pt;">https://qiita.com/omiita/items/01855ff13cc6d3720ea4#3-%E3%83%90%E3%83%83%E3%83%81%E3%83%8E%E3%83%BC%E3%83%9E%E3%83%A9%E3%82%A4%E3%82%BC%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3%E3%81%AE%E4%BB%B2%E9%96%93%E3%81%9F%E3%81%A1</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■バッチノーマライゼーションの派生</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■レイヤーノーマライゼーション：</span></div><div><span style="caret-color: rgb(51, 51, 51); -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); text-size-adjust: auto;"><span style="font-size: 10pt; color: rgb(51, 51, 51); font-family: -apple-system, &quot;Segoe UI&quot;, &quot;Helvetica Neue&quot;, &quot;Hiragino Kaku Gothic ProN&quot;, メイリオ, meiryo, sans-serif; font-weight: bold;">1つのデータの全チャネルに対して正規化</span></span></div><div><br/></div><div><font style="font-size: 10pt;"><span style="caret-color: rgb(51, 51, 51); -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); font-size: 10pt; color: rgb(51, 51, 51); font-family: -apple-system, &quot;Segoe UI&quot;, &quot;Helvetica Neue&quot;, &quot;Hiragino Kaku Gothic ProN&quot;, メイリオ, meiryo, sans-serif; font-weight: bold;">■インスタンスノーム：1つのデータの1つのチャネルに対して正規化</span><span style="caret-color: rgb(51, 51, 51); -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); font-size: 10pt; color: rgb(51, 51, 51); font-family: -apple-system, &quot;Segoe UI&quot;, &quot;Helvetica Neue&quot;, &quot;Hiragino Kaku Gothic ProN&quot;, メイリオ, meiryo, sans-serif;"> </span></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■</span><span style="caret-color: rgb(51, 51, 51); -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); font-size: 10pt; color: rgb(51, 51, 51); font-family: -apple-system, &quot;Segoe UI&quot;, &quot;Helvetica Neue&quot;, &quot;Hiragino Kaku Gothic ProN&quot;, メイリオ, meiryo, sans-serif; font-weight: bold;">グループノーム：</span><span style="caret-color: rgb(51, 51, 51); -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); font-size: 10pt; color: rgb(51, 51, 51); font-family: -apple-system, &quot;Segoe UI&quot;, &quot;Helvetica Neue&quot;, &quot;Hiragino Kaku Gothic ProN&quot;, メイリオ, meiryo, sans-serif;">これは</span><span style="caret-color: rgb(51, 51, 51); -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); background-color: rgb(255, 255, 255); font-size: 10pt; color: rgb(51, 51, 51); font-family: -apple-system, &quot;Segoe UI&quot;, &quot;Helvetica Neue&quot;, &quot;Hiragino Kaku Gothic ProN&quot;, メイリオ, meiryo, sans-serif;"> </span><span style="caret-color: rgb(51, 51, 51); -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); font-size: 10pt; color: rgb(51, 51, 51); font-family: -apple-system, &quot;Segoe UI&quot;, &quot;Helvetica Neue&quot;, &quot;Hiragino Kaku Gothic ProN&quot;, メイリオ, meiryo, sans-serif; font-weight: bold;">1つのデータの任意のチャネル数に対して正規化</span></div><div><br/></div><div><font style="font-size: 10pt;"><span style="caret-color: rgb(51, 51, 51); letter-spacing: normal; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); -webkit-text-stroke-width: 0px; font-size: 10pt;">→</span><span style="caret-color: rgb(51, 51, 51); letter-spacing: normal; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); -webkit-text-stroke-width: 0px; font-size: 10pt; color: rgb(51, 51, 51); font-family: -apple-system, &quot;Segoe UI&quot;, &quot;Helvetica Neue&quot;, &quot;Hiragino Kaku Gothic ProN&quot;, メイリオ, meiryo, sans-serif; font-variant-caps: normal;">実はいずれの方法もバッチノームの性能を越えることができていない。</span></font></div><div><span style="caret-color: rgb(51, 51, 51); letter-spacing: normal; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); text-size-adjust: auto; -webkit-text-stroke-width: 0px;"><span style="font-size: 10pt; color: rgb(51, 51, 51); font-family: -apple-system, &quot;Segoe UI&quot;, &quot;Helvetica Neue&quot;, &quot;Hiragino Kaku Gothic ProN&quot;, メイリオ, meiryo, sans-serif; font-variant-caps: normal; font-weight: bold;">あくまでもバッチサイズが小さい時にしかバッチノームよりも良い効果を発揮しない</span></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><span style="font-size: 10pt;-en-paragraph:true;">■重み標準化（Weight Standardization）:</span><span style="font-size: 10pt;-en-paragraph:true;">出力チャンネル毎に重みを標準化する。</span><span style="font-size: 10pt;-en-paragraph:true;">Group Normalization</span><span style="font-size: 10pt;-en-paragraph:true;">とあわせて適用した場合バッチサイズが</span><span style="font-size: 10pt;-en-paragraph:true;">1</span><span style="font-size: 10pt;-en-paragraph:true;">でも</span><span style="font-size: 10pt;-en-paragraph:true;">BN</span><span style="font-size: 10pt;-en-paragraph:true;">と同じか超える効果を得られる</span><span style="font-size: 10pt;-en-paragraph:true;"> </span><span style="-en-paragraph:true;"><a href="http://arxiv.org/abs/1903.10520" style="font-size: 10pt;">arxiv.org/abs/1903.10520</a></span></font></div><div><br/></div><div><span style="font-size: 10pt;">■Open Images Dataset：Googleが提供する機械学習用の画像データセット。6000カテゴリのラベル付き900万件画像がある。（20180122時点）</span></div><div><a href="https://github.com/openimages/dataset" style="font-size: 10pt;">https://github.com/openimages/dataset</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■MNIST：28×28ピクセルの手書き数字のデータセット。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Fashion-MNIST：手書き文字のMNISTのファッション画像版。</span></div><div><span style="font-size: 10pt;">　60,000サンプルの訓練セットと10,000サンプルのテストセットから成る。</span></div><div><span style="font-size: 10pt;">　各サンプルは28×28グレースケール画像で10クラス（T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot）のラベルを持つ。</span></div><div><span style="font-size: 10pt;">　</span><a href="https://github.com/zalandoresearch/fashion-mnist" rev="en_rl_none" style="font-size: 10pt;">https://github.com/zalandoresearch/fashion-mnist</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■CIFAR10：一般物体認識のベンチマークの画像データセット。</span></div><div><span style="font-size: 10pt;">　50,000サンプルの訓練セットと10,000サンプルのテストセットから成る。</span></div><div><span style="font-size: 10pt;">　各サンプルは32×32カラー画像で10クラス（airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck）のラベルを持つ。</span></div><div><span style="font-size: 10pt;">　</span><a href="https://www.cs.toronto.edu/~kriz/cifar.html" rev="en_rl_none" style="font-size: 10pt;">https://www.cs.toronto.edu/~kriz/cifar.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Nesterov accelerated gradient (NAG) ：「慣性に加えて⼀歩先の位置での勾配を使おう」 現在のパラメータの勾配ではなく、未来のパラメータの推定を計算することで効率的な予測が可能になる</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■AMSGrad：Adamと比べてより小さい学習率を使用し，学習率に過去の勾配の勾配の影響をゆっくりと減衰させる</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■VGG(Visual Geometry Group)16は2014年のILSVRC（ImageNet Large Scale Visual Recognition Challenge）で提案された畳み込み13層とフル結合3層の計16層から成る畳み込みニューラルネットワーク。出力層は1000ユニットあり、1000クラスを分類する。</span></div><div><span style="font-size: 10pt;">　ImageNetと呼ばれる大規模な画像データセットを使った学習済みモデルが公開されている。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Grad-CAM（gradcam:Gradient-weighted Class Activation Mapping）：あるクラスについてCNN（畳み込みニューラルネットワーク）が分類するのに注目した領域をヒートマップで表示する技術。</span></div><div><span style="font-size: 10pt;">Grad-CAM はCNNの特徴マップ（フィルタをかけた出力画像）と指定クラスについての損失関数の勾配を計算し、掛け合わせることで各クラスに影響する部分を可視化する。</span></div><div><span style="font-size: 10pt;">→予測クラス(y)と特徴量マップ（フィルタをかけた出力画像）のピクセル値(x)のloss(f(x))の勾配が大きい所が予測クラスにとって重要と考えている。</span></div><div><span style="font-size: 10pt;">（その他特徴：</span><a href="http://www.thothchildren.com/chapter/5a54ef797ae87e33511e0d68" rev="en_rl_none" style="font-size: 10pt;">http://www.thothchildren.com/chapter/5a54ef797ae87e33511e0d68</a></div><ul><li><div><span style="font-size: 10pt;">以下の手順で実現する。</span></div></li></ul><div><span style="font-size: 10pt;">        1.最後の出力において、対象としているクラス等のみを1にして他を0にした状態で誤差逆伝播をする。</span></div><div><span style="font-size: 10pt;">        2.CNNの最終層についたら、GAP(Global Average Pooling)によって各チャネルについて損失関数の勾配の平均を計算する。</span></div><div><span style="font-size: 10pt;">        3. 2.で計算された各チャネルの勾配の平均に従い、ヒートマップを生成する。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [93].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://qiita.com/kinziro/items/69f996065b4a658c42e8" style="font-size: 10pt;">https://qiita.com/kinziro/items/69f996065b4a658c42e8</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Grad-CAMをもう少し詳しく解説</span></div><div><span style="font-size: 10pt;">CNNでは入力からの出力に近づいた層ほど、解像度が低く、最終的な判定結果と紐づいた特徴を抽出するとされており、領域の抽出には適していると考えられます。</span></div><div><span style="font-size: 10pt;">したがって、出力に最も近い、最後の畳み込み層の特徴マップを可視化すれば、判定結果と最も紐づく領域が得られそうです。</span></div><div><span style="font-size: 10pt;">そのためには、多くの次元を持つ特徴マップのうち、出力結果に最も関連する次元を求める必要があります。</span></div><div><span style="font-size: 10pt;">そのために、特徴量マップの各次元の出力結果に対する影響度を算出し、特徴量マップの各次元を重みづけ平均して可視化することにより、最終出力に影響度の高い領域を可視化します。</span></div><div><a href="https://qiita.com/bele_m/items/a7bb15313e2a52d68865" style="font-size: 10pt;">https://qiita.com/bele_m/items/a7bb15313e2a52d68865</a></div><div><span style="font-size: 10pt;">&lt;Grad-CAMの手順&gt;</span></div><div><span style="font-size: 10pt;">1. 最後の出力において、対象としているクラスcのみを1にして他を0にした状態で誤差逆伝播する。要はone-hotのラベルで誤差逆伝播（y = [0,0,1,0,0,…]）</span></div><div><span style="font-size: 10pt;">2. CNNの最終層まで指定レイヤーのk番目の特徴マップ（フィルタをかけた出力画像）の (i,j) ピクセルの強度（画素）Aij_kを微分して各画素に対する勾配 ∂yc/∂Aij_k を計算</span></div><div><span style="font-size: 10pt;">　※kerasでは勾配を計算するGrad-CAMの損失関数はK.sum(y_pred * y_true)らしい</span></div><div><span style="font-size: 10pt;">　→予測と正解ラベルのOne_hotベクトル同士の内積</span></div><div><span style="font-size: 10pt;">　</span><a href="https://qiita.com/nishiha/items/8509dc4146442ad5315c" rev="en_rl_none" style="font-size: 10pt;">https://qiita.com/nishiha/items/8509dc4146442ad5315c</a></div><div><span style="font-size: 10pt;">3. 2を繰り返してその特徴マップの全ピクセルで勾配を計算</span></div><div><span style="font-size: 10pt;">4. 3の全ピクセルの勾配を平均 (global average pooling)して、対象クラスcのk番目フィルタに関する重み係数α^c_kを算出</span></div><div><span style="font-size: 10pt;">　（その1フィルタと対象クラスcとの影響度を出したということ。</span></div><div><span style="font-size: 10pt;">　　この重み係数が大きいほど、その特徴マップA_kがそのクラスcにとって重要と言える。</span></div><div><span style="font-size: 10pt;">　　→重み係数は勾配から出しているから、クラスcの確率スコアの値を変化させたら重み係数が大きく変化するということなので）</span></div><div><span style="font-size: 10pt;">5. 指定レイヤーのk個のフィルタそれぞれで重み係数α^c_kを計算</span></div><div><span style="font-size: 10pt;">　</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [94].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">6. 計算した各フィルタの重み係数α^c_kと対応する特徴マップの画素A_kで加重平均を計算する</span></div><div><span style="font-size: 10pt;">　 α^c_1*A_1 + α^c_2*A_2 + …</span></div><div><span style="font-size: 10pt;">7. 計算した加重平均を活性化関数 ReLU(x)≡max{x,0} に掛けて出力をヒートマップとして定義し、ヒートマップを元画像に振りかける</span></div><div><span style="font-size: 10pt;">　　</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [95].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">Grad-CAMにおいて重要なのは、各フィルターで勾配の平均を取ることでクラスの違いを明瞭にする点。</span></div><div><span style="font-size: 10pt;">実際、各フィルターで勾配の平均を取らず、勾配の正の値（ReLU(x)≡max{x,0}でマイナスは0にする）に限ってCNNを逆伝搬させて可視化することもできます。</span></div><div><span style="font-size: 10pt;">この手法は、Guided Backpropagation と呼ばれる手法で、概念図でいうと出力部分から入力画像に向かう上側の矢印で表されるルートです。</span></div><div><a href="http://blog.brainpad.co.jp/entry/2017/07/10/163000" style="font-size: 10pt;">http://blog.brainpad.co.jp/entry/2017/07/10/163000</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Grad-CAM ++ （Gradient-weighted Class Activation Mapping++）: Grad-CAMの一般版で、Grad-CAMよりも人間の感覚に近い領域にCNNモデルの予測に重要な領域をヒートマップで可視化する技術</span></div><div><a href="https://www.arxiv-vanity.com/papers/1710.11063/" style="font-size: 10pt;">https://www.arxiv-vanity.com/papers/1710.11063/</a></div><div><span style="font-size: 10pt;">　Grad-CAMの欠点であった以下の2点をよくした手法</span></div><div><span style="font-size: 10pt;">　1.画像に複数のクラスが写っている場合、そのクラスのすべての位置にヒートマップを作成する。</span></div><div><span style="font-size: 10pt;">　（例. 犬が3匹写っている画像の場合、Grad-CAMでは1,2匹目はヒートマップ作成されるが、3匹目に対しては作成されない。Grad-CAM++なら3匹全体にヒートマップ作成。）</span></div><div><span style="font-size: 10pt;">　2.より正確に予測クラスの位置を特定する</span></div><div><span style="font-size: 10pt;">　（例. 鳥が1羽写っている画像の時、Grad-CAMでは体の部分しか注目しないが、Grad-CAM++は鳥の足まで注目。）</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■guided backpropagation: ネットワークをある層まで伝搬させ、伝搬/逆伝搬時に伝搬させている値がマイナスになるような箇所(画素)を0にして入力層まで伝搬し、残った箇所(画素)の値がモデルの注目箇所とする手法</span></div><div><span style="font-size: 10pt;">論文:Striving for Simplicity: The All Convolutional Net</span></div><div><a href="https://arxiv.org/pdf/1412.6806.pdf" style="font-size: 10pt;">https://arxiv.org/pdf/1412.6806.pdf</a></div><div><span style="font-size: 10pt;">→考え方は出力から入力までの経路を逆にたどって、入力画像の重要な箇所を可視化する</span></div><div><span style="font-size: 10pt;">→逆伝搬においてActivationを減衰させるようなマイナスの値を取り除くために、伝搬/逆伝搬時に値がマイナスになるような箇所を0にして伝搬しています(結果的に、ReLUと同等な非線形の処理を行っている)</span></div><div><span style="font-size: 10pt;">→似た手法であるGrad-CAMはクラスの分類に寄与したところだけ知りたいので望むラベルから勾配を逆にたどる。guided backpropagationはラベルの指定はできない。</span></div><div><span style="font-size: 10pt;">参考:ディープラーニングの判断根拠を理解する手法</span></div><div><a href="https://qiita.com/icoxfog417/items/8689f943fd1225e24358" style="font-size: 10pt;">https://qiita.com/icoxfog417/items/8689f943fd1225e24358</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■マイクロ平均(micro-average): Nセットのテストをする場合、テストを合計してから評価値を計算。</span></div><div><span style="font-size: 10pt;">■マクロ平均(macro-average): Nセットのテストをする場合、各セットを計算してからそれらを平均する計算。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■AUC(Area Under the Curve)：ROC曲線のグラフ（偽陽性率を横軸、真陽性率を縦軸）の曲線よりも下の部分の面積。分類の性能を表し、0から1までの値をとる。完全な分類が可能なときの面積は1。ランダムな分類の場合は0.5。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ROC曲線：偽陽性率を横軸、真陽性率を縦軸にとったグラフ。予測のランク付けがどれだけ優れているかを示す。正例負例等しく精度測る場合はroc_aucが適している。ただし、非常に不均衡なデータセットの場合は多数の真陰性のために偽陽性率が低下する。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■PR曲線(Precision-Recall Curve)：特定のクラスでデータ数が多い場合に有効な 2クラス分類モデルの評価指標</span></div><div><span style="font-size: 10pt;">　PR曲線は2クラス分類の評価指標を与える曲線で、精度(Precision)と再現率(Recall)を軸にプロットしたグラフ。</span></div><div><span style="font-size: 10pt;">　ROC曲線と同じように曲線の下面積(AUC)が1に近いほど高精度。</span></div><div><span style="font-size: 10pt;">　正例のラベルが非常に少ないデータ不均衡な時に正例の精度を測る場合はpr_aucが適している。pr_aucは負例を重視しない</span></div><div><span style="font-size: 10pt;">　</span><span style="font-size: 10pt; text-decoration: line-through;">ROC曲線はインバランスデータだとデータ数が多いクラスを正しく分類すればAUC増えるが、PR曲線はデータ数が少ないクラスを正しく分類しないとAUC増えない。</span></div><div><span style="font-size: 10pt;">　→言いたいこと（PR曲線は少ないクラスのデータ当てないと面積増えない）は合っているが、誤解を招く（PR曲線でも正しく分類できればAUC上がるので）ので書かないこと</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><img src="用語_files/Image [96].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div><a href="http://mikemoke.hatenablog.com/entry/2018/09/10/ROC%E5%88%86%E6%9E%90%E3%81%A8Precision-Recall%E5%88%86%E6%9E%90%E3%81%AE%E4%BD%BF%E3%81%84%E5%88%86%E3%81%91">http://mikemoke.hatenablog.com/entry/2018/09/10/ROC%E5%88%86%E6%9E%90%E3%81%A8Precision-Recall%E5%88%86%E6%9E%90%E3%81%AE%E4%BD%BF%E3%81%84%E5%88%86%E3%81%91</a></div><div><br/></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■hyperas:&lt;</span><a href="https://github.com/maxpumperla/hyperas" rev="en_rl_none" style="font-size: 10pt;">https://github.com/maxpumperla/hyperas</a><span style="font-size: 10pt;">&gt;</span></div><div><span style="font-size: 10pt;">■hyperopt:&lt;</span><a href="https://github.com/hyperopt/hyperopt" rev="en_rl_none" style="font-size: 10pt;">https://github.com/hyperopt/hyperopt</a><span style="font-size: 10pt;">&gt;</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Docker：OS上に、コンテナと呼ばれる、隔離されたアプリケーションの実行環境を作り、一台のホスト上であたかも複数のホストが動いているかのような環境を実現する技術</span></div><div><span style="font-size: 10pt;">（コンテナ型の仮想環境を作成、配布、実行するためのプラットフォーム</span></div><div><span style="font-size: 10pt;">　コンテナを作る設定ファイルの「Dockerfile」をビルド（Dockerイメージの作成）、デプロイ（コンテナの起動）して隔離された仮想環境を構築できる）</span></div><div><span style="font-size: 10pt;">　</span><a href="https://www.docker.com/" rev="en_rl_none" style="font-size: 10pt;">https://www.docker.com/</a></div><div><span style="font-size: 10pt;">　</span><a href="https://knowledge.sakura.ad.jp/13265/" rev="en_rl_none" style="font-size: 10pt;">https://knowledge.sakura.ad.jp/13265/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [97].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">Dockerでは、最終的なアプリケーションはホストOSのプロセスとして実行されているため、余計なオーバーヘッドがない。</span></div><div><span style="font-size: 10pt;">ただし通常のLinuxのプロセスと違って、Linuxカーネルのサポートする「コンテナ」技術などをベースにしている。</span></div><div><span style="font-size: 10pt;">コンテナは、他のプロセスからは隔離された、ある程度独立したプロセス実行環境である。</span></div><div><span style="font-size: 10pt;">コンテナごとにファイルシステムやネットワークインタフェース、名前空間などが独立している。</span></div><div><span style="font-size: 10pt;">図中の「Docker Engine」はDockerのコンテナの実行をサポートするためのモジュールであり、外部からはDocker APIを使って制御される。</span></div><div><span style="font-size: 10pt; font-weight: bold;">Docker for Windowsでは、このLinux OS部分はHyper-V上で動作している。</span></div><div><span style="font-size: 10pt; font-weight: bold;">Docker for Windowsのコンテナが実行される環境は、仮想マシンを動かしてLinuxの上で実行しているのは変わりません。</span></div><div><span style="font-size: 10pt; font-weight: bold;">Windows上からコマンドで、Linux上のコンテナを操作しているわけです。</span></div><div><a href="https://www.atmarkit.co.jp/ait/articles/1609/01/news053.html" style="font-size: 10pt;">https://www.atmarkit.co.jp/ait/articles/1609/01/news053.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Dockerfile:Dockerイメージの定義。どういうOSを使って、どういうライブラリをインストールするのかを書いたファイル</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">イメージはDockerfileをビルドして作ります。イメージはDockerfileという設計図を元に作られた「型」</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [98].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■docker-compose:Dockerfileのビルドと起動を管理するツール</span></div><div><span style="font-size: 10pt;">docker-composeを使えば、様々なオプションをdocker-compose.ymlというYAMLファイルで設定することができる</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [99].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">docker-composeの基本のフォルダ構成</span></div><div><span style="font-size: 10pt;">.</span></div><div><span style="font-size: 10pt;">├── .dockerignore # Dockerfileをビルドする際に無視するファイルを列挙するもの</span></div><div><span style="font-size: 10pt;">├── Dockerfile</span></div><div><span style="font-size: 10pt;">├── docker-compose.yml</span></div><div><span style="font-size: 10pt;">├── input/     # データ</span></div><div><span style="font-size: 10pt;">├── notebook/  # Jupyter Notebook</span></div><div><span style="font-size: 10pt;">└── script/    # スクリプト</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><a href="https://amalog.hateblo.jp/entry/data-analysis-docker" style="font-size: 10pt;">https://amalog.hateblo.jp/entry/data-analysis-docker</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">VM、Docker、pipの比較</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [100].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">Dockerの機械学習の環境としてのメリットは以下です。</span></div><ul><li><div><span style="font-size: 10pt;">ローカルマシンで構築した環境をサーバー上のマシンで一瞬で構築できる</span></div></li><li><div><span style="font-size: 10pt;">今ある環境を壊さずに使い捨てのテスト環境を一瞬で構築・破壊できる</span></div></li><li><div><span style="font-size: 10pt;">環境まるごとイメージ化できるので問題の出やすいバージョン依存関係エラーが起きない</span></div></li><li><div><span style="font-size: 10pt;">DockerHubという公開ファイルサーバーに無料で大きいイメージファイルをバックアップできる</span></div></li><li><div><span style="font-size: 10pt;">便利な環境が揃ったDockerイメージが豊富にある</span></div></li></ul><div><a href="https://karaage.hatenadiary.jp/entry/2019/05/17/073000" style="font-size: 10pt;">https://karaage.hatenadiary.jp/entry/2019/05/17/073000</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■コンテナ：OS上に隔離されたプロセス郡</span></div><div><span style="font-size: 10pt;">　ユーザからは独立した個別のサーバのように見える</span></div><div><span style="font-size: 10pt;">　VMとの違いとして、VMはOSも含めて仮想化するが、コンテナはOSまでは仮想化しない（DockerのOS上のプロセスを仮想化）</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Docker compose：</span></div><div><span style="font-size: 10pt;">　docker-compose.ymlからDockerを管理する機能（複数のコンテナから成るサービスを構築・実行する手順を自動的にし、管理を容易にする機能）</span></div><div><span style="font-size: 10pt;">　docker-compose.ymlから設定を読み込んですべてのコンテナサービスを起動することができる</span></div><div><span style="font-size: 10pt;">　</span><a href="https://qiita.com/TsutomuNakamura/items/7e90e5efb36601c5bc8a" rev="en_rl_none" style="font-size: 10pt;">https://qiita.com/TsutomuNakamura/items/7e90e5efb36601c5bc8a</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■VM(Virtual Machine: 仮想マシン)：1台のコンピュータで複数のコンピュータを動かす技術。仮想的に別のOS丸ごと導入できる</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [101].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://ferret-plus.com/8461" style="font-size: 10pt;">https://ferret-plus.com/8461</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Hyper-V：米マイクロソフト（Microsoft）社の仮想化ソフト（ハイパーバイザー）製品。</span></div><div><span style="font-size: 10pt;">一台の物理的なコンピュータ上で複数の仮想的なコンピュータ（仮想マシン）を稼働させ、それぞれ独立にオペレーティングシステム（OS）を起動することができる。</span></div><div><span style="font-size: 10pt;">Hyper-Vを動かすためのプラットフォームとなる物理マシンを「ホストマシン」と呼び、Hyper-Vによって作り出した仮想パソコンを「仮想マシン」と呼びます。</span></div><div><span style="font-size: 10pt;">ホスト型の仮想化はホストOSに仮想化ソフトウェアをインストールし、その上で仮想マシンを動かす方式</span></div><div><span style="font-size: 10pt;">→Oracle VirtualBoxや VMwareはホスト型</span></div><div><span style="font-size: 10pt;">ハイパーバイザー型の仮想化はホストOSを使わず直接サーバにインストールし仮想マシン環境を作り出す方式</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [102].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><a href="http://e-words.jp/w/Hyper-V.html" style="font-size: 10pt;">http://e-words.jp/w/Hyper-V.html</a></div><div><a href="https://persol-tech-s.co.jp/hatalabo/it_engineer/446.html" style="font-size: 10pt;">https://persol-tech-s.co.jp/hatalabo/it_engineer/446.html</a></div><div><a href="https://ferret-plus.com/8461" style="font-size: 10pt;">https://ferret-plus.com/8461</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Flask：PythonのWebアプリケーションフレームワーク</span></div><div><span style="font-size: 10pt;">Flaskの基本構成</span></div><div><span style="font-size: 10pt;">flask_base</span></div><div><span style="font-size: 10pt;">├── main.py # flaskのメインモジュール</span></div><div><span style="font-size: 10pt;">├── models # 「DBから値をとってくる」などの処理を記述したファイルをいれる場所。ディレクトリ名は別にmodelsじゃなくていい</span></div><div><span style="font-size: 10pt;">├── static # CSSやJSなどのファイルをいれて使用。他にはimagesやsqliteなどのファイルもいれたりして使用。</span></div><div><span style="font-size: 10pt;">│   ├── css</span></div><div><span style="font-size: 10pt;">│   │   └── hello.css</span></div><div><span style="font-size: 10pt;">│   └── js</span></div><div><span style="font-size: 10pt;">│       └── hello.js</span></div><div><span style="font-size: 10pt;">└── templates # main.pyで返すHTMlを入れるディレクトリ。htmlはtemplatesディレクトリに入れないと読み取ってくれない</span></div><div><span style="font-size: 10pt;">    ├── hello.html  </span></div><div><span style="font-size: 10pt;">    └── layout.html</span></div><div><a href="https://qiita.com/strawberryjam/items/710a21329db109f62084" style="font-size: 10pt;">https://qiita.com/strawberryjam/items/710a21329db109f62084</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Gunicorn：Python製の軽量の（WSGI）サーバー</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■WSGI（Web Server Gateway Interface）：</span></div><div><span style="font-size: 10pt;">　読み方は「ウィズギー」</span></div><div><span style="font-size: 10pt;">　WebサーバーとWebアプリケーションをつなぐ共通のインターフェースをPythonで定義したもの</span></div><div><span style="font-size: 10pt;">　WSGIはJavaにおけるJava Servelet API</span></div><div><span style="font-size: 10pt;">　FlaskはWSGIをサポートするWebアプリケーションフレームワークの1つ</span></div><div><span style="font-size: 10pt;">　GunicornはWSGIをサポートするWebサーバの1つ</span></div><div><span style="font-size: 10pt;">　</span><a href="https://dot-blog.jp/news/wsgi-all/" rev="en_rl_none" style="font-size: 10pt;">https://dot-blog.jp/news/wsgi-all/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Ajax: 「Asynchronous JavaScript + XML」の略。あるWebページを表示した状態のまま、別のページや再読込などを伴わずにWebサーバ側と通信（非同期通信）を行い、動的に表示内容を変更する手法＝JavaScriptでサーバー側と非同期通信を行うための技術。</span></div><div><span style="font-size: 10pt;">→JavaScriptとXMLを使って非同期にサーバとの間の通信を行うことが出来る</span></div><div><span style="font-size: 10pt;">→ページ遷移をさせず（非同期処理で）コンテンツを書き換えられる</span></div><div><span style="font-size: 10pt;">→他の処理と同時並行で、サーバとやりとりができる</span></div><div><a href="https://qiita.com/strawberryjam/items/710a21329db109f62084" style="font-size: 10pt;">https://qiita.com/strawberryjam/items/710a21329db109f62084</a></div><div><a href="https://qiita.com/zakiyamaaaaa/items/bdda422db2ccbaea60d9" style="font-size: 10pt;">https://qiita.com/zakiyamaaaaa/items/bdda422db2ccbaea60d9</a></div><div><a href="https://www.sejuku.net/blog/30245" style="font-size: 10pt;">https://www.sejuku.net/blog/30245</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">Ajaxというのは一つの機能でできているのではなく、複数の機能が組み合わさって実装しています。</span></div><div><span style="font-size: 10pt;">①XMLHttpRequest:ブラウザ上でサーバーとHTTP通信を行うためのAPI</span></div><div><span style="font-size: 10pt;">②JavaScript:XMLHttpRequestを使って実装をするのもの</span></div><div><span style="font-size: 10pt;">　XMLHttpRequestがjavascriptの組み込みオブジェクトだからです。</span></div><div><span style="font-size: 10pt;">　組み込みオブジェクトとは、あらかじめ定義されているオブジェクトのことですね。</span></div><div><span style="font-size: 10pt;">　Ajaxという名前にある通り、非同期通信はjavascriptを使わないと実装できない</span></div><div><span style="font-size: 10pt;">③XML:Extensible Markup Languageの略。</span></div><div><span style="font-size: 10pt;">　文書やデータの意味や構造を記述するためのマークアップ言語の一つ（HTMLと似たようなもの)</span></div><div><span style="font-size: 10pt;">　AjaxにはXMLの代わりにJSONという型がよく使われています</span></div><div><span style="font-size: 10pt;">④Json:JavaScript Object Notationの略。</span></div><div><span style="font-size: 10pt;">　軽量のデータ交換フォーマットで、人間にとって読み書きが容易で、マシンにとっても簡単にパースや生成を行なえる形式</span></div><div><span style="font-size: 10pt;">　現在ではJSONを使用して、非同期通信を行うのが主流</span></div><div><span style="font-size: 10pt;">⑤DOM:Document Object Model (DOM) は、HTML および XML ドキュメントのための API</span></div><div><span style="font-size: 10pt;">xmlやhtmlの各要素、たとえば&lt;p&gt;とか&lt;img&gt;とかそういった類の要素にアクセスする仕組みのこと</span></div><div><a href="http://piyo-js.com/05/dom.html" style="font-size: 10pt;">http://piyo-js.com/05/dom.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">Ajaxの処理フロー</span></div><div><span style="font-size: 10pt;">①ページ上で任意のイベントが発生（ボタンクリックなど）</span></div><div><span style="font-size: 10pt;">②JavaScript + XMLHttpRequestでサーバーに対してリクエストを送信（非同期通信）</span></div><div><span style="font-size: 10pt;">　ほしい情報、返ってくるレスポンスの情報を指定してリクエスト</span></div><div><span style="font-size: 10pt;">③サーバーで受け取った情報を処理</span></div><div><span style="font-size: 10pt;">　サーバーの処理中もクライアントは操作を継続できる</span></div><div><span style="font-size: 10pt;">④処理結果をJSONやXMLなどの形式で応答</span></div><div><span style="font-size: 10pt;">⑤受診したレスポンスを受けて、DOMでページを更新</span></div><div><span style="font-size: 10pt;">　更新のあった部分だけを書き換えるため、画像が一瞬白くなることはない。</span></div><div><a href="https://qiita.com/hisamura333/items/e3ea6ae549eb09b7efb9" style="font-size: 10pt;">https://qiita.com/hisamura333/items/e3ea6ae549eb09b7efb9</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■jQuery：JavaScriptのライブラリ。JavaScriptを少ない記述で書けるようにしており、短い記述で実装できるのがjQueryのメリット</span></div><div><span style="font-size: 10pt;">jQueryを使うには、head内で以下のような記述が必要</span></div><div><span style="font-size: 10pt;">&lt;head&gt;</span></div><div><span style="font-size: 10pt;">&lt;script src=&quot;</span><a href="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js" rev="en_rl_none" style="font-size: 10pt;">https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js</a><span style="font-size: 10pt;">&quot;&gt;&lt;/script&gt;</span></div><div><span style="font-size: 10pt;">&lt;script type=&quot;text/javascript&quot; src=&quot;js/sample.js&quot;&gt;&lt;/script&gt;</span></div><div><span style="font-size: 10pt;">&lt;/head&gt;</span></div><div><span style="font-size: 10pt;">jQueryのコードはHTML内に直接記述するか、jsファイルを作るかの２パターン</span></div><div><span style="font-size: 10pt;">ページ読み込み完了のタイミングで実行するために、$(function(){ });を使う</span></div><div><span style="font-size: 10pt;">基本的な構成は「セレクタ」と「メソッド」でなりたっている</span></div><div><span style="font-size: 10pt;">$()は、jQuery()とも書ける</span></div><div><span style="font-size: 10pt;">メソッドを覚えていくと、色々な処理を実行できる</span></div><div><a href="https://handywebdesign.net/2017/09/jquery-for-beginner/" style="font-size: 10pt;">https://handywebdesign.net/2017/09/jquery-for-beginner/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■SQLite3: データベースをファイルとして保存するもの</span></div><div><a href="https://qiita.com/strawberryjam/items/710a21329db109f62084" style="font-size: 10pt;">https://qiita.com/strawberryjam/items/710a21329db109f62084</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Kubernetes：Dockerコンテナをクラスタ化した際の管理ツール。複数のホストで動いてるDockerコンテナを管理できる。</span></div><div><span style="font-size: 10pt;">　</span><a href="https://kubernetes.io/" rev="en_rl_none" style="font-size: 10pt;">https://kubernetes.io/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Horovod：Deep Learning用フレームワークを複数GPUで分散処理させるアプリケーション。Tensorflow、keras、pytorchに対応している。</span></div><div><span style="font-size: 10pt;">　</span><a href="https://github.com/uber/horovod" rev="en_rl_none" style="font-size: 10pt;">https://github.com/uber/horovod</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Cloud Machine Learning Engine（Cloud ML Engine）：TensorFlow実行するためのGoogleのクラウドサービス。並列GPUの合計最大数は30に制限されているが、割り当ての増加リクエストで上限を引き上げることもできる。</span></div><div><span style="font-size: 10pt;">　</span><a href="https://cloud.google.com/ml-engine/" rev="en_rl_none" style="font-size: 10pt;">https://cloud.google.com/ml-engine/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Multitask Learning: 1つのニューラルネットワークを使用して、複数のタスクを同時に行う。</span></div><div><span style="font-size: 10pt;">　各タスクのデータを一度に学習させるため、タスク間の一般的な表現を学習することができる。</span></div><div><span style="font-size: 10pt;">　複数タスクを一度に学習せるため、損失関数は各タスクの総和をとる。</span></div><div><span style="font-size: 10pt;">　Multi-task Learningの手法は</span></div><div><span style="font-size: 10pt;">　すべてのタスク間の共有レイヤーを作り、各タスク固有の出力レイヤーを作成するHard parameter sharingや</span></div><div><span style="font-size: 10pt;">　各タスクで固有のモデルを持つが、各タスクのパラメータを類似させるためにモデルのパラメータ間の距離を正規化するSoft parameter sharingがある。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Test-Time Augmentation(TTA)：予測対象の画像をdata augmentationで増加させた後で各パターンについて予測を行い、予測結果を平均する手法。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■t-SNE (t-distributed Stochastic Neighbor Embedding)：高次元データの次元を圧縮するアルゴリズム。高次元データを可視化する際に有用。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■UMAP(Uniform Manifold Approximation and Projection)：次元圧縮&amp;可視化のための手法の一種。t-SNEよりも高速に次元圧縮できる。</span></div><div><span style="font-size: 10pt;">　主要パラメータ</span></div><div><span style="font-size: 10pt;">    n_neighbors :局所近似で使われる隣接点の数。デフォルトは15。5から50の範囲であることが多く、適切なデフォルトは10から15の範囲。</span></div><div><span style="font-size: 10pt;">    min_dist :同一クラスタとして判定する距離。デフォルトは0.1。適切な値は0.001から0.5の範囲。</span></div><div><span style="font-size: 10pt;">    metric :使用する距離指標。デフォルトはユークリッド距離(</span> <span style="font-size: 10pt; color: rgb(3, 47, 98); font-family: SFMono-Regular, Consolas, &quot;Liberation Mono&quot;, Menlo, Courier, monospace;">euclidean</span> <span style="font-size: 10pt;">)。</span></div><div><span style="font-size: 10pt;">　</span><a href="https://arxiv.org/abs/1802.03426" rev="en_rl_none" style="font-size: 10pt;">https://arxiv.org/abs/1802.03426</a></div><div><span style="font-size: 10pt;">　</span><a href="https://github.com/lmcinnes/umap" rev="en_rl_none" style="font-size: 10pt;">https://github.com/lmcinnes/umap</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Papermill（</span><a href="https://github.com/nteract/papermill" rev="en_rl_none" style="font-size: 10pt;">https://github.com/nteract/papermill</a><span style="font-size: 10pt;">）： jupyter notebookをパラメータ化、実行、分析できる pythonライブラリ。</span></div><div><span style="font-size: 10pt;">　引数を渡してコマンドラインからnotebookを実行できる。</span></div><div><span style="font-size: 10pt;">　notebookのセルで出力した画像を別のnotebookにコピーすることもできる。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Cyclical Learning Rate (CLR)：&lt;</span><a href="https://github.com/bckenstler/CLR" rev="en_rl_none" style="font-size: 10pt;">https://github.com/bckenstler/CLR</a><span style="font-size: 10pt;">&gt;モデルの学習率を周期的に変更するpythonライブラリ</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■BERT（Pre-training of Deep Bidirectional Transformers for Language Understanding）</span></div><div><span style="font-size: 10pt;">　</span><a href="https://arxiv.org/pdf/1810.04805.pdf" rev="en_rl_none" style="font-size: 10pt;">https://arxiv.org/pdf/1810.04805.pdf</a></div><div><span style="font-size: 10pt;">　対話システムや自動翻訳などに活用される自然言語処理の汎用的な事前学習モデル。</span></div><div><span style="font-size: 10pt;">　BERTをFine-tuningすることで短い学習時間で高精度の自然言語処理モデルを作成できる。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■</span><span style="font-size: 13px; -webkit-text-size-adjust: 100%;">Transformer: Self-Attentionという機構でデータ内の時系列的特徴を抽出でき、従来のRNNを始めとするNNに対して 100倍以上計算効率が優れる。</span></div><div><span style="font-size: 13px;">この優れた計算効率を活用し、近年は 莫大なデータ量と計算量で巨大なモデルを学習し、NLPモデルの精度を凄まじい勢いで改善している。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■NASNetLarge：2017年のImageNetコンペ（ILSVRC 2017）優勝モデルとほぼ同精度のモデル。</span></div><div><span style="font-size: 10pt;">　ニューラルネットワークの構造を強化学習で探索し、最適化するNAS（Neural Architecture Search）を利用して、CNN（畳み込み層）の構造を最適化したモデル。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■SENets：2017年のImageNetコンペ優勝モデル。</span></div><div><span style="font-size: 10pt;">　CNNのチャネル間の相関を強調する2層のニューラルネットワークであるSE(Squeeze and Excitation)ブロックをResNetやInceptionなどの既存モデルと組み合わせたモデルの総称</span></div><div><span style="font-size: 10pt; font-weight: bold;">　SEブロックはAttentionの一種</span></div><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">　特徴量mapの各チャネルの代表値をAttentionレイヤーの入力として、チャネルの重みベクトルを出力し、</span><span style="font-size: 10pt; font-weight: bold;">特徴量mapの</span><span style="font-size: 10pt; color: rgb(255, 0, 0); font-weight: bold;">特定のチャネルを注目させる</span></font></div><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">SEブロック</span><span style="font-size: 10pt; font-weight: bold;">は </span></font></div><div><span style="font-size: 10pt; color: rgb(255, 0, 0); font-weight: bold;">Global Average Pooling→Conv2D→ReLU→Conv2D→sigmoid</span></div><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">を分岐させて、<span style="font-size: 10pt; font-weight: bold; color: rgb(255, 0, 0);">サイズが</span></span><span style="font-size: 10pt; color: rgb(255, 0, 0); font-weight: bold;">チャネル数のベクトルを出力し、分岐元の特徴量mapにチャネルごとに掛け算</span><span style="font-size: 10pt; font-weight: bold;">する（sigmoid使うことで二値化して黒塗り実現している）</span></font></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [103].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><a href="https://www.youtube.com/watch?v=g5DSLeJozdw" style="font-size: 10pt;">https://www.youtube.com/watch?v=g5DSLeJozdw</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Optuna：ハイパーパラメータの最適化を自動化するpythonライブラリ。</span></div><div><span style="font-size: 10pt;">　　ハイパーパラメータの値に関する試行錯誤を自動的に行いながら、優れた性能を発揮するハイパーパラメータの値を自動的に発見する。</span></div><div><span style="font-size: 10pt;">　　Preferred Networks製だがChainer以外の機械学習ソフトウェア(Kerasなど)と一緒に使うことができる。</span></div><div><span style="font-size: 10pt;">　　他のハイパーパラメータの自動最適化フレームワーク（Hyperopt, Spearmint, SMAC）には無い、学習曲線を用いた試行の枝刈りができる。</span></div><div><span style="font-size: 10pt;">　　※試行の枝刈り：最終的な結果がどのぐらいうまくいきそうかを大まかに予測し、良い結果を残すことが見込まれない試行は、最後まで行うことなく早期終了すること。</span></div><div><span style="font-size: 10pt;">　　</span><a href="https://github.com/pfnet/optuna" rev="en_rl_none" style="font-size: 10pt;">https://github.com/pfnet/optuna</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■scikit-learn：Pythonの代表的な機械学習ライブラリ</span></div><div><span style="font-size: 10pt;">　</span><a href="https://scikit-learn.org/stable/" rev="en_rl_none" style="font-size: 10pt;">https://scikit-learn.org/stable/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■グリッドサーチ(Grid Search)：モデルのパラメータ探索手法。指定したパラメータの組み合わせを全て試し、最も評価精度の良いパラメータを見つける。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Chainer：国産のDeep learningフレームワーク。特徴の1つとしてDefine-by-Runでニューラルネットを学習させる。</span></div><div><span style="font-size: 10pt;">■Define-by-Run：ニューラルネットの構造の構築をデータを流しながら行う手法。</span></div><div><span style="font-size: 10pt;">　　ミニバッチ毎に（あるいは1データ毎に）違うニューラルネットの構造を準備することもできる。</span></div><div><span style="font-size: 10pt;">　　TensorflowやCaffeなどのDeep learningフレームワークはニューラルネットの構造の構築し、構築した構造にデータを流すDefine-and-Run。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Chainer Chemistry： Chainerをベースとした化学、生物学分野のための Deep learningフレームワーク。</span></div><div><span style="font-size: 10pt;">　　化学的性質予測のための様々な最先端モデル（特にGCNN - Graph Convolutional Neural Network）をサポート。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■MLflow：機械学習ライフサイクル(実験・再現・デプロイ)を支援するためのオープンソースプラットフォーム。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■AutoEncoder（AE）：NNを用いて次元削減する手法（教師なし学習）。入力画像を次元圧縮してから復元させることによって特徴量を学習し、学習後は入力画像とそっくりな画像を出力させるネットワーク。圧縮していく過程をエンコーダと呼び、復元する過程をデコーダと呼ぶ。</span></div><div><span style="font-size: 10pt;">参考：</span><a href="http://cedro3.com/ai/keras-autoencoder-anomaly/" rev="en_rl_none" style="font-size: 10pt;">http://cedro3.com/ai/keras-autoencoder-anomaly/</a></div><div><span style="font-size: 10pt;">　　　</span><a href="https://qiita.com/kenchin110100/items/7ceb5b8e8b21c551d69a" rev="en_rl_none" style="font-size: 10pt;">https://qiita.com/kenchin110100/items/7ceb5b8e8b21c551d69a</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Variational AutoEncoder（VAE）:AutoEncoderの潜在変数部分に確率分布を導入したもの。DNNの生成モデル。連続的に変化する画像を生成しやすい。エンコーダーは入力(X)の特徴を圧縮してN次元のガウス分布の平均μと分散σを出力し、その2つをもとにして潜在変数Zをサンプリングで求めます。</span></div><div><span style="font-size: 10pt;">参考：</span><a href="https://products.sint.co.jp/aisia/blog/vol1-21" rev="en_rl_none" style="font-size: 10pt;">https://products.sint.co.jp/aisia/blog/vol1-21</a></div><div><span style="font-size: 10pt;">　　　</span><a href="https://qiita.com/kenchin110100/items/7ceb5b8e8b21c551d69a" rev="en_rl_none" style="font-size: 10pt;">https://qiita.com/kenchin110100/items/7ceb5b8e8b21c551d69a</a></div><div><span style="font-size: 10pt;">　　　</span><a href="https://qiita.com/kenmatsu4/items/b029d697e9995d93aa24" rev="en_rl_none" style="font-size: 10pt;">https://qiita.com/kenmatsu4/items/b029d697e9995d93aa24</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■SSD(Single Shot MultiBox Detector)：物体検出とクラス分類を同時に行う物体検出の手法</span></div><div><span style="font-size: 10pt;">・SSDでは枠の大きさが違うアンカーボックスの数を変えて、サイズが小さい物体を検出する</span></div><div><span style="font-size: 10pt;">　なので、アンカーボックスの数の種類だけ出力層がある（SSDはマルチ出力のモデル）</span></div><div><span style="font-size: 10pt;">　この出力層は[256,7,7]、[256,2,2]のように[特徴量ベクトル, 縦横のアンカーの数]として出力される</span></div><div><span style="font-size: 10pt;">　（[256,7,7]は画像にアンカーボックスを7*7=49個隙間なく埋め込んだってこと</span></div><div><span style="font-size: 10pt;">　　＝各アンカーボックスの縦横のサイズは同じものが画像に隙間なく49個並べ、その49領域の256次元の特徴量ベクトルってこと）</span></div><div><span style="font-size: 10pt;">　※アンカーボックス：中心にアンカー点がありそれを囲む枠のこと。</span></div><div><span style="font-size: 10pt;">　　Faster-RCNNの2段階検出の1段階目のスライドウインドウに相当するもの</span></div><div><span style="font-size: 10pt;">　※スライドウインドウ：検出モデルが画像をスライドして、オブジェクト検出したら境界ボックスつける枠のこと。</span></div><div><span style="font-size: 10pt;">　　Faster-RCNNはスライドウインドウの大きさが指定サイズの割合で様々な大きさでとってた気がする</span></div><div><span style="font-size: 10pt;">　　（そのせいで遅い）</span></div><div><span style="font-size: 10pt;">・出力した[256,7,7]、[256,2,2]のような[特徴量ベクトル, 縦横のアンカーの数]を</span></div><div><span style="font-size: 10pt;">　4隅回帰：[4,7,7]、[4,2,2]</span></div><div><span style="font-size: 10pt;">　クラス分類：[N+1,7,7]、[N+1,2,2]（SSDでは背景クラスが必要なので分類クラスはN+1）</span></div><div><span style="font-size: 10pt;">　の出力層に渡してloss計算する</span></div><div><span style="font-size: 10pt;">・ちなみに、YOLOはアンカーボックスのサイズ1種類だけだから（画像を細かく格子状に切った枠をアンカーボックスとするため）シングル出力のモデル</span></div><div><span style="font-size: 10pt;">参考： </span><a href="https://towardsdatascience.com/retinanet-how-focal-loss-fixes-single-shot-detection-cb320e3bb0de" rev="en_rl_none" style="font-size: 10pt;">https://towardsdatascience.com/retinanet-how-focal-loss-fixes-single-shot-detection-cb320e3bb0de</a></div><div><span style="font-size: 10pt;">  　　   </span><a href="https://www.slideshare.net/cvpaperchallenge/meta-study-group" rev="en_rl_none" style="font-size: 10pt;">https://www.slideshare.net/cvpaperchallenge/meta-study-group</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■SSDやYOLOのシングルショットの物体検出モデルについて</span></div><div><span style="font-size: 10pt;">・出力層は分類クラス（N）+グラウンドトゥルースバウンディングボックスの4隅の座標位置（4）クラス</span></div><div><span style="font-size: 10pt;">・ネットワークは</span></div><div><span style="font-size: 10pt;">①Resnetとかの分類モデルの出力層削除</span></div><div><span style="font-size: 10pt;">②全結合層とノード数がN+4の出力層くっつける</span></div><div><span style="font-size: 10pt;">　4隅の座標位置を回帰するために全結合層の前のpooling層も削除するらしい（空間情報破壊するためらしい）</span></div><div><span style="font-size: 10pt;">・モデル全体のloss = 分類用のloss+α*4隅の回帰用loss</span></div><div><span style="font-size: 10pt;">　→αは4隅の回帰用lossを小さくor大きくするための係数</span></div><div><span style="font-size: 10pt;">　　分類と回帰で損失関数異なるので、どちらも同じオーダにするために回帰の係数かけてる</span></div><div><span style="font-size: 10pt;">・分類用のlossはクロスエントロピー損失</span></div><div><span style="font-size: 10pt;">・4隅の回帰用lossは</span></div><div><span style="font-size: 10pt;">　(x_pred_1-x_true_1)+(x_pred_2-x_true_2)+(y_pred_1-y_true_1)+(y_pred_2-y_true_2)</span></div><div><span style="font-size: 10pt;">　という単純なL1損失</span></div><div><span style="font-size: 10pt;">　（2乗するL2損失でも良いが外れ値に敏感になるから基本L1らしい）</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Focal loss：確信度がでかいときはloss=0 （予測が完ぺきに予測してる）とする損失関数。</span></div><div><span style="font-size: 10pt;">こうすることでラベル数が少ないクラスのlossの値だけが計上され、不均衡データでも上手く学習できるみたい。</span></div><div><span style="font-size: 10pt;">bboxのほとんどが背景となってしまう物体検出モデルで提案されたloss</span></div><div><span style="font-size: 10pt;">参考：</span><a href="https://medium.com/beyondminds/a-simple-guide-to-semantic-segmentation-effcf83e" rev="en_rl_none" style="font-size: 10pt;">https://medium.com/beyondminds/a-simple-guide-to-semantic-segmentation-effcf83e</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■SMOTE(Synthetic Minority Over-sampling Technique)：不均衡データに対して、少ない方のデータを人工的に生成して、均衡データに近づけるという手法</span></div><div><span style="font-size: 10pt;">アルゴリズムは以下</span></div><div><span style="font-size: 10pt;">1.少数ラベルのデータからランダムに1件選択</span></div><div><span style="font-size: 10pt;">2.少数ラベルのデータからランダムにもう1件選択</span></div><div><span style="font-size: 10pt;">3.1と2で選択したデータから線形補完で新しい少数ラベルのデータ作成</span></div><div><span style="font-size: 10pt;">　→[1のデータ]+([2のデータ]-[1のデータ]）*乱数</span></div><div><span style="font-size: 10pt;">4.指定したデータ数になるまで1-3を繰り返す</span></div><div><span style="font-size: 10pt;">Pythonであればimbalanced-learnという名前通りのライブラリを使って以下のようにすればデータの不均衡を正すことができます。</span></div><div><span style="font-size: 10pt;">SMOTEでは主にマイナーデータを増やします（Oversampling）。メジャーデータを多少Undersamplingしますが、重要なのは主にOversamplingのほうです。</span></div><div><span style="font-size: 10pt;">参考:</span></div><div><a href="https://rpubs.com/hoxo_m/54954" style="font-size: 10pt;">https://rpubs.com/hoxo_m/54954</a></div><div><a href="https://qiita.com/cvusk/items/aa628e84e72cdf0a6e77" style="font-size: 10pt;">https://qiita.com/cvusk/items/aa628e84e72cdf0a6e77</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■GANs (Generative Adversarial Networks)</span></div><div><span style="font-size: 10pt;">偽のデータを生成するGenerator (生成器)と、データが本物か偽物かを判別するDiscriminator (識別器)を敵対させ、競わせることで互いの性能が向上するように学習させる機械学習のモデル。</span></div><div><span style="font-size: 10pt;">→ディープラーニングのネットワーク内で、「ニセモノを生成」→「ニセモノを見抜く」→「さらに精巧なニセモノを生成」といった「いたちごっこ」を繰り返すことで、リアルな画像を生成するもの</span></div><div><span style="font-size: 10pt;">2014年に提案されて派生研究が爆発的に増えた。</span></div><div><span style="font-size: 10pt;">GANはゲーム理論から着想を得ており、プレーヤー同士が互いに自身の利得を最大化する戦略取ることで均衡する「ナッシュ均衡」という状態を目指して学習する。</span></div><div><span style="font-size: 10pt;">まあ要するに、プレーヤー同士が互角のバランスで強くなっていくライバル関係を実現するような学習方法。</span></div><div><span style="font-size: 10pt;">GANの基本的な考え方に則れば、GeneratorとDiscriminatorはニューラルネットワークに限らずどんな最適化モデルでも良いそうです。</span></div><div><span style="font-size: 10pt;">(と言いつつ、高い性能を示した例は全部ディープラーニングなので実質ニューラルネットワークがデファクトスタンダード)</span></div><div><a href="https://blog.negativemind.com/2019/06/22/generative-adversarial-networks/" style="font-size: 10pt;">https://blog.negativemind.com/2019/06/22/generative-adversarial-networks/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■DCGAN(Deep Convolutional GAN):</span></div><div><span style="font-size: 10pt;">CNN使った一番シンプルなGAN。教師なしでノイズから画像生成する。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■CycleGAN:</span></div><div><span style="font-size: 10pt;">画像変換するGAN（DCGAN,AC-GAN,PGGAN,BigGAN,StyleGANなどの生成系GANとは違う）</span></div><div><span style="font-size: 10pt;">ウマの画像とシマウマの画像の相互変換とかが有名。</span></div><div><a href="https://ainow.ai/2019/07/17/173382/#BigGAN" style="font-size: 10pt;">https://ainow.ai/2019/07/17/173382/#BigGAN</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■SNGAN:</span></div><div><span style="font-size: 10pt;">Spectral Normalizationを使ったGAN。</span></div><div><span style="font-size: 10pt;">従来のDiscriminator（D）のBatch NormalizationをSpectral Normalizationに置き換えることで、WGANやWGAN-GPで前提としているようなリプシッツ制約を満たし、GANの安定性が向上する。</span></div><div><span style="font-size: 10pt;">要はDのBatch Normの置き換えをするだけで、WGAN相応のものができますよということ。しかも論文によると、WGAN-GPより性能が良い（Inception Scoreが高い、様々なハイパーパラメーターに対するロバスト性が高い）とのこと。</span></div><div><a href="https://blog.shikoan.com/sngan/" style="font-size: 10pt;">https://blog.shikoan.com/sngan/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■AC-GAN:</span></div><div><span style="font-size: 10pt;">GANのGeneratorの入力に画像のクラス情報を入れる（Coditional GAN：CGAN）と同時に、Discriminatorの出力に通常の画像分類のような「多クラス分類」を入れます。</span></div><div><span style="font-size: 10pt;">通常のGANの損失関数にある「本物か偽物か」に加えて、「多クラス分類の損失項」を加えることで、よりバリエーションの多い画像出力を可能とする手法。</span></div><div><span style="font-size: 10pt;">2016年に出たGAN。</span></div><div><span style="font-size: 10pt;">ネットワークを同一にして1000クラスを一気に生成しようとすると出力画像の質が落ちるという現象が確認されています。</span></div><div><span style="font-size: 10pt;">そのため、論文では1000クラスを10クラス×100ケースに分割し、100ケースをそれぞれ個別に訓練しています。クラス数が多いケースではここが注意が必要。</span></div><div><a href="https://qiita.com/koshian2/items/1a1c8d59b81fb121e012" style="font-size: 10pt;">https://qiita.com/koshian2/items/1a1c8d59b81fb121e012</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ProgressiveGAN(PGGAN):</span></div><div><span style="font-size: 10pt;">低解像画像の生成からスタートし、徐々にレイヤを追加して解像度を上げていくというプログレッシブな学習方法を提案したGAN。</span></div><div><span style="font-size: 10pt;">段階的にネットワークを成長させることでメガピクセル画像の生成を実現。</span></div><div><a href="https://qiita.com/kzykmyzw/items/5c4a6c2ee19ddd59e810" style="font-size: 10pt;">https://qiita.com/kzykmyzw/items/5c4a6c2ee19ddd59e810</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■BigGAN:</span></div><div><span style="font-size: 10pt;">ネットワークの大規模化によりImageNet級の超多クラス画像生成を実現したGAN。</span></div><div><span style="font-size: 10pt;">ImageNetのような極めてバリエーションが多く複雑なデータセットを使いclass-conditionalな画像生成を過去最大規模で学習させることに成功している。</span></div><div><span style="font-size: 10pt;">最大512×512ピクセルの高解像度画像を条件付きで生成するモデルで、1000カテゴリーの画像を生成できる。</span></div><div><span style="font-size: 10pt;">ベースはSAGAN(Self-AttentionGAN)</span></div><div><span style="font-size: 10pt;">高精度な画像生成できるStyleGANとの大きな違いとしてBigGANはclass-conditionalな画像生成ができる。</span></div><div><span style="font-size: 10pt;">2018年9月に出たGAN。</span></div><div><a href="https://qiita.com/kzykmyzw/items/5c4a6c2ee19ddd59e810" style="font-size: 10pt;">https://qiita.com/kzykmyzw/items/5c4a6c2ee19ddd59e810</a></div><div><a href="https://ai-scholar.tech/treatise/vq-vae-2-ai-168/" style="font-size: 10pt;">https://ai-scholar.tech/treatise/vq-vae-2-ai-168/</a></div><div><a href="https://www.slideshare.net/DeepLearningJP2016/dlhacksstyleganbigganstyle-mixing-morphing-163980989" style="font-size: 10pt;">https://www.slideshare.net/DeepLearningJP2016/dlhacksstyleganbigganstyle-mixing-morphing-163980989</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■StyleGAN:</span></div><div><span style="font-size: 10pt;">ProgressiveGANと同じNVIDIAのT.Kerrasらにより2018年12月にarXiv投稿された最新手法。</span></div><div><span style="font-size: 10pt;">ノイズベクトルを入力層から与えるという従来のgenerator構造を大きく変更し、まずノイズベクトルを別の空間にマッピングし、</span></div><div><span style="font-size: 10pt;">そこから得られた情報をgeneratorの各層に入力していくことで生成画像の大局的な構造から詳細構造までを柔軟に制御することを実現している。</span></div><div><span style="font-size: 10pt;">各層に入力信号を与えることでアトリビュート単位での生成画像の制御を実現。</span></div><div><span style="font-size: 10pt;">ベースラインとなっているのは上述したProgressiveGAN。</span></div><div><span style="font-size: 10pt;">本論文では、各層に入力されるstyleの影響をそれぞれの層に局在化させることを目的としてmixing regularizationという正則化手法を提案している。</span></div><div><span style="font-size: 10pt;">こうした正則化を入れることにより、synthesis networkは隣接した層間でstyleに相関があるという前提を置くことができなくなるため、styleの影響を各層に局在化させることが可能となる。</span></div><div><a href="https://qiita.com/kzykmyzw/items/5c4a6c2ee19ddd59e810" style="font-size: 10pt;">https://qiita.com/kzykmyzw/items/5c4a6c2ee19ddd59e810</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■inception score:</span></div><div><span style="font-size: 10pt;">GAN (Generative Adversarial Network)が生成した画像の評価値。</span></div><div><span style="font-size: 10pt;">Inceptionモデルで識別しやすい画像であるほど、かつ、識別されるラベルのバリエーションが豊富であるほどスコアが高くなるように設計されたスコア。</span></div><div><span style="font-size: 10pt;">inception scoreが大きいほど下記2点の性能が高い。</span></div><div><span style="font-size: 10pt;">1.画像識別器が識別しやすい</span></div><div><span style="font-size: 10pt;">2.物体クラスのバリエーションが豊富</span></div><div><span style="font-size: 10pt;">imagenetの1,000クラスのInceptionV3で生成画像をうまく分類したかに基づいて、生成画像の識別性能とクラスのバリエーションを推定する。</span></div><div><a href="http://bluewidz.blogspot.com/2017/12/inception-score.html" style="font-size: 10pt;">http://bluewidz.blogspot.com/2017/12/inception-score.html</a></div><div><a href="https://qiita.com/kzykmyzw/items/5c4a6c2ee19ddd59e810" style="font-size: 10pt;">https://qiita.com/kzykmyzw/items/5c4a6c2ee19ddd59e810</a></div><div><a href="https://machinelearningmastery.com/how-to-implement-the-frechet-inception-distance-fid-from-scratch/" style="font-size: 10pt;">https://machinelearningmastery.com/how-to-implement-the-frechet-inception-distance-fid-from-scratch/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Fréchet Inception Distance (FID):</span></div><div><span style="font-size: 10pt;">Generative Adversarial Network (GAN)が生成する画像の品質を評価する指標。</span></div><div><span style="font-size: 10pt;">画像の集合間の距離を表す。</span></div><div><span style="font-size: 10pt;">GANで再現したい真の分布から生成された画像の集合と、GANで再現した分布から生成した画像の集合との距離を計算することになります。</span></div><div><span style="font-size: 10pt;">距離が近ければ近いほど良い画像であると判断。</span></div><div><span style="font-size: 10pt;">実画像の分布が考慮されていないというinception scoreの欠点を改善するため、実画像と生成画像の分布間の距離を測っている。</span></div><div><span style="font-size: 10pt;">FIDが低いほど、画像の品質が高く、スコアが高いほど画像の品質が低くなる。</span></div><div><span style="font-size: 10pt;">FIDが高いと画像がぐちゃぐちゃに崩れてくる。</span></div><div><a href="http://bluewidz.blogspot.com/2017/12/inception-score.html" style="font-size: 10pt;">http://bluewidz.blogspot.com/2017/12/inception-score.html</a></div><div><a href="https://qiita.com/kzykmyzw/items/5c4a6c2ee19ddd59e810" style="font-size: 10pt;">https://qiita.com/kzykmyzw/items/5c4a6c2ee19ddd59e810</a></div><div><a href="https://machinelearningmastery.com/how-to-implement-the-frechet-inception-distance-fid-from-scratch/" style="font-size: 10pt;">https://machinelearningmastery.com/how-to-implement-the-frechet-inception-distance-fid-from-scratch/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■画像様式の（モード）崩壊（mode collapse）:</span></div><div><span style="font-size: 10pt;">GANが克服すべき問題</span></div><div><span style="font-size: 10pt;">本来は多様な見た目の画像を生成するはずのGANがほぼ同一の画像しか生成しなくなる現象。</span></div><div><a href="https://ainow.ai/2019/07/17/173382/" style="font-size: 10pt;">https://ainow.ai/2019/07/17/173382/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■内的な共変量シフト（internal covariate shift）:</span></div><div><span style="font-size: 10pt;">（GANの）モデルが学習する過程で入力データの分布が変化してしまうことを指す。入力データの分布が変わってしまうと、特徴量の抽出に不備が生じる。</span></div><div><a href="https://ainow.ai/2019/07/17/173382/" style="font-size: 10pt;">https://ainow.ai/2019/07/17/173382/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■交差検証(cross-validation): 機械学習モデルの汎化性能を検証するための手法。</span></div><div><span style="font-size: 10pt;">一般的にはK分割交差検証(K-fold cross validation)を指す。</span></div><div><span style="font-size: 10pt;">訓練データセットをK分割し、一つのグループ以外の(K-1)個のかたまりでモデルを学習し、残りの一つのグループで性能を評価する。</span></div><div><span style="font-size: 10pt;">これをK回、すなわち性能評価用のデータを毎回変えながら実験し、最後にK回の性能評価を平均して最終的な出力とする。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■一個抜き交差検証(leave-one-out cross-validation): K-foldのKをデータ数と同じ個数にしたcross-validation。</span></div><div><span style="font-size: 10pt;">訓練データ一つを選び、それ以外のデータで学習して最後に一つの評価データで評価する。これを全データで繰り返し行い最後にK-foldの結果を平均する。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■敵対的サンプル(Adversarial Examples:AEs): モデルを誤認識させる小さなノイズを人工的に作り出し、それを載せたデータのこと。（攻撃者によって意図的にモデルが間違えるように設計された入力）</span></div><div><span style="font-size: 10pt;">最初に提案したのはGANの作者のGoodfellow氏。</span></div><div><span style="font-size: 10pt;">AEsの手法としては、lossが大きくなる摂動を入力データに加えたり、幾何学変換やドメインシフト（見るべきものの位置をずらす）、モザイクのような加工入れたりする</span></div><div><span style="font-size: 10pt;">AEsに強いモデルにするためにtrainデータにAEs含めたりする(=Adversarial Training:AT)が、AEsを増やすと以下の3つの問題が出てくる(2020年時点では)</span></div><div><span style="font-size: 10pt;">・データ増えるので計算コストが高い</span></div><div><span style="font-size: 10pt;">・trainに入れたAEsの種類にした頑健になっていないかも（例えば回転加えたデータは大丈夫だが縦横シフトするとだめとか）</span></div><div><span style="font-size: 10pt;">・AEsに強くなるが普通のデータの予測精度が下がる</span></div><div><span style="font-size: 10pt;">→これはモデルが「分類に有効かつ脆弱」な特徴量を学習するためと予想</span></div><div><span style="font-size: 10pt;">　「分類に有効」かつ「摂動に頑健」な特徴量</span></div><div><span style="font-size: 10pt;">　「分類に有効」かつ「摂動に脆弱」な特徴量</span></div><div><span style="font-size: 10pt;">　の両方を学習するため脆弱になるらしい</span></div><div><span style="font-size: 10pt;">　</span><a href="https://arxiv.org/abs/1905.02175v3" style="font-size: 10pt;">https://arxiv.org/abs/1905.02175v3</a></div><div><span style="font-size: 10pt;">ATすることでモデルは通常のtrainとは違うところを注目するらしい</span></div><div><span style="font-size: 10pt;">→画像ならエッジ（物体の境界。顔でいうところの輪郭）を注目するようになる。</span></div><div><span style="font-size: 10pt;">　これは人間に近い見方。</span></div><div><span style="font-size: 10pt;">　通常のモデルはエッジではなくテクスチャ（物体の表面の変化。色の強弱や模様など）をもとには予測している（Imagenetではそうだった</span> <a href="https://arxiv.org/abs/1811.12231" style="font-size: 10pt;">https://arxiv.org/abs/1811.12231</a><span style="font-size: 10pt;">）</span></div><div><a href="https://www.slideshare.net/cvpaperchallenge/adversarial-examples-229232499" style="font-size: 10pt;">https://www.slideshare.net/cvpaperchallenge/adversarial-examples-229232499</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Adversarial Training: モデルを誤判断させる敵対的サンプルを学習データに入れる学習法。</span></div><div><span style="font-size: 10pt;">予め敵対的サンプルを学習データに加えておくことにより、少し敵対的サンプルへの耐性が向上し、場合によっては汎化性能も向上することもあります（工夫加えずにただtrainデータに敵対的サンプル加えるだけだと、敵対的サンプルの予測精度は上がるが、敵対的サンプルではないデータの精度下がる）。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Human-in-the-Loop：</span></div><div><span style="font-size: 10pt;">本来機械学習に限らずシミュレータなどのシステムの中に実際の人間が入り込みモックアップなどを果たすことを指す</span></div><div><span style="font-size: 10pt;">近年では機械学習の文脈で、機械学習モデルと人間が補完し合いながら動作するシステムを指して使われている</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■数列: 1,2,3,4,…のように，ある規則によって並べられた数の列。{a_n}と書く。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■級数: a0+a1+⋯+an+ のような数列の和。Σa_n と表す。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■べき級数: Σ(a_n * x^n) のように数列にxべき掛けたもの</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■べき級数展開: べき級数をf(x)=a0+a1x+a2x2+a3x3+⋯のように単純な多項式に変形すること</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■虚数(i): 実際には存在しない数。虚数の定義は２回かけると−1になる値。</span></div><div><span style="font-size: 10pt;">実数では同じ数を２回かけても負の値にはなりません。</span></div><div><span style="font-size: 10pt;">複素数は実部と虚部からなる２次元のベクトル(z=a+bi)と考えることができます．</span></div><div><a href="https://qiita.com/mebiusbox2/items/657df6e458d85303e50d" style="font-size: 10pt;">https://qiita.com/mebiusbox2/items/657df6e458d85303e50d</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■波数: 長さが2πのなかにある波の数</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■フーリエ級数展開: 任意の関数f(x)を基本的な波であるsin(kx)とcos(kx)（波数kは整数）で級数展開する手法。</span></div><div><span style="font-size: 10pt;">f(x)=a0cos0x+a1cos1x+a2cos2x+a3cos3x+⋯みたいな三角関数(sinやcos)の足し算で表現する手法。</span></div><div><span style="font-size: 10pt;">フーリエ級数展開の目的は周波数の世界であるsin,cosの係数an,bn（いわゆる波の振幅）を求めること。</span></div><div><span style="font-size: 10pt;">どの波がどれぐらい強いのかがわかる。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■スペクトル: 横軸を周波数、縦軸を波の振幅で表した度数分布表。</span> <span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><span style="font-size: 10pt; color: rgb(0, 0, 0); font-family: メイリオ; font-variant-caps: normal; font-variant-ligatures: normal;">フーリエ級数展開したときのplotといえる</span></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■フーリエ変換（Fourier transform; FT）: フーリエ級数展開して、時間tの関数f(t)を、周波数ωの関数F(ω)に移す変換</span></div><div><span style="font-size: 10pt;">変換後の関数はもとの関数に含まれる周波数を記述し、しばしばもとの関数の周波数領域表現 (frequency domain representation) と呼ばれる。</span></div><div><span style="font-size: 10pt;">t=時間, y=波の強度をx=周波数(ω=2πv(位相速度)/λ(=1/k(波数))), y=デジタルデータ（スペクトル）に変換するとかで使う。</span></div><div><span style="font-size: 10pt;">フーリエ級数展開では、周期を持つ関数（持たない場合も繰り返して周期関数と考えて）を考えているが、</span></div><div><span style="font-size: 10pt;">周期を持たない一般的な関数（非周期関数）をフーリエ級数展開するために、フーリエ変換では展開は無限級数の形ではなく積分の形で表現される</span></div><div><a href="https://ja.wikipedia.org/wiki/%E3%83%95%E3%83%BC%E3%83%AA%E3%82%A8%E5%A4%89%E6%8F%9B" style="font-size: 10pt;">https://ja.wikipedia.org/wiki/%E3%83%95%E3%83%BC%E3%83%AA%E3%82%A8%E5%A4%89%E6%8F%9B</a></div><div><a href="http://www.ml.seikei.ac.jp/biolab/lecture/Bioelecronics/2.5Fourier.pdf" style="font-size: 10pt;">http://www.ml.seikei.ac.jp/biolab/lecture/Bioelecronics/2.5Fourier.pdf</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■離散フーリエ級数(Discrete Fourier Transform, DFT):</span></div><div><span style="font-size: 10pt;">一定期間の時系列の数値列を、その期間を周期的に繰り返す合成波とみなし、その周波数成分の複素数値の列(低周波数から高周波数の順)を算出する仕組み</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■高速フーリエ変換: 離散フーリエ変換という処理を高速に行うアルゴリズムです。これを利用して、多項式乗算が高速に行えます。</span></div><div><a href="https://qiita.com/ageprocpp/items/0d63d4ed80de4a35fe79" style="font-size: 10pt;">https://qiita.com/ageprocpp/items/0d63d4ed80de4a35fe79</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■窓関数(window function): ある有限区間以外で0となる関数</span></div><div><span style="font-size: 10pt;">ある関数や信号（データ）に窓関数が掛け合わせられると、区間外は0になり、有限区間内だけが残るので、数値解析が容易になる。</span></div><div><span style="font-size: 10pt;">窓関数は、スペクトル分析、フィルタ・デザインや、音声圧縮に応用される。</span></div><div><span style="font-size: 10pt;">窓関数を単に窓 (window) ともいい、データに窓関数を掛け合わせることを窓を掛ける (windowing) という</span></div><div><span style="font-size: 10pt;">フーリエ変換では、関数f(x)も三角関数も、無限区間で定義されている。</span></div><div><span style="font-size: 10pt;">しかし、実データを数値的にフーリエ変換するなら、無限の長さは扱えないので、有限区間でフーリエ変換をおこない、区間外は無視することになる。</span></div><div><span style="font-size: 10pt;">これは、関数f(x)を区間外で0とみなすことに等しい（「区間内のデータを周期的に繰り返す」という表現をすることもあるが、DFT（離散フーリエ変換）の場合はこの2つは等価である）。</span></div><div><span style="font-size: 10pt;">この区間外を0とするのにwindow関数が使われる</span></div><div><a href="https://ja.wikipedia.org/wiki/%E7%AA%93%E9%96%A2%E6%95%B0" style="font-size: 10pt;">https://ja.wikipedia.org/wiki/%E7%AA%93%E9%96%A2%E6%95%B0</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ガウシアンフィルタ: 画像処理では写真の平滑化などに使われるフィルタの1つ。 ノイズ除去を目的としたフィルターの代表。</span></div><div><span style="font-size: 10pt;">ガウス分布を利用して「注目画素からの距離に応じて近傍の画素値に重みをかける」という処理を行い、自然な平滑化を実現します。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■スケール不変: 画像を拡大しても画像の角（コーナー）の位置が変わらないこと</span></div><div><a href="http://whitewell.sakura.ne.jp/OpenCV/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html" style="font-size: 10pt;">http://whitewell.sakura.ne.jp/OpenCV/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■DevOps:開発と運用をくっつけたもの。開発から本番環境にコードを素早く映し、本番環境に贈られたコードを見るための部門を超えたプロセス。MLOpsの機械学習モデルがないだけ</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■MLOps(Machine Learning Operations):</span></div><div><span style="font-size: 10pt;">MLOpsの定義は様々。ざっくり言うと、機械学習の開発から運用までの全サイクル（モデルやサービスの学習、テスト、デプロイ）を継続的に行うための仕組み。MLOpsの1番の目的は「機械学習の価値を本番環境に素早く適用すること」</span></div><div><span style="font-size: 10pt;">以下はGoogleの定義。</span></div><div><span style="font-size: 10pt;">MLシステムにおいて</span></div><div><span style="font-size: 10pt;">CI (continuous integration：継続的インテグレーション=コードの変更をリポジトリにマージし、その度に自動化されたビルドとテストを実行する手法)、</span></div><div><span style="font-size: 10pt;">CD (continuous delivery:継続的デリバリー=継続的インテグレーションを拡張し、ビルド・テストが通ったらステージング環境や本番環境への反映(の準備)まで行う手法)、</span></div><div><span style="font-size: 10pt;">CT (continuous training:継続的学習=?)が自動化されている基盤</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■（2020年にAIと呼ばれる）機械学習モデル:与えられたデータを特定の指標(正解率など)で最適化したコンピュータのプログラム。</span></div><div><span style="font-size: 10pt;">機械学習モデルはブラックボックス(=使い方だけわかっていて、動作原理がわからない装置)と言われる理由は、モデルが出力する予測の根拠を人間が直感的または論理的に理解できないため。</span></div><div><span style="font-size: 10pt;">モデル内部の計算手順はプログラムであるし、実行時の内部の各計算結果も出力できるので機械学習モデルの動作原理は理解できる。</span></div><div><a href="https://www.slideshare.net/SatoshiHara3/ver2-225753735" style="font-size: 10pt;">https://www.slideshare.net/SatoshiHara3/ver2-225753735</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■XAI(Explainable AI):予測根拠を説明できるAI。予測結果以外の追加情報をモデルから抽出する技術(Grad-CAMとか)を入れたモデルと言える</span></div><div><a href="https://www.slideshare.net/SatoshiHara3/ver2-225753735" style="font-size: 10pt;">https://www.slideshare.net/SatoshiHara3/ver2-225753735</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Feature Importance:決定木モデルが使用する各特徴量の重要度を数値化したもの。</span></div><div><span style="font-size: 10pt;">そのモデルの分岐条件でどの説明変数が支配的かを示す。モデル内部での分岐条件がもとになる。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Permutation Importance:予測に各特徴量がどう影響するかを数値化したもの。</span></div><div><span style="font-size: 10pt;">これはある列（説明変数）を選択し、その列内の値をランダムに並び替えてモデルで再度予測することでその精度の変化から逆算的に、選択した列の重要度を測るアルゴリズムをつかう。</span></div><div><span style="font-size: 10pt;">（精度の差が大きい列ほど予測に重要な列と言える）</span></div><div><span style="font-size: 10pt;">予測に使う説明変数の順番入れ替えて精度比較するだけなので、モデルによらずリッジとかラッソ回帰でも使える</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">SHAP(SHapley Additive exPlanations):予測結果1件について、その予測を出すのにモデルがどの特徴量を重要視したか（寄与したか）を数値化したもの。</span></div><div><span style="font-size: 10pt;">機械学習の手法を問わず使うことができる。</span></div><div><span style="font-size: 10pt;">SHAPアルゴリズムについてはLIMEというものがベース。LIMEでは対象のデータ周辺で局所的にモデルを線形近似する。</span></div><div><a href="https://own-search-and-study.xyz/2019/10/05/shap-all-methods/" style="font-size: 10pt;">https://own-search-and-study.xyz/2019/10/05/shap-all-methods/</a></div><div><a href="https://qiita.com/Derek/items/262f1ea39c6298cdd1bb" style="font-size: 10pt;">https://qiita.com/Derek/items/262f1ea39c6298cdd1bb</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [104].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://research.miidas.jp/2019/11/shap%E5%80%A4%E3%81%A7%E8%A7%A3%E9%87%88%E3%81%99%E3%82%8B%E5%89%8D%E3%81%ABpermutation-importance%E3%82%92%E7%9F%A5%E3%82%8B/" style="font-size: 10pt;">https://research.miidas.jp/2019/11/shap%E5%80%A4%E3%81%A7%E8%A7%A3%E9%87%88%E3%81%99%E3%82%8B%E5%89%8D%E3%81%ABpermutation-importance%E3%82%92%E7%9F%A5%E3%82%8B/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><img src="用語_files/Image [105].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div><span style="font-weight: bold;">縦軸：上から順に、目的変数に対する寄与の大きさ</span></div><div><span style="font-weight: bold;">横軸：左側は目的変数に対して負の寄与、右側は正の寄与</span></div><div><span style="font-weight: bold;">色：青は小さな値、赤は大きな値</span></div><div><span style="font-weight: bold;">→色がくっきり分かれていて、左右に大きく散らばってるのであれば強い相関がある説明変数である</span></div><div><span style="font-weight: bold;">→青が左側、赤が右側なら正の相関</span></div><div><span style="font-weight: bold;">→赤が左側、青が右側なら負の相関</span></div><div><br/></div><div><span style="font-weight: bold;">上のplotはボストンの住宅価格予測問題のsummary_plotの結果</span></div><div><span style="font-weight: bold;">→LSTAT（地域の低所得者の割合）が大きいと、住宅価格は下がる</span></div><div><span style="font-weight: bold;">→RM（住居の平均部屋数）が大きいと。住宅価格は上がる</span></div><div><span style="font-weight: bold;">→CRIM（地域の犯罪発生率）が大きいと、住宅価格は下がる</span></div><div><span style="font-weight: bold;">→NOX（NOx 酸化窒素濃度）が大きいと、住宅価格は下がる</span></div><div><span style="font-weight: bold;">→DIS（ボストン雇用センターまでの距離）が小さいと、住宅価格は下がる</span></div><div><span style="font-weight: bold;">→PTRATIO（教師一人当たりの学生数）が大きいと、住宅価格は下がる</span></div><div><a href="https://data-viz-lab.com/teleworkanalysis-shap">https://data-viz-lab.com/teleworkanalysis-shap</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Graph Convolutional Network(GCN): 頂点(node)と辺(edge)からなるネットワーク構造を取るグラフに対して畳み込み演算 (Graph convolution) を行うニューラルネットワーク。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■DeepChem: 化合物のDeepLearningライブラリ。TensorFlowでGCNを実装しており、化合物のグラフ構造からGCNのモデルを作成することができる。</span></div><div><span style="font-size: 10pt;">　</span><a href="https://deepchem.io/" rev="en_rl_none" style="font-size: 10pt;">https://deepchem.io/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■SMURFF: Matrix Factorizationを行うためのライブラリ。</span></div><div><span style="font-size: 10pt;">　</span><a href="https://smurff.readthedocs.io/en/latest/" rev="en_rl_none" style="font-size: 10pt;">https://smurff.readthedocs.io/en/latest/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Matrix factorization: 1つの行列を2つの行列の積に分解する機械学習アルゴリズム。2つの行列に分けることで元の行列の疎の要素を予測することができる。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Adversarial Autoencoders: 中間層にGANを使って正則化させるオートエンコーダー</span></div><div><a href="http://musyoku.github.io/2016/02/22/adversarial-autoencoder/" style="font-size: 10pt;">http://musyoku.github.io/2016/02/22/adversarial-autoencoder/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■形態素解析: 文章を名詞/動詞/副詞などの単語に辞書データを用いて分解する自然言語処理の前処理。英語だと、gets→get, got→get にするなどの前処理も行う</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■N-gram: 自然言語処理の前処理。連続したN文字以上の文章から1文字ずつずらしながら文字を取り出す方法。N=3の時に「明日は晴れ。」という文章からは下記のような文字の塊が取得できる</span></div><ul><li><div><span style="font-size: 10pt;">「明」「日」「は」「晴」「れ」「。」</span></div></li><li><div><span style="font-size: 10pt;">「明日」「日は」「は晴」「晴れ」「れ。」</span></div></li><li><div><span style="font-size: 10pt;">「明日は」「日は晴」「は晴れ」「晴れ。」</span></div></li></ul><div><span style="font-size: 10pt;">N-gramは抜け漏れなく文書を分解できるため、検索するための前処理や新たな単語を抽出する前処理として利用される</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■構文解析: 定義された文法に従って文の構造を明確にする前処理。文の語順（語が、文や句の中でとる位置の順序）を考慮したデータ形式</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■bag of words(言葉の袋): 文章に含まれる単語の種類ごとの指標を数値化する（単語の出現回数の表を作るような）前処理</span></div><div><span style="font-size: 10pt;">文の語順（語が、文や句の中でとる位置の順序）を考慮しないデータ形式</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [106].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■TF-IDF: 単語の重要度（=ある単語が文章の特徴づけにどの程度影響力があるのかを示した値）を表す指標</span></div><div><span style="font-size: 10pt;">Trem Frequency(文章中の単語の出現割合=[対象の単語の数] / [文章の単語の総数]) - Inverse Document Frequency(文章内の単語の出現割合によるスコア=log([全文章数] / [対象の単語が出現している文章数] + 1))</span></div><div><span style="font-size: 10pt;">TF * IDFの値が単語の重要度。重要な単語ほど値が大きくなる</span></div><div><span style="font-size: 10pt;">文章によって長さが違うため、TF-IDFは正規化が必要。 </span></div><div><span style="font-size: 10pt;">→文章ごとに、TF-IDFの2乗の合計を1にそろえるL2ノルムという手法がよく使われる。</span></div><div><span style="font-size: 10pt;">（ここでいう文章の単位は小説や論文など。例えば「走れメロス」1冊が1文書と考える）</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■山下り法：近傍の内で最も成績の良いものが現在の解より良ければ入れ替えるを繰り返す最適化手法。初期値によって局所解になりうる。</span></div><div><span style="font-size: 10pt;">　バッチ学習:</span></div><div><span style="font-size: 10pt;">　　勾配法 - 最急勾配法，(準)ニュートン法，L-BFGS</span></div><div><span style="font-size: 10pt;">　オンライン学習</span></div><div><span style="font-size: 10pt;">　　確率的勾配降下法 (SGD)</span></div><div><span style="font-size: 10pt;">　</span><a href="https://hayashibe.jp/note/ml/optimization/" style="font-size: 10pt;">https://hayashibe.jp/note/ml/optimization/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Facets：jupyterで動かすGoogle製の機械学習向け可視化ツール</span></div><div><a href="https://github.com/PAIR-code/facets" style="font-size: 10pt;">https://github.com/PAIR-code/facets</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ノルム：大きさを表す量。</span></div><div><span style="font-size: 10pt;">ベクトル空間ではベクトル大きさ=ノルム</span></div><div><span style="font-size: 10pt;">2点間の直線距離の差からなるベクトルのユークリッドノルム=ユークリッド距離</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■線形：下記の2つの性質が成り立つこと</span></div><div><span style="font-size: 10pt;">一言で言うと、加算や分割しても元の性質が保持できる（=変数と変数の関係が直線的）ってこと</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [107].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://qiita.com/te20/items/e91faee8f9eb9b1a869c" style="font-size: 10pt;">https://qiita.com/te20/items/e91faee8f9eb9b1a869c</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ユークリッド距離(Euclidean distance)：人が定規で測るような二点間の「通常の」距離のこと。</span><span style="font-size: 10pt; font-weight: bold;">点pと点qを直線でつないだ時の長さ</span></div><div><span style="font-size: 10pt;">ベクトルとしての大きさや距離の大きさだけなら、ユークリッドノルムと呼ばれる</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [108].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■マンハッタン距離（Manhattan distance）：</span><span style="font-size: 10pt; font-weight: bold;">各座標の差（の絶対値）の総和を2点間の距離</span><span style="font-size: 10pt;">とする概念</span></div><div><span style="font-size: 10pt;">L1-距離とも呼ばれる</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [109].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [110].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://mathtrain.jp/manhattan" style="font-size: 10pt;">https://mathtrain.jp/manhattan</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■チェビシェフ距離（Chebyshev distance）：</span><span style="font-size: 10pt; font-weight: bold;">各座標の差（の絶対値）の最大値を2点間の距離</span><span style="font-size: 10pt;">とした概念。</span></div><div><span style="font-size: 10pt;">例えば、p(0, 0)とq(1, 2)の2点間のチェビシェフ距離は2になる</span></div><div><span style="font-size: 10pt;">L∞-距離とも呼ばれる</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [111].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■マハラノビス距離：あるデータが平均からどれくらい離れているのかを表す量</span></div><div><span style="font-size: 10pt; font-weight: bold;">データの各方向への散らばり具合（=相関関係+分散）まで考慮した「データ群からの距離」を測る</span><span style="font-size: 10pt;">ときに使える</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [112].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">a:対象ベクトル、H:クラスターの重心ベクトル</span></div><div><span style="font-size: 10pt;">マハラノビス距離を使うことで「データの異常度」を定量化することができるため、</span><span style="font-size: 10pt; font-weight: bold;">異常検知などに使えます</span></div><div><span style="font-size: 10pt;">下記の例なら、赤データ群とのマハラノビス距離は緑の方が遠い。青の方が近い</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [113].png" type="image/png" data-filename="Image.png" title="Attachment"/><img src="用語_files/Image [114].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">上の例を、ユークリッド距離で測ると赤データ群を円として考えて測るため緑の方が近くなる</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [115].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■重心（幾何学的中心、centroid）：ベクトル空間での中心点。</span></div><div><span style="font-size: 10pt;">三角形の重心をベクトルで表すとき:g=a+b+c/3 となる</span></div><div><span style="font-size: 10pt;">要は、</span><span style="font-size: 10pt; font-weight: bold;">オブジェクトの全ての点の平均位置</span></div><div><span style="font-size: 10pt;">その点で図形をピン止めすればその図形が完全に釣り合うような点</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■重心（center of gravity）：その質量に対して他の物体から働く万有引力（重力）の合力の作用点。密度が全体で同じであるときcentroidと同じ</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■コサイン類似度：</span><span style="font-size: 10pt; font-weight: bold;">2本のベクトルがどれくらい同じ向きを向いているのかを表す指標</span><span style="font-size: 10pt;">。−1以上1以下の値をとる。</span></div><div><span style="font-size: 10pt;">値は2本のベクトルのなす角に相当、90 度のときが 0 である</span></div><div><span style="font-size: 10pt;">コサイン類似度が 1 に近い =  2本のベクトルは同じ向きに近い</span></div><div><span style="font-size: 10pt;">コサイン類似度が -1 に近い =  2本のベクトルは逆向きに近い</span></div><div><span style="font-size: 10pt;">コサイン距離と呼ぶこともある</span></div><div><span style="font-size: 10pt;">2次元ベクトルのコサイン類似度</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [116].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://mathtrain.jp/cosdistance" style="font-size: 10pt;">https://mathtrain.jp/cosdistance</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">下の図は2つのベクトルについて、dがユークリッド距離。θがコサイン類似度に相当</span></div><div><span style="font-size: 10pt; font-weight: bold;">コサイン類似度は、一般的にベクトル間の数値的な大きさを考慮しない場合に使用する測定方法</span></div><div><span style="font-size: 10pt; font-weight: bold;">あくまで2つのベクトルのなす角だけ気にするから</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [117].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://enjoyworks.jp/tech-blog/2242" style="font-size: 10pt;">https://enjoyworks.jp/tech-blog/2242</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Metric Learning：</span><span style="font-size: 10pt; font-weight: bold;">距離学習</span><span style="font-size: 10pt;">。機械学習の手法</span></div><div><span style="font-size: 10pt; font-weight: bold;">同じクラスのデータは距離が近くなるように、違うクラスのデータは距離が遠くなるように移動させてモデルを学習させる方法</span></div><div><span style="font-size: 10pt;">説明変数の数だけ次元持たせた埋め込み空間上で距離を測り（距離の概念はユークリッド距離やマハラノビス距離とかなんでもいい）学習させる</span></div><div><a href="https://cpp-learning.com/metric-learning/" style="font-size: 10pt;">https://cpp-learning.com/metric-learning/</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ペアワイズ距離：</span><span style="font-size: 10pt; font-weight: bold;">2つの集合の間の距離っぽい</span><span style="font-size: 10pt;">。要は2点（ペア）間の距離。距離の尺度はユークリッド距離やマハラノビス距離など何でもいい</span></div><div><span style="font-size: 10pt;">scipy.spatial.distance.pdistはpairwise distanceの意味で、二点間距離を一つのarrayで表現する</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Non-Maximum Suppression(NMS)：同じクラスとして認識された重なっている状態の領域を抑制するための物体検出アルゴリズム</span></div><div><span style="font-size: 10pt;">R-CNNの公式の論文で出た手法</span></div><div><span style="font-size: 10pt; font-weight: bold;">ただ単に、重なりの大きい領域を抑制（削除）しているだけ</span></div><div><span style="font-size: 10pt;">IoU値が大きければ、領域の重なりが大きいとみなして一方の領域を抑制（削除）します。</span></div><div><span style="font-size: 10pt;">逆に、IoU値が小さければ、領域の重なりが小さいとみなして両方の領域をそのままにしておきます。</span></div><div><span style="font-size: 10pt; font-weight: bold;"><img src="用語_files/Image [118].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt; font-weight: bold;">↓IoU値の閾値を0.3としたNMS（Iou&gt;0.3のbounding boxは削除、Iou&lt;0.3のbounding boxだけ残す）</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [119].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://meideru.com/archives/3538" style="font-size: 10pt;">https://meideru.com/archives/3538</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Intersection over Union(IoU)：画像の重なりの割合を表す値。</span></div><div><span style="font-size: 10pt; font-weight: bold;">IoU値が大きいほど、画像が重なっている</span></div><div><span style="font-size: 10pt;">IoU値=0のとき、画像は全く重なっていない</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [120].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://meideru.com/archives/3538" style="font-size: 10pt;">https://meideru.com/archives/3538</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Anchor(アンカー)：</span><span style="font-size: 10pt; font-weight: bold;">矩形（bounding box）の中心</span><span style="font-size: 10pt;">。物体検出モデルでよく出るワード</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Resion Proposal Network(RPN)：「ある画像のどこに物体が写っているか」=</span><span style="font-size: 10pt; font-weight: bold;">「物体が写っている場所と、その矩形の形」を検出できる機械学習モデル</span></div><div><span style="font-size: 10pt;">何が写っているか（分類）まではRPNではフォローしない。あくまで検出のみ</span></div><div><span style="font-size: 10pt;">分類はしないから、RPNは畳み込み層とプーリング層のみ</span></div><div><span style="font-size: 10pt;">Faster R-CNNで使用された</span></div><div><span style="font-size: 10pt;">手順は以下</span></div><div><span style="font-size: 10pt; font-weight: bold;">1.入力画像から特徴マップを出力</span></div><div><span style="font-size: 10pt;">※</span><span style="font-size: 10pt; font-weight: bold;">特徴マップはcnn通した後のデータ</span><span style="font-size: 10pt;">。元の画像が300x400x3であれば、feature mapsは18x25x512みたいなもの</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [121].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt; font-weight: bold;">2.特徴マップに対してAnchor boxesを設定する。</span></div><div><span style="font-size: 10pt;">※</span><span style="font-size: 10pt; font-weight: bold;">Anchorはfeature mapsの0,1次元数だけ作る。feature mapsは18x25x512なら、18x25=450個の点全てがAnchorとなる</span></div><div><span style="font-size: 10pt;">これにより、物体がどこにあってもどれかしらのAnchorがその物体の中心の役割を担う事ができる</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [122].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">※</span><span style="font-size: 10pt; font-weight: bold;">各Anchorから「基準の長さ」「縦横比」をそれぞれ決めることで、複数のAnchor boxesを作り出す</span></div><div><span style="font-size: 10pt;">基準の長さ→64,128,256（画像の長さを超えないように注意）</span></div><div><span style="font-size: 10pt;">縦横比→1:1, 1:2, 2:1（正方形、横長、縦長）</span></div><div><span style="font-size: 10pt;">のように設定すると、</span></div><div><span style="font-size: 10pt;">例えば(x,y)=(11,12)であるAnchorに対してAnchor boxesは以下のように作られる</span></div><div><span style="font-size: 10pt; font-weight: bold;">Anchor boxesは一つのAnchorに対して3（基準の長さ）x3（縦横比）=9つ作られます</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [123].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt; font-weight: bold;">Anchor boxesを作る際には各基準の長さごとにAnchor boxesの面積は揃える必要があることには注意してください。</span></div><div><span style="font-size: 10pt; font-weight: bold;">つまり基準の長さ=64のとき、1つのAnchor boxはのサイズは</span></div><div><span style="font-size: 10pt; font-weight: bold;">1:1→64×64 (=4096)</span></div><div><span style="font-size: 10pt; font-weight: bold;">1:2→45×91 (≒4096)</span></div><div><span style="font-size: 10pt; font-weight: bold;">2:1→91×45 (≒4096)</span></div><div><span style="font-size: 10pt; font-weight: bold;">になります。</span></div><div><span style="font-size: 10pt;">Anchor boxesは一つのAnchorに対して3（基準の長さ）x3（縦横比）=9つ作られ、アンカーは18x25個なので、</span><span style="font-size: 10pt; font-weight: bold;">18x25x9=4050個Anchor boxesできる</span></div><div><span style="font-size: 10pt; font-weight: bold;">3.Anchor boxesとGround Truthの情報を比較しながらRPNの教師データ(=RPNモデルの出力層）を作成する。</span></div><div><span style="font-size: 10pt; font-weight: bold;">※RPNでは</span></div><div><span style="font-size: 10pt; font-weight: bold;">1.あるAnchor boxの中身が背景か物体か</span></div><div><span style="font-size: 10pt; font-weight: bold;">2.物体だった場合、ground truthとどのくらいズレているか</span></div><div><span style="font-size: 10pt; font-weight: bold;">の2つを学習</span></div><div><span style="font-size: 10pt;">要は、2つの出力層がある</span></div><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">・「背景か物体か」については、ground truthとAnchor boxesのIOUを計算して、IOU&lt;0.3なら「背景」、IOU&gt;0.7なら「物体」とラベルを付けするので、18x25x18(=9x2:9個の</span><span style="font-size: 10pt; font-weight: bold;">Anchor boxesについて</span><span style="font-size: 10pt; font-weight: bold;">背景クラスと物体クラスの2つ)が出力層のサイズになる</span></font></div><div><span style="font-size: 10pt; font-weight: bold;"><img src="用語_files/Image [124].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">・「</span><span style="font-size: 10pt; font-weight: bold;">ground truthとのズレ</span><span style="font-size: 10pt; font-weight: bold;">」については、</span></font></div><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">(アンカーボックスのx座標のズレ|アンカーボックスのy座標のズレ|アンカーボックスの横幅のズレ|アンカーボックスの縦幅のズレ)の4クラスで各</span><span style="font-size: 10pt; font-weight: bold;">Anchor boxesのズレを表現する。</span><span style="font-size: 10pt; font-weight: bold;">18x25x36(=9x4)が出力層のサイズ</span></font></div><div><span style="font-size: 10pt; font-weight: bold;"><img src="用語_files/Image [125].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">RPNはこのように</span></div><div><span style="font-size: 10pt;">「物体か背景か」の分類問題</span></div><div><span style="font-size: 10pt;">と</span></div><div><span style="font-size: 10pt;">「ground truthとのズレ」の回帰問題</span></div><div><span style="font-size: 10pt;">を同時に解いていきます。</span></div><div><span style="font-size: 10pt;">前者はバイナリクロスエントロピー、後者はL1ノルム（絶対値誤差）をベースにした誤差関数を適用</span></div><div><a href="https://medium.com/lsc-psd/faster-r-cnn%E3%81%AB%E3%81%8A%E3%81%91%E3%82%8Brpn%E3%81%AE%E4%B8%96%E7%95%8C%E4%B8%80%E5%88%86%E3%81%8B%E3%82%8A%E3%82%84%E3%81%99%E3%81%84%E8%A7%A3%E8%AA%AC-dfc0c293cb69" style="font-size: 10pt;">https://medium.com/lsc-psd/faster-r-cnn%E3%81%AB%E3%81%8A%E3%81%91%E3%82%8Brpn%E3%81%AE%E4%B8%96%E7%95%8C%E4%B8%80%E5%88%86%E3%81%8B%E3%82%8A%E3%82%84%E3%81%99%E3%81%84%E8%A7%A3%E8%AA%AC-dfc0c293cb69</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Faster R-CNN：Microsoftが発明した物体検出アルゴリズム</span></div><div><span style="font-size: 10pt; font-weight: bold;">1.ある矩形が物体なのか背景なのかを学習</span></div><div><span style="font-size: 10pt; font-weight: bold;">2.1で検出した場所に、具体的に何が写っているのかを学習</span></div><div><span style="font-size: 10pt; font-weight: bold;">という2段構造</span></div><div><span style="font-size: 10pt; font-weight: bold;">Faster R-CNNの偉い所は1においてResion Proposal Network(RPN)と呼ばれるCNN構造を用いたところ</span></div><div><span style="font-size: 10pt;">今までは画像処理の手法であるSelective Searchを使っていたのですが、そこをDeep Learningによって実装している点が画期的だった</span></div><div><span style="font-size: 10pt;">&lt;Faster-RCNN全体の構造&gt;</span></div><div><span style="font-size: 10pt;">1. 入力画像をVGG16等に通し、特徴マップを得る。</span></div><div><span style="font-size: 10pt; font-weight: bold;">※このVGG16はRPNと共有する</span></div><div><span style="font-size: 10pt;">2. ROI Poolingという、入力を固定長に変換するアルゴリズムを適用し、入力特徴マップのサイズにかかわらず固定長の長さに変換する。</span><span style="font-size: 10pt; font-weight: bold;">（これによりRPNの出力層のデータのサイズに依存しない学習が可能となっている）</span></div><div><span style="font-size: 10pt;">3. 4096の全結合層に2回通し、最後にクラス分類用と、矩形のズレ回帰用の2種類の出力を得る。</span></div><div><span style="font-size: 10pt;">4. 1入力2出力のタスクとして、機械学習する。</span></div><div><span style="font-size: 10pt; font-weight: bold;">①RPN部分の勾配更新</span></div><div><span style="font-size: 10pt; font-weight: bold;">②Faster-RCNN全体の勾配更新</span></div><div><span style="font-size: 10pt; font-weight: bold;">を交互に行い、それぞれの精度を高めながら学習を進めます</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [126].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://medium.com/lsc-psd/faster-r-cnn%E3%81%AB%E3%81%8A%E3%81%91%E3%82%8Brpn%E3%81%AE%E4%B8%96%E7%95%8C%E4%B8%80%E5%88%86%E3%81%8B%E3%82%8A%E3%82%84%E3%81%99%E3%81%84%E8%A7%A3%E8%AA%AC-dfc0c293cb69" style="font-size: 10pt;">https://medium.com/lsc-psd/faster-r-cnn%E3%81%AB%E3%81%8A%E3%81%91%E3%82%8Brpn%E3%81%AE%E4%B8%96%E7%95%8C%E4%B8%80%E5%88%86%E3%81%8B%E3%82%8A%E3%82%84%E3%81%99%E3%81%84%E8%A7%A3%E8%AA%AC-dfc0c293cb69</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■sliding window：ただの畳み込み層のこと。昔は畳み込み層のことをsliding windowとも呼んでいたみたい</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ドメインシフト：学習に使用したデータと本番環境で予測の対象となるデータの性質が異なること</span></div><div><font style="font-size: 10pt;"><br/></font></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="font-size: 10pt;-en-paragraph:true;">■Big Transfer(BiT)</span> <span style="font-size: 10pt;-en-paragraph:true;">：どう工夫すれば転移学習の精度上げれるか調べた論文</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="font-size: 10pt;-en-paragraph:true;">解説</span><span style="font-size: 10pt;-en-paragraph:true;">:</span> <a href="https://qiita.com/omiita/items/90abe0799cf3efe8d93d" style="font-size: 10pt;-en-paragraph:true;">https://qiita.com/omiita/items/90abe0799cf3efe8d93d</a></div><div style="margin-top: 1em; margin-bottom: 1em;"><font style="font-size: 10pt;"><span style="font-size: 10pt;-en-paragraph:true;">&lt;</span><span style="font-size: 10pt;-en-paragraph:true;">この論文でやったこと</span><span style="font-size: 10pt;-en-paragraph:true;">&gt;</span></font></div><div style="margin-top: 1em; margin-bottom: 1em;"><font style="font-size: 10pt;"><span style="font-size: 10pt;-en-paragraph:true;">300</span><span style="font-size: 10pt;-en-paragraph:true;">万枚の画像</span><span style="font-size: 10pt;-en-paragraph:true;">を含む巨大なデータセットで</span><span style="font-size: 10pt;-en-paragraph:true;">ResNetv2-151</span><span style="font-size: 10pt;-en-paragraph:true;">の幅を</span><span style="font-size: 10pt;-en-paragraph:true;">4</span><span style="font-size: 10pt;-en-paragraph:true;">倍にした</span><span style="font-size: 10pt;-en-paragraph:true;">パラメータ数</span><span style="font-size: 10pt;-en-paragraph:true;">10</span><span style="font-size: 10pt;-en-paragraph:true;">億の巨大なモデルを事前学習</span><span style="font-size: 10pt;-en-paragraph:true;">してから、各タスクに</span><span style="font-size: 10pt;-en-paragraph:true;">ファインチューニングしたら</span><span style="font-size: 10pt;-en-paragraph:true;">SoTA</span><span style="font-size: 10pt;-en-paragraph:true;">叩き出しまくった</span></font></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="font-size: 10pt;-en-paragraph:true;">→</span> <span style="font-size: 10pt;-en-paragraph:true;">使用したデータセットの大きさによって</span><span style="font-size: 10pt;-en-paragraph:true;">BiT-S, BiT-M, BiT-L</span><span style="font-size: 10pt;-en-paragraph:true;">のモデルがあるよ。</span><span style="font-size: 10pt;-en-paragraph:true;">SoTA</span><span style="font-size: 10pt;-en-paragraph:true;">は</span><span style="font-size: 10pt;-en-paragraph:true;">BiT-L</span><span style="font-size: 10pt;-en-paragraph:true;">だよ。</span><span style="font-size: 10pt;-en-paragraph:true;">BiT-M</span><span style="font-size: 10pt;-en-paragraph:true;">は公開される予定</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><font style="font-size: 10pt;"><span style="font-size: 10pt;-en-paragraph:true;">→</span><span style="font-size: 10pt;-en-paragraph:true;">転移先のモデルは特別なハイパーパラメータチューニングなし</span><span style="font-size: 10pt;-en-paragraph:true;">(</span><span style="font-size: 10pt;-en-paragraph:true;">BiT</span><span style="font-size: 10pt;-en-paragraph:true;">ハイパーパラメータ</span><span style="font-size: 10pt;-en-paragraph:true;">:</span><span style="font-size: 10pt;-en-paragraph:true;">画像サイズ、</span><span style="font-size: 10pt;-en-paragraph:true;">Mixup</span><span style="font-size: 10pt;-en-paragraph:true;">つけるか否か、学習ステップ数だけ</span><span style="font-size: 10pt;-en-paragraph:true;">)</span><span style="font-size: 10pt;-en-paragraph:true;">で</span><span style="font-size: 10pt;-en-paragraph:true;">EfficientNet</span><span style="font-size: 10pt;-en-paragraph:true;">より高精度だった</span></font></div><div style="margin-top: 1em; margin-bottom: 1em;"><font style="font-size: 10pt;"><span style="font-size: 10pt;-en-paragraph:true;">※</span><span style="font-size: 10pt;-en-paragraph:true;">Bit</span><span style="font-size: 10pt;-en-paragraph:true;">ハイパーパラメーター以外のパラメーター（学習率とか）チューニングした方が精度上がるらしい。探索は</span><span style="font-size: 10pt;-en-paragraph:true;">20</span><span style="font-size: 10pt;-en-paragraph:true;">回程度で効果出るらしい。（実験では出来るだけパラメーターチューニングしたくないので</span><span style="font-size: 10pt;-en-paragraph:true;">3</span><span style="font-size: 10pt;-en-paragraph:true;">つに絞ってる）</span></font></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="font-size: 10pt;-en-paragraph:true;">→</span> <span style="font-size: 10pt;-en-paragraph:true;">BiT-L</span><span style="font-size: 10pt;-en-paragraph:true;">は</span><span style="font-size: 10pt;-en-paragraph:true;">1</span><span style="font-size: 10pt;-en-paragraph:true;">クラスあたりのデータ数が少なくても高い精度を叩き出し</span><span style="font-size: 10pt;-en-paragraph:true;">た。特筆すべきは、</span><span style="font-size: 10pt;-en-paragraph:true;">ILSVRC-2012</span><span style="font-size: 10pt;-en-paragraph:true;">データセットにおいては</span><span style="font-size: 10pt;-en-paragraph:true;">1</span><span style="font-size: 10pt;-en-paragraph:true;">クラスあたり</span><span style="font-size: 10pt;-en-paragraph:true;">1</span><span style="font-size: 10pt;-en-paragraph:true;">枚しか画像がない場合でも精度</span><span style="font-size: 10pt;-en-paragraph:true;">74.1 %</span> <span style="font-size: 10pt;-en-paragraph:true;">を叩き出している</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="font-size: 10pt;-en-paragraph:true;">→</span> <span style="font-size: 10pt;-en-paragraph:true;">大きいデータセットで大きいモデルを事前学習すると精度が上がることがわかった</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="font-size: 10pt;-en-paragraph:true;">→</span> <span style="font-size: 10pt;-en-paragraph:true;">巨大な事前学習データセットに対するベストプラクティスとしてわかったことは以下。</span></div><ul><li><div><span style="font-size: 10pt;">かなり長い学習を要する(数週間精度向上が見えなくても数ヶ月単位で見ると精度が上がり続けていた)</span></div></li><li><div><span style="font-size: 10pt;">Weight Decayが小さい方(1e−5)が収束が早いが、最終的な精度はWeight Decayが大きい方(1e−4)がよかった。</span></div></li><li><div><span style="font-size: 10pt;">最適化アルゴリズムはシンプルな</span><a href="https://qiita.com/omiita/items/1735c1d048fe5f611f80#5-%E3%83%A2%E3%83%BC%E3%83%A1%E3%83%B3%E3%82%BF%E3%83%A0" style="font-size: 10pt;">モーメンタム</a><a href="https://qiita.com/omiita/items/1735c1d048fe5f611f80#5-%E3%83%A2%E3%83%BC%E3%83%A1%E3%83%B3%E3%82%BF%E3%83%A0" style="font-size: 10pt;">SGD</a><span style="font-size: 10pt;">が一番よかった。</span></div></li></ul><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■PatchUp : あるデータでの畳み込み層の特徴マップ上のある領域の値を、他のデータの特徴マップの同じ領域の値に入れ替える or 混合する画像系DLの正則化手法</span></div><div><img src="用語_files/Image [127].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div><a href="https://www.arxiv-vanity.com/papers/2006.07794/#A4" style="font-size: 10pt;">https://www.arxiv-vanity.com/papers/2006.07794/#A4</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Attention：入力したデータのどこに注目するかを表現し、注目したデータに従って出力を行う仕組み</span></div><div><span style="font-size: 10pt; font-weight: bold;">CNNは画像全体に対して畳み込んでGlobal Average Pooling で背景含めて画像全体の特徴を平均化する</span></div><div><span style="font-size: 10pt; font-weight: bold;">→CNNは画像全体を畳み込むので余分な領域も特徴に含めている</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [128].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt; font-weight: bold;">Attentionレイヤーで注目してほしい特定の領域以外を黒塗りにした画像（特徴量map）を出力し、特定の領域のみを注目するニューラルネットにする</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [129].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt; font-weight: bold;">Attentionレイヤーは </span></div><div><span style="font-size: 10pt; color: rgb(255, 0, 0); font-weight: bold;">Conv2D→BatchNorm→ReLU→Conv2D→sigmoid</span></div><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">を分岐させて、</span><span style="font-size: 10pt; color: rgb(255, 0, 0); font-weight: bold;">出力の1枚(チャネルが1)の特徴量mapを分岐元の特徴量mapに掛け算</span><span style="font-size: 10pt; font-weight: bold;">する（sigmoid使うことで二値化して黒塗り実現している）</span></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [130].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">Attentionの良いところ</span></div><div><span style="font-size: 10pt;">→局所的なニューロンの接続なので全結合層よりパラメータの数減らせるので過学習起きにくい</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><a href="https://www.youtube.com/watch?v=g5DSLeJozdw" style="font-size: 10pt;">https://www.youtube.com/watch?v=g5DSLeJozdw</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Self-Attention：自身の特定のベクトルで自身の別のベクトルを特徴づけるAttention</span></div><div><span style="font-size: 10pt;">自然言語の場合</span></div><div><span style="font-size: 10pt;">→否定の「ない」が嫌いを特徴づける</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [131].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;"><br/></span></div><div><img src="用語_files/Scannable の文書 (2020-09-29 19_20_52).png" type="image/png" data-filename="Scannable の文書 (2020-09-29 19_20_52).png"/></div><div><span style="font-size: 10pt;">画像の場合</span></div><div><span style="font-size: 10pt;">→ピクセル毎のattention</span></div><div><span style="font-size: 10pt;">　CNNでは実現できない位置が離れたもの同士を特徴づけることができる</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [132].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [133].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><a href="https://www.youtube.com/watch?v=g5DSLeJozdw" style="font-size: 10pt;">https://www.youtube.com/watch?v=g5DSLeJozdw</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■RNN(Recurrent Neural Networks)：出力側から入力側へのフィードバックを持つニューラルネット(NN)。前時刻の出力を次の時刻の入力に連結して時系列を考慮できる</span></div><div><span style="font-size: 10pt;">時系列データの予測に向いている</span></div><div><span style="font-size: 10pt;">誤差逆伝播を各時刻で流さないとだめだからCNNに比べて時間かかる</span></div><div><span style="font-size: 10pt;">（CNNはフォワードNNだから1回誤差逆伝播するだけ。RNNはt=0のとき、t=-1のとき、t=-2のとき…と時刻ごとにN回誤差逆伝播する）</span></div><div><span style="font-size: 10pt;">弱点は前の出力を次の入力に入れてるから、時刻が長くなると昔の出力はかき消される</span></div><div><span style="font-size: 10pt;">（1つ前の結果だけを次の結果に考慮しないからマルコフ連鎖っぽいなあ）</span></div><div><span style="font-size: 10pt;">この弱点を克服したのがLSTM</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [134].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">RNNのレイヤーの構造はConv2DやDenseなど何でもいい（ふつうはDenseとTanh）</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [135].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">RNNにはいろいろ種類がある</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [136].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><a href="https://www.youtube.com/watch?v=yvqgQZIUAKg" style="font-size: 10pt;">https://www.youtube.com/watch?v=yvqgQZIUAKg</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■LSTM (Long short-term memory)：長期の記憶の保存と学習の安定性に優れたRNN</span></div><div><span style="font-size: 10pt; font-weight: bold;">LSTM = RNN + Cell State + Input Gate + Output Gate + Forget Gate の構造</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [137].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">→Cell State：毎時刻のRNNの出力を次の時刻のRNNの入力に足し算し続ける（書き換えず足し合わせるのがポイント。足し合わせの誤差逆伝播は来た勾配を加工せずに前の時刻に流すだけになるので学習が安定しやすい）</span></div><div><span style="font-size: 10pt;">例. t=0のRNN出力をt=1のRNNの入力に足し算</span></div><div><span style="font-size: 10pt;">　  t=1のRNN出力をt=2のRNNの入力に足し算</span></div><div><span style="font-size: 10pt;">     …</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [138].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">→Input Gate：各時刻の入力を選択的に取り込む。DenseとSigmoidでその時刻のデータを記憶すべきか制御する</span></div><div><span style="font-size: 10pt;">RNNの入力をDense→Sigmoid通したもの（sigmoidなので各要素は0.0-1.0の大きさのベクトル。0.0-1.0で大小変えるから門=Gateと読んでる）をRNNの出力の要素毎に掛け合わせる（アダマール積）。畳み込みみたいに内積ではない</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [139].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">→Output Gate：各時刻の出力を選択的に取り込む。Cell Stateの後ろについたInput Gate と同じもの（DenseとSigmoidで0.0-1.0出して掛け算して重みづけ）</span></div><div><span style="font-size: 10pt;">次の処理に有効そうな出力なら通す</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [140].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">→Forget Gate：記憶を選択的に忘却する</span></div><div><span style="font-size: 10pt;">Cell Stateで加算され続けているが不要な時刻の出力は消したほうが良い時に動作する</span></div><div><span style="font-size: 10pt;">他のGateと同様に DenseとSigmoidで0.0-1.0出して掛け算して重みづけし、</span></div><div><span style="font-size: 10pt;">1.0の値のときはCell stateの値を保持する</span></div><div><span style="font-size: 10pt;">0.0の値のときはCell stateの値を破棄する</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [141].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://www.youtube.com/watch?v=unE_hofrYrk" style="font-size: 10pt;">https://www.youtube.com/watch?v=unE_hofrYrk</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ダウンサンプリング：Poolingやストライド2以上のConvにより特徴量mapの解像度が小さくなること</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■アップサンプリング：ダウンサンプリングの逆。線形補間やPoolingと逆の操作などで特徴量mapの解像度を大きくすること</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ResNetで3x3のカーネルサイズ（NNのフィルターの大きさ）のConv2Dが多い理由</span></div><div><span style="font-size: 10pt;">小さいカーネルサイズ重ねると大きいカーネルサイズの演算と同じ処理になり、推論時大きいカーネルサイズよりもメモリや演算量がが削減できるから。また各層のReLUにより非線形の構造も取り込める</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [142].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://www.youtube.com/watch?v=UximUEjPQco" style="font-size: 10pt;">https://www.youtube.com/watch?v=UximUEjPQco</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■1x1(Pointwise) Convolution：カーネルサイズが1x1の畳み込み</span></div><div><span style="font-size: 10pt;">畳み込みだが要素1なので内積にはならず、N枚ある特徴量mapの同じ座標に位置する各1ピクセルを全結合(y = a*x1 + b*x2 + …)すること</span></div><div><span style="font-size: 10pt;">例えば、(128, 12, 12)の特徴量mapにそのまま1x1 Convolution かけると、(128, 12, 12)が出力される</span></div><div><span style="font-size: 10pt;">ResNetとかでは、不要な空間方向の特徴抽出を省き、演算量を抑え表現力上げている=ボトルネック構造</span></div><div><span style="font-size: 10pt; font-weight: bold;">→1x1 Convolutionは画像の枚数（特徴量mapのチャネルの枚数）を任意の枚数に減らすのに使われる</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [143].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://www.youtube.com/watch?v=UximUEjPQco" style="font-size: 10pt;">https://www.youtube.com/watch?v=UximUEjPQco</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Grouped Convolution：特徴量mapのすべての画像（チャネル）は使わず、グループに分けた画像のn枚だけで畳み込みを行うConv</span></div><div><span style="font-size: 10pt; font-weight: bold;">特徴量mapのすべての画像でConvすると計算膨大になるので、画像をグループでまとめて、グループ単位でConv実行する</span></div><div><span style="font-size: 10pt; font-weight: bold;">Grouped Convolution実行後、すべての画像の情報を共有するために1x1 Convolutionで特徴量mapのサイズグループ化前に戻す</span></div><div><span style="font-size: 10pt;">→ResNeXtはこれ使ってる</span></div><div><span style="font-size: 10pt; font-weight: bold;">Xceptionは1枚ごとのGrouped Convolution(Separable Conv)=各チャネルごとにConv実行している</span></div><div><span style="font-size: 10pt;">まとめると</span></div><div><span style="font-size: 10pt;">ResNet:Conv2D(Normal) &gt; ResNeXt:Grouped Convolution + 1x1 Convolution &gt; Xception:Separable Conv</span></div><div><span style="font-size: 10pt;">の順番でチャネル間の計算が複雑化していることになる</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [144].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://www.youtube.com/watch?v=UximUEjPQco" style="font-size: 10pt;">https://www.youtube.com/watch?v=UximUEjPQco</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Inception block：違うサイズのフィルターでConv2Dして混ぜるCNNの層</span></div><div><span style="font-size: 10pt;">普通のConv2Dは(128, 12, 12, batch_size)のようなテンソルにすべてに対して、5x5のようなフィルターを畳み込む（内積）</span></div><div><span style="font-size: 10pt;">Inception blockでは、5x5, 3x3x, 1x1, Poolig の4種類の操作を4分割したテンソル(32, 12, 12, batch_size)について畳み込み結合する</span></div><div><span style="font-size: 10pt;">→すべての特徴量mapに5x5する必要はないため。カーネルサイズ小さくしたらメモリも演算量も削減できる</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [145].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [146].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://www.youtube.com/watch?v=dqa2UGUH-48&amp;t=8s" style="font-size: 10pt;">https://www.youtube.com/watch?v=dqa2UGUH-48&amp;t=8s</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■DenseNet：各層は手前のすべての層の入力を用いて演算する</span></div><div><span style="font-size: 10pt;">本来は多層で行われているものを、入力を複数にしConvすることで入力サイズでかいが、出力サイズ小さくなるのでメモリと演算量削減できる</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [147].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [148].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://www.youtube.com/watch?v=dqa2UGUH-48&amp;t=8s" style="font-size: 10pt;">https://www.youtube.com/watch?v=dqa2UGUH-48&amp;t=8s</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■CNNは深い層では画像全体を見てる理由</span></div><div><span style="font-size: 10pt;">→深くなるほど特徴量mapの縦横が小さくなってフィルターのカーネルサイズで覆える範囲が大きくなるから</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [149].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><a href="https://www.youtube.com/watch?v=05qlCP-xL9Y" style="font-size: 10pt;">https://www.youtube.com/watch?v=05qlCP-xL9Y</a></div><div><font style="font-size: 10pt;"><br/></font></div><div>■<span style="font-weight: bold;">valid padding（paddingなし）のConv後の特徴量mapのサイズ = 入力サイズ - カーネルサイズ + 1</span></div><div>例. input=(128, 12, 12), kernel=(3, 3) → output=(128, 10, 10)</div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■RISE(</span>Randomized Image Sampling for Explanations<span style="font-size: 10pt;">)：</span></div><div>RIMEみたいに入力画像マスクしてCNNの重要な領域可視化する手法</div><div>論文ではGrad-CAMより対象物をはっきり可視化してたケースあり</div><div>Grad-CAMはGlobal Average Poolig(GAP)で1次元化してるNNでないと使えない（はず）だが、</div><div>RISEは画像隠してモデルの重要領域探してるだけなので、GAPの制限なしで使えるはず</div><div>RISEの制限</div><div>- RGBの3チャネル持つCNNである必要っぽい</div><div>- マスクする割合のパラメータ(N,s,p1)に依存しそう</div><div><img src="用語_files/Image [150].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div><a href="https://github.com/eclique/RISE">https://github.com/eclique/RISE</a></div><div><font style="font-size: 10pt;"><br/></font></div><div>■Representer Point Selection（RPS）：</div><div>学習データのラベルの付け間違いを検出</div><div>各テストデータの予測結果がどの学習データからどのくらい影響を受けているかを数値化</div><div>する手法</div><div>NeurIPS 2018に採択された論文</div><div><a href="https://arxiv.org/abs/1811.09720">https://arxiv.org/abs/1811.09720</a></div><div><br/></div><div>学習した画像xiの重み係数αiを計算して、</div><div>αiが大きい入力データは学習データの影響を受けている＝入力データは学習データに含まれている と解釈する</div><div>αiは予測ラベルと正解ラベルの差として計算する</div><div><br/></div><div>論文では|αij|（jは任意のクラスを示す）の大きい要素を有する画像xiを「ラベルの付け間違い」とし、</div><div>ラベルを修正して再学習させることで、精度の向上を達成している</div><div>αi の要素|αij|は、学習データ数 × クラス数 個できることになる</div><div><img src="用語_files/Image [151].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div><a href="https://www.kccs.co.jp/labellio/blog/2019/07/rps.html">https://www.kccs.co.jp/labellio/blog/2019/07/rps.html</a></div><div><br/></div><div><br/></div><div>■ConvLSTM2D: 時系列畳み込み。時間発展する画像の予測に適したCNN</div><div>基本的には入力とひとつ前の出力を畳み込んで活性化関数に適用して足すのを繰り返す</div><div>ひとつのConvLSTM2DにたくさんのConv2Dがあるのでかなり計算時間が遅くなる</div><div><br/></div><div>・入力: （&lt;batch_size&gt;, time_length, width, height, channel）のバッチサイズ+四次元</div><div>通常のConv2Dと違うところは一番最初のtime_length。</div><div>画像処理だったら、次の画像についての予測を何枚前の画像までその判断材料にするかを表す</div><div><br/></div><div>・出力: 2パターンある</div><div>（time_length,width,height,channel）の4次元</div><div>（width,height,channel）の3次元</div><div>動画像のシーンごとで解析を行いたい場合は4次元出力を、</div><div>最終的な出力を得たい場合は三次元</div><div>など用途に分けて使い分ける</div><div><br/></div><div>・モデル内での記述方法:</div><div>最低限必要な引数は以下の通り</div><div>filters：出力のチャンネル数です。</div><div>kernel：カーネルサイズです。3,５が多いです。</div><div>strides：言うまでもありませんね。</div><div>padding：パディングの方法です。</div><div>return_sequences：これが曲者です。</div><div>return_sequences = True　としたら4次元で出力されて</div><div>return_sequences = False　としたら3次元で出力されます。</div><div><img src="用語_files/Image [152].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div><a href="https://learnprogra.com/2019/11/03/convlstm2d/">https://learnprogra.com/2019/11/03/convlstm2d/</a></div><div><br/></div><div>tensoflow.kerasでの例(動画のデータで時系列画像モデル): <a href="https://keras.io/examples/vision/conv_lstm/">https://keras.io/examples/vision/conv_lstm/</a></div><div>CTスキャン画像での例: <a href="https://www.kaggle.com/kmader/what-is-convlstm">https://www.kaggle.com/kmader/what-is-convlstm</a></div><div><br/></div><div><br/></div><div>■Integrated Gradients: 勾配を用いて機械学習のモデルの出力データとラベルに対する入力された特徴量の寄与を求める手法</div><div>baseline: x′から入力: xまでの勾配を積分し、入力とbaselineとの差と積を取るだけ</div><div><img src="用語_files/Image [153].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div><a href="https://arxiv.org/abs/1703.01365">https://arxiv.org/abs/1703.01365</a></div><div><br/></div><div>Sensitivity(a), Implementation Invariance という2つの指標を満たしている手法なので、</div><div>勾配だけよりも特徴量の寄与をはっきり可視化できる</div><div>勾配から出すだけなので、画像やテキストなど入力の種類によらず使える</div><div><br/></div><div>Integrated Gradientsはbaselineの画像と出力の画像を比較して特徴量の寄与を求める</div><div>→真っ黒の何も写っていない画像(baseline)に比べて猫の写った画像はこういう風に異なるから、</div><div>これは猫の画像と判断したんだな、というように考えていく</div><div><br/></div><div>GoogleNetでIntegrated Gradientsを適用した結果</div><div><img src="用語_files/Image [154].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div>一番左が入力画像で、</div><div>その隣にラベルとスコアが書いてあり、</div><div>3列目がIntegrated Gradients×入力画像、</div><div>4列目が勾配×入力画像です。</div><div>これを見ると、単に勾配を用いる場合に比べて、物体を認識するのに必要そうな箇所が寄与していると判定されている</div><div><br/></div><div>同じような勾配ベースの手法であるDeepLiftはIntegrated Gradientsの性能が低いと指摘しているらしい</div><div><a href="https://qrunch.net/%40opqrstuvcut/entries/FKxqQpXc0lhh3LMn">https://qrunch.net/@opqrstuvcut/entries/FKxqQpXc0lhh3LMn</a></div><div><br/></div><div><br/></div><div>■スペクトログラム（声紋）: <span style="font-weight: bold;">周波数分析を時間的に連続して行い、色によって強さを表すことで、強さ、周波数、時間の３次元表示</span></div><div><span style="font-weight: bold;"><img src="用語_files/Image [155].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div>横軸が時間、縦軸が周波数（上に行くほど音が高い）、色が黄色いほど音が強い（音が大きい）</div><div>周波数と強さは線型目盛でも対数目盛でもよく、用途によって使い分ける</div><div>例えば音声信号の場合、強さを対数目盛（通常、dB）で表し、</div><div>倍音の関係を示す場合は周波数を線型目盛で表し、音楽的または音色的関係を表す場合は周波数を対数目盛で表す</div><div>スペクトログラムを作成する方法は主に2種類存在する</div><div>1つはバンドパスフィルタ群を使う方法、</div><div>もう1つは短時間フーリエ変換（STFT）で計算する方法</div><div><a href="http://adlib.rsch.tuis.ac.jp/~akira/zemi/ocampus/html/specana.html">http://adlib.rsch.tuis.ac.jp/~akira/zemi/ocampus/html/specana.html</a></div><div><a href="https://ja.wikipedia.org/wiki/%E3%82%B9%E3%83%9A%E3%82%AF%E3%83%88%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%A0#:~:text=%E3%82%B9%E3%83%9A%E3%82%AF%E3%83%88%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%A0%EF%BC%88%E8%8B%B1:%20Spectrogram%EF%BC%89%E3%81%A8%2C%E3%81%95%EF%BC%89%E3%81%A7%E8%A1%A8%E3%81%95%E3%82%8C%E3%82%8B%E3%80%82">https://ja.wikipedia.org/wiki/%E3%82%B9%E3%83%9A%E3%82%AF%E3%83%88%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%A0#:~:text=%E3%82%B9%E3%83%9A%E3%82%AF%E3%83%88%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%A0%EF%BC%88%E8%8B%B1%3A%20Spectrogram%EF%BC%89%E3%81%A8,%E3%81%95%EF%BC%89%E3%81%A7%E8%A1%A8%E3%81%95%E3%82%8C%E3%82%8B%E3%80%82</a></div><div><br/></div><div>メルスペクトログラムへ変換することにより、以下の効果があるそうです。</div><div>　・次元が下がる（本稿の場合、513次元　→　128次元）</div><div>　・低音域では敏感に、高音域では鈍感になる（人間の耳の仕様に近づく）</div><div><a href="https://qiita.com/shinmura0/items/858214154f889c05e4f4">https://qiita.com/shinmura0/items/858214154f889c05e4f4</a></div><div><br/></div><div>■スペクトル: 波形の短時間（512サンプルなど）の波形データの中にどのような周波数成分がどれだけ含まれるかを表した図</div><div><span style="font-weight: bold;">スペクトルは、横軸が周波数で縦軸が強度（振幅の2乗）</span></div><div><span style="font-weight: bold;">スペクトルのグラフには、時間が入ってこないのが特徴的</span></div><div>時間はFFTをかける波形の位置で決まるので時間はFFTする前にあらかじめ固定</div><div>ちなみに、元の波形は横軸が時間なので時間領域、スペクトルは横軸が周波数なので周波数領域という</div><div>この時間領域と周波数領域を相互に変換するのがフーリエ変換、逆フーリエ変換</div><div><img src="用語_files/Image [156].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div>スペクトルは、時間軸がないので「波形の時間経過に伴ってスペクトルがどのように変化するか」を見たい場合は、別の手法が必要になります</div><div>そのひとつが短時間離散フーリエ変換を使ったアニメーション</div><div>もうひとつの方法がスペクトログラム</div><div><span style="font-weight: bold;">スペクトログラムを使うとスペクトルの時間変化が一枚の画像で表せる</span></div><div><a href="http://aidiary.hatenablog.com/entry/20111001/1317441171">http://aidiary.hatenablog.com/entry/20111001/1317441171</a></div><div><br/></div><div>■内挿：trainデータの予測のこと</div><div>■外挿：testデータの予測のこと。</div><div>　→モデルの適用範囲 (Applicability Domain, AD) の外をどのくらい予測できるのか</div><div>　<a href="https://datachemeng.com/validation_of_extrapolation/">https://datachemeng.com/validation_of_extrapolation/</a></div><div><br/></div><div>■xfeat: pnfが作った特徴量エンジニアリングのライブラリ</div><div>データフレームから特徴量を作成するための各種エンコーダーを実装しています。cuDF を使うことでエンコーダーによっては 10~30 倍の高速化が可能となります。</div><div>Code: <a href="https://t.co/IbqRET9YA2">https://t.co/IbqRET9YA2</a> </div><div>Slides: <a href="https://t.co/8CY0IdCuJM">https://t.co/8CY0IdCuJM</a> <a href="http://pic.twitter.com/xM5HxRMQtj">pic.twitter.com/xM5HxRMQtj</a></div><div><br/></div><div><br/></div><hr/><div><span style="font-size: 10pt; font-weight: bold;">その他IT系</span></div><div><span style="font-size: 10pt;"><br/></span></div><div><span style="font-size: 10pt; font-weight: bold;">■クラウド: </span><span style="font-weight: bold;">プログラムからのコマンドによってメモリ増設などハードウエアのスペック変更できるサーバの仕組み</span></div><div><span style="font-weight: bold;">（ストレージを1TBから10TBに拡張しようと思ったならば，そのようにコマンドを送ればよい）</span></div><div><span style="font-weight: bold;">→ソフトウェアの一部かのように，物理的なハードウェアの管理・運用を行うことができる</span></div><div>クラウドの実態はデータセンターに置かれた膨大な数の計算機が大量の電力を消費しながら稼働している</div><div>クラウドプロバイダーはデータセンターの計算資源を上手にやりくりし，ソフトウェアとしてのインターフェースをユーザーに提供することで，このような仮想化・抽象化を達成している</div><div>ローカル計算環境と比べたクラウドの長所短所</div><div>&lt;長所&gt;</div><div>・自由にサーバーのサイズをスケールできる</div><div>・自分でサーバーをメンテナンスする必要がない</div><div>・初期コスト0（ランニングコストはかかるが）</div><div>&lt;短所&gt;</div><div>・使った分だけ金かかる（使い終わった計算資源はすぐに削除しないとすさまじいい請求がくる）</div><div>・セキュリティ（クラウドは，インターネットを通じて，世界のどこからでもアクセスできる）</div><div><a href="https://tomomano.gitlab.io/intro-aws/?utm_campaign=piqcy&amp;utm_medium=email&amp;utm_source=Revue%20newsletter#_hands_on_5_bashoutter">https://tomomano.gitlab.io/intro-aws/?utm_campaign=piqcy&amp;utm_medium=email&amp;utm_source=Revue%20newsletter#_hands_on_5_bashoutter</a></div><div><span style="font-size: 10pt;"><br/></span></div><div><span style="font-size: 10pt;">■REST(REpresentational State Transfer):分散型システムにおける複数のソフトウェアを連携させるのに適した設計原則の集合&lt;</span><a href="https://ja.wikipedia.org/wiki/REST" style="font-size: 10pt;">https://ja.wikipedia.org</a><a href="https://ja.wikipedia.org/wiki/REST" style="font-size: 10pt;">出力する</a><a href="https://ja.wikipedia.org/wiki/REST" style="font-size: 10pt;">/wiki/REST</a><span style="font-size: 10pt;">&gt;</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■REST API(or RESTful API)：RESTの原則に則って構築されたWeb API。&lt;</span><a href="https://en.wikipedia.org/wiki/Representational_state_transfer" rev="en_rl_none" style="font-size: 10pt;">https://en.wikipedia.org/wiki/Representational_state_transfer</a><span style="font-size: 10pt;">&gt;</span></div><div>REST API はMethod と URI (Universal Resource Identifier) の組からなる</div><div><img src="用語_files/Image [157].png" type="image/png" data-filename="Image.png" title="Attachment"/></div><div>Method (メソッド) とは，&quot;どのような操作を行いたいか&quot;を抽象的に表す (&quot;動詞&quot;と捉えてもよい)</div><div>一方， URI は，操作が行われる対象 (リソースとも呼ばれる) を表す． </div><div>メソッドが動詞であることに対して， URI は&quot;目的語&quot;であると捉えても良い．</div><div>/status/home_timeline というリソース (ホームタイムラインのツイートの一覧) を取得せよ，という意味になる</div><div><a href="https://tomomano.gitlab.io/intro-aws/?utm_campaign=piqcy&amp;utm_medium=email&amp;utm_source=Revue%20newsletter#_hands_on_5_bashoutter">https://tomomano.gitlab.io/intro-aws/?utm_campaign=piqcy&amp;utm_medium=email&amp;utm_source=Revue%20newsletter#_hands_on_5_bashoutter</a></div><div><br/></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">■API(Application Programming Interface)：自己のソフトウェアを一部（WEB上などに）公開して、他のソフトウェアと機能を共有できるようにしたもの。</span></span></div><div><span style="font-size: 10pt;">　APIはWEB上で通信して利用することが普通なのでWEBAPIと呼ばれることも多く、APIと書いている場合は通常WEBAPIを指す。</span></div><div><span style="font-size: 10pt;">　APIは内部のコードまでは公開しないので「API」＝「機能＋仕様書」ともいえる。</span></div><div><span style="font-size: 10pt;">　自分のソフトウェア（アプリなど）にAPIの機能を埋め込むことで、アプリケーション同士で連携することが可能になる。</span></div><div><span style="font-size: 10pt;">　＜APIの例＞</span></div><div><span style="font-size: 10pt;">　インスタグラムはFacebook APIを使っている。Facebook APIを利用することで、同じFaceBookの友達がインスタグラムを使いはじめた時や、友達が投稿した時にお知らせしてくれたりします。</span></div><div><span style="font-size: 10pt;">　→APIによってアプリケーション同士連携</span></div><div><span style="font-size: 10pt;">　APIを利用する場合には「APIキー」と「シークレット」といわれるメールアドレスとパスワードの組み合わせのようなものが必ず必要。</span></div><div><span style="font-size: 10pt;">　API提供元から「APIキー」と「シークレット」をアプリ側に設定（これで、外部から勝手に利用されることがなくなる）したら、アプリでAPI使えるようになる。</span></div><div><span style="font-size: 10pt;">　</span><a href="https://www.sejuku.net/blog/7087#API-2" rev="en_rl_none" style="font-size: 10pt;">https://www.sejuku.net/blog/7087#API-2</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■YAML(YAML Ain't Markup Language): （人間にとって読みやすい）データ直列化フォーマット</span></div><div><span style="font-size: 10pt;">　YAMLは主に記号とインデントで構造を表現し、汎用性ではXMLに劣るもののXMLよりずっと読みやすく、編集、修正、および作成がずっと容易になっている</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Cytoscape：生物学上のパスウェイや分子間相互作用ネットワークを可視化＆解析できるフリーソフト。ネットワークの画像を出力することや解析も可能。</span></div><div><span style="font-size: 10pt;">　&lt;</span><a href="http://www.cytoscape.org/" rev="en_rl_none" style="font-size: 10pt;">http://www.cytoscape.org/</a><span style="font-size: 10pt;">&gt;</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Selenium WebDriver：ブラウザ操作を自動化できるテストツール。様々な言語、ブラウザに対応している。</span></div><div><span style="font-size: 10pt;">　https://www.seleniumhq.org/projects/webdriver/</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■DICOM（Digital Imaging and Communications in Medicine）：医療用の画像を装置間でやりとりするための通信プロトコル</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Visual Studio Code(VSCode): Microsoft社が開発したオープンソースのテキストエディタ。</span></div><div><span style="font-size: 10pt;">同じMicrosoft社製品のIDE(統合開発環境)である&quot;Visual Studio&quot;とは関係ない別製品。</span></div><div><span style="font-size: 10pt;">AtomとPycharmの良いとこ取りみたいなエディタ。</span></div><div><span style="font-size: 10pt;">Windows、MacOS、Linuxのいずれの環境にもインストールできる。</span></div><div><span style="font-size: 10pt;">特徴は、軽量なので起動が早い、デバッグ機能、JavaやPython、SQL、JSONなど多くの言語をサポート、etc。</span></div><div><a href="https://eng-entrance.com/texteditor-vscode" style="font-size: 10pt;">https://eng-entrance.com/texteditor-vscode</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■Git:ソースコードの（分散）バージョン管理ツール</span></div><div><span style="font-size: 10pt;">Gitでは、全体を統括する「リモートリポジトリ」の他に、開発者ごとに「ローカルリポジトリ」を持つ。</span></div><div><span style="font-size: 10pt;">自分のマシンの中にあるローカルリポジトリに変更を記録し、しかるべきタイミングでリモートリポジトリに変更履歴をアップします。</span></div><div><span style="font-size: 10pt;">これにより、ネットワークが繋がらない環境でもバージョン管理ができ、全体の整合性を保ちやすい。</span></div><div><span style="font-size: 10pt;">※TortoiseSVNなどのバージョン管理システムは、リポジトリは全体で1つだけ。</span></div><div><span style="font-size: 10pt;">　そのため、開発者が増えるとそれぞれの変更箇所がぶつかるなど、リポジトリに不整合が起こることもありました。</span></div><div><a href="https://www.i3design.jp/in-pocket/3111" style="font-size: 10pt;">https://www.i3design.jp/in-pocket/3111</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■GitHub:Gitを利用した、開発者を支援するWebサービス。クラウド上でGitを用いたソースコードのバージョン管理を行う。</span></div><div><span style="font-size: 10pt;">Gitには無い機能も追加されており、</span></div><div><span style="font-size: 10pt;">特にプルリクエスト（Pull Request）は、ソースコードの変更点について他のメンバーにレビュー依頼ができる機能。</span></div><div><span style="font-size: 10pt;">レビューがOKとなったソースコードだけを反映させることができるので、品質の向上に役立ちます。</span></div><div><a href="https://www.i3design.jp/in-pocket/3111" style="font-size: 10pt;">https://www.i3design.jp/in-pocket/3111</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■バックテスト:過去のデータに対してテストを行うこと</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■FLOPS: コンピュータさんの処理速度の指標。「1秒間に何問解けるか」を表す数値。でかいほど高性能</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■</span>情報管理の3原則</div><ol><li><div><span style="font-size: 10pt;">機密性:</span>情報が漏れないように管理すること</div></li></ol><div style="margin-left: 40px;">→不正アクセス対策、パスワードは誰でも見れるところに放置しないなど</div><div><br/></div><ol start="2"><li><div>可用性: 情報を使いたいときに使える状態にしておくこと</div></li></ol><div style="margin-left: 40px;">→データを別の媒体にバックアップしておくことなど</div><div><font style="font-size: 10pt;"><br/></font></div><ol start="3"><li><div>完全性: 情報を正確かつ最新の状態で管理すること</div></li></ol><div style="margin-left: 40px;">→ <span style="font-size: 13px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(51, 51, 51); font-family: メイリオ, Meiryo, &quot;ヒラギノ角ゴ Pro W3&quot;, &quot;Hiragino Kaku Gothic Pro&quot;, Verdana, &quot;ＭＳ Ｐゴシック&quot;, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal;">氏名や電話番号などが間違っている場合や、古くて役に立たないことがないようにする、</span><span style="color: rgb(51, 51, 51);">データを入力する際はダブルチェックを実施するなど</span></div><div style="margin-left: 40px;"></div><div><br/></div><div>■データウェアハウス: 分析業務で利用するための構造化された処理済みデータ格納先</div><div>■データレイク: とりあえず貯めてる生データ格納先</div><div><br/></div><div>■デジタルトランスフォーメーション（DX：Digital Transformation）:</div><div>進化したデジタル技術を浸透させることで人々の生活をより良いものへと変革すること</div><div>データやデジタル技術を駆使して、ビジネスに関わるすべての事象に変革をもたらすこと</div><div>→<span style="font-weight: bold;">データやデジタル技術の活用を軸に、</span></div><div><span style="font-weight: bold;">・従来なかった製品・サービス、ビジネスモデルを生み出す</span></div><div><span style="font-weight: bold;">・プロセスを再構築し、既存ビジネスに生産性の向上・コスト削減・時間短縮をもたらす</span></div><div><span style="font-weight: bold;">・業務そのものを見直し、働き方に変革をもたらす</span></div><div><span style="font-weight: bold;">・上記を実現する土壌として企業の在り方自体を見直す</span></div><div>→DXを実現するデジタルテクノロジー = AI, IoT, 5G etc</div><div><br/></div><div>■シリアライズ（serialize）: プログラミングでオプジェクト化された<span style="font-weight: bold;">データを、ファイルやストレージに保存したり、ネットワークで送受信したりできるような形に変換すること</span></div><div>逆に、シリアライズされたデータをプログラミングで扱えるようにオブジェクトの型に復元することをデシリアライズ（deserialize）という</div><div><br/></div><hr/><div><span style="font-size: 10pt; font-weight: bold;">画像変換系</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■濃度ヒストグラム(単に、ヒストグラムともいう)：画像の明るさの分布を表わす棒グラフ。</span></div><div><span style="font-size: 10pt;">　各濃度値に対し 画像全体で同じ濃度値を持つ画素数を求め、グラフ化したもの。</span></div><div><span style="font-size: 10pt;">　簡単に いえば、濃度値0の画素は、何個で、濃度値1の画素は、何個で......と数えて、棒グラフにして表したもの。</span></div><div><span style="font-size: 10pt;">　濃度ヒストグラムから濃度値の分布がわかる。</span></div><div><span style="font-size: 10pt;">　</span><a href="http://ipr20.cs.ehime-u.ac.jp/column/gazo_syori/chapter2.html" rev="en_rl_none" style="font-size: 10pt;">http://ipr20.cs.ehime-u.ac.jp/column/gazo_syori/chapter2.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ヒストグラム伸張化：画像の濃度値がある範囲に偏って分布しているような画像に対して、もっと広い範囲に濃度の分布を無理やり拡げるという濃度変換方法。</span></div><div><span style="font-size: 10pt;">　ヒストグラム伸張化をすることで、画像に使われている濃度値の幅を広くし、すべての濃度値が使われ、画像の明暗がわかりやすくなる。</span></div><div><span style="font-size: 10pt;">　</span><a href="http://ipr20.cs.ehime-u.ac.jp/column/gazo_syori/chapter2.html" rev="en_rl_none" style="font-size: 10pt;">http://ipr20.cs.ehime-u.ac.jp/column/gazo_syori/chapter2.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ヒストグラム平坦化：画像の濃度ヒストグラム（画像の明るさの分布を表わす棒グラフ）の度数を平坦にすることで、画像の明暗をよりはっきりさせる濃度変換方法。</span></div><div><span style="font-size: 10pt;">　また、ニューラルネットワークに入れて学習させる画像に対してヒストグラム平坦化を施すと、精度が上昇する場合がある。</span></div><div><span style="font-size: 10pt;">　</span><a href="http://ipr20.cs.ehime-u.ac.jp/column/gazo_syori/chapter2.html" rev="en_rl_none" style="font-size: 10pt;">http://ipr20.cs.ehime-u.ac.jp/column/gazo_syori/chapter2.html</a></div><div><span style="font-size: 10pt;">　</span><a href="https://qiita.com/Dason08/items/1b28e24d12630182fd69" rev="en_rl_none" style="font-size: 10pt;">https://qiita.com/Dason08/items/1b28e24d12630182fd69</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■適用的ヒストグラム平坦化：画像を小領域に分割して領域毎にヒストグラム平坦化を適用する変換方法。</span></div><div><span style="font-size: 10pt;">　</span><a href="http://labs.eecs.tottori-u.ac.jp/sd/Member/oyamada/OpenCV/html/py_tutorials/py_imgproc/py_histograms/py_histogram_equalization/py_histogram_equalization.html" rev="en_rl_none" style="font-size: 10pt;">http://labs.eecs.tottori-u.ac.jp/sd/Member/oyamada/OpenCV/html/py_tutorials/py_imgproc/py_histograms/py_histogram_equalization/py_histogram_equalization.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■トーンカーブ：画像の濃度変換関数をグラフにしたもの</span></div><div><span style="font-size: 10pt;">　</span><a href="http://satoh.cs.uec.ac.jp/ja/lecture/ComputerGraphics/13.pdf" rev="en_rl_none" style="font-size: 10pt;">http://satoh.cs.uec.ac.jp/ja/lecture/ComputerGraphics/13.pdf</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ガンマ補正：トーンカーブをガンマ曲線にした変換方法。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■シグモイド曲線補正：トーンカーブをシグモイド関数にした変換方法。</span></div><div><font style="font-size: 10pt;"><br/></font></div><hr/><div><span style="font-size: 10pt; font-weight: bold;">株関連</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■大陽線（大陽線）: 相場がすごく強い事を表わすローソク足（あし）。買いの勢い　&gt;&gt; 売りの勢い　」と表す</span></div><div><span style="font-size: 10pt;">概ね株価の５％程度以上の実体（胴体）をともなった時に、陽線であれば大陽線、陰線であれば大陰線と呼びます</span></div><div><span style="font-size: 10pt;">個人によってまちまちだが、</span></div><div><span style="font-size: 10pt;">大陽線の定義は高値から安値の値幅が7％以上であり、実体部分（始値から終値の幅）がローソク足の大半を占める場合</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■大陰線（だいいんせん）: 相場が強い先安感であることを表わすローソク足。買いの勢い　&lt;&lt; 売りの勢い　」と表す</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [158].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">■始値（はじめち）: 1日の取引がスタートして最初に付いた株価</span></div><div><span style="font-size: 10pt;">■終値（おわりね）: 1日の取引が終わった時の株価</span></div><div><span style="font-size: 10pt;">■高値（たかね）: 1日の取引で一番高い株価</span></div><div><span style="font-size: 10pt;">■安値（やすね）: 1日の取引で一番安い株価</span></div><div><span style="font-size: 10pt;">■約定（やくじょう）: 株式取引などの売買が成立すること</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■逆指値: 通常の指値注文とは逆で、指定したトリガー条件より</span><span style="font-size: 10pt; font-weight: bold;">株価が高くなったら「買い」、安くなったら「売り」</span><span style="font-size: 10pt;">の注文が自動的に発注される注文方法</span></div><div><span style="font-size: 10pt;">逆指値注文で「110円以下になったら売る」という条件を設定した場合、現在の株価が120円のため市場にはまだ売り注文は発注されない</span></div><div><span style="font-size: 10pt;">現在の株価が120円の時に110円で売り指値注文を出した場合、</span></div><div><span style="font-size: 10pt;">指値価格よりも高い価格で売れるため120円で約定</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [159].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ETF：取引所に上場したインデックスファンドをETFといいます。</span></div><div><span style="font-size: 10pt;">インデックスファンドとは、日経平均株価や東証株価指数（TOPIX）などの特定の指数の動きに連動する運用成果を目指した投資信託。</span></div><div><span style="font-size: 10pt;">ザックリ理解するのであれば、ETFとは“特定の指数に連動して価格が変化する銘柄</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■REIT：投資家から集めた資金をもとに不動産への投資を行い、その利益を投資者に配当する商品です。（ソーシャルレンディング？）</span></div><div><span style="font-size: 10pt;">ザックリ理解するのであれば、REITとは“事業内容が不動産の運用に限定された会社の株”のようなもの</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■セクター（業種）：銘柄コードの設定を行っている「証券コード協議会」では、業種（セクター）を10の大分類と33の中分類に分けている。</span></div><div><span style="font-size: 10pt;">特に、中分類は東証33業種といわれ、企業の業種を分類する際に広く利用されている。</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■出来高：一定期間において売買が成立した株数</span></div><div><font style="font-size: 10pt;"><br/></font></div></div><div><span style="font-size: 10pt;">■クロス取引：現物買いと信用売り※（空売り）を、同じ株数・同じ値段で同時におこなうこと</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■レーティング：証券会社などがそれぞれの銘柄について行う格付けのこと</span></div><div><span style="font-size: 10pt;">各証券会社は、投資家向けに市場の分析結果などの投資判断情報をレポートとして提供しており、</span></div><div><span style="font-size: 10pt;">そのなかでこの株は「買い」、この株は「売り」、この株はこの先も今ぐらいの株価のままだろう、といった格付けを行っている</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■勝率: 全トレードに対する勝ちトレードの割合</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [160].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ペイオフレシオ（損益率）: 勝ちトレードの平均利益額が負けトレードの平均損失額の何倍かを表す指標</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [161].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■プロフィットファクター: 総利益が総損失の何倍かを表す指標</span></div><div><span style="font-size: 10pt;">少ない損失で大きな利益を得られたほうが良いシステムと言えるため、プロフィットファクターがより大きい戦略の方が一般によい</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [162].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ドローダウン: 累計利益または総資産額が極大値から一時的に落ち込んだときに、その落ち込み具合（下落率）を表す指標</span></div><div><span style="font-size: 10pt;">最大ドローダウン50%という値が出た場合、その戦略は使えないと考えたほうがよい</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [163].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■シャープレシオ: 投資のリスク(株価のばらつき)に対するリターンの大きさを示す指標のひとつ</span></div><div><span style="font-size: 10pt;">シャープレシオは本来、「リスクに対し、安全資産（無リスク資産）に対する超過リターンがどの程度か」を示す値</span></div><div><span style="font-size: 10pt;">シャープレシオが大きいほど優れた商品（標準偏差が小さいので!!!!）</span></div><div><span style="font-size: 10pt;">目安としては、0.5～0.9で普通、1.0～1.9で優秀、2.0以上だと大変優秀</span></div><div><span style="font-size: 10pt;">→リスクを抑えながら高いリターンを獲得しているかを見る際には、「シャープレシオ」</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [164].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [165].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">実際に一般的なシステムトレードツールにおけるシャープレシオは次の式で求められた値が利用されている</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [166].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■インフォメーションレシオ: 「リスクに対し、ベンチマークに対する超過リターンがどの程度か」を示す値</span></div><div><span style="font-size: 10pt;">ゴールデンクロス・デッドクロスを利用した戦略、また目標株価で売買 を行う戦略が有効かどうかを判断する比較対象として、TOPIX Core30や日経 平均株価などの指数を利用しました。この比較対象となるものをベンチマークと言います。</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [167].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ソルティノレシオ: シャープレシオを改良したもの。</span></div><div><span style="font-size: 10pt;">シャープレシオだけでは分からない下落リスクの抑制度合い</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [168].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">→シャープレシオと同じく、大きいほど優れた商品（下方リスク=株価下がる方の大きさ が小さいので!!!!）</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [169].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">→T（目標リターン）は一般にはシャープレシオと同じように無リスク資産のリターンを使いますが、ソルティノレシオを複数の売買戦略のシミュレーション結果の比較をするのに用いる場合は、単純にT=0として問題ない</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■カルマーレシオ: 同じ安全さに対してどれだけ利益を上げられそうかという指標。運用実績がより良いことを示す</span></div><div><span style="font-size: 10pt;"><img src="用語_files/Image [170].png" type="image/png" data-filename="Image.png" title="Attachment"/></span></div><div><span style="font-size: 10pt;">→シャープレシオと同じく、大きいほど優れた商品（最大ドローダウンが小さいので!!!!）</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■良い企業を判断するために有用な代表的な指標</span></div><div><span style="font-size: 10pt;">①成長性の判断に有用な指標</span></div><div><span style="font-size: 10pt;">- 売上高: 物やサービスを売り、提供する対価として手に入れたお金</span></div><div><span style="font-size: 10pt;">- 営業利益: その企業がどれだけ本業でお金を儲けているかを示す重要な指標。売上高から原価と販売費および一般管理費（販管費）を引いた利益</span></div><div><span style="font-size: 10pt;">- 経常利益: 。財務面も含めた事業全体でどれだけ儲けているかを示す指標。営業利益に受け取り利息などの営業外収益を足して、銀行などに支払う利息などの営業外費用を引いたもの</span></div><div><span style="font-size: 10pt;">- 最終利益（純利益）: 税金を引いた残りが最終的に儲けとしてその企業に残るお金</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">②収益性(効率的にお金を稼ぐことができているか)の判断に有用な指標</span></div><div><span style="font-size: 10pt;">- 売上高営業利益率: 営業利益/売上高。この割合が大きいということは、その企業の本業において少ない費用で多くの利益を得ている</span></div><div><span style="font-size: 10pt;">- 売上高経常利益率: 経常利益/売上高。資金調達がうまくできているかなども含めた企業の総合的な収益性をみることができる指標</span></div><div><span style="font-size: 10pt;">- ROE （自己資本利益率）: 最終利益/自己資本(返済の義務がない資本)。ROEはその会社が株主からの投資資金を効率的に使ってお金を儲けたかを示す値</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">③安全性の判断に有用な指標</span></div><div><span style="font-size: 10pt;">- 流動比率: 1年以内に現金化することができる資産（流動資産）の額を、1年以内に支払期限が到来する負債（流動負債）で割った値</span></div><div><span style="font-size: 10pt;">- ROE(自己資本比率): 返済の義務のない資本を総資本で割った値。自己資本比率が高い会社は負債の割合が少ない会社ですので、一般に経営は安定し、倒産しにくい会社</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■割安・割高な株を見つけるのに有用な代表的な指標</span></div><div><span style="font-size: 10pt;">- PER（株価収益率）: 現在の株価が一株当たりの利益（EPS）の何倍かを示す値。一般にPERの数値が低いほど割安な株</span></div><div><span style="font-size: 10pt;">- PBR（株価純資産倍率）: 現在の株価が一株当たりの純資産（BPS）の何倍かを示す値。一般にPBRの数値が低いほど割安な株</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■VaR(Value at Risk)：あるリスク資産に投資した投資家が、将来の期間内において、ある確率の下で被る損失額の最大値</span></div><div><span style="font-size: 10pt;">99%の確率で起きる可能性のある損失額の内最大値を「信頼水準99%のVaR」と呼びVaR99と書く</span></div><div><span style="font-size: 10pt;">VaRはモンテカルロシミュレーション(正規分布乱数発生させて疑似的に値を生成する)で出せる</span></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ペアトレード：</span></div><div><span style="font-size: 10pt;">相関関係の高い二つの金融商品の差額の変動を利用して収益をあげる取引方法。</span></div><div><span style="font-size: 10pt;">株式の場合、同業種の株価は似たような動きをすることが多いため、</span></div><div><span style="font-size: 10pt;">よく似た動きをする2つの銘柄を保有し、</span></div><div><span style="font-size: 10pt; font-weight: bold;">割安な銘柄を「買い」、割高な銘柄を「空売り」しておき、株価が近づいたタイミングで反対売買を行い利益を確定する。</span></div><div><span style="font-size: 10pt;">相場がどのように変化しても影響を受けにくい。</span></div><div><a href="https://www.nomura.co.jp/terms/japan/he/A02607.html" style="font-size: 10pt;">https://www.nomura.co.jp/terms/japan/he/A02607.html</a></div><div><font style="font-size: 10pt;"><br/></font></div><div><span style="font-size: 10pt;">■ペアトレード戦略：</span></div><div><span style="font-size: 10pt;">共和分性（非定常のデータの足し算が定常になる性質）の平均回帰性を利用した売買戦略。</span></div><ol><li><div><span style="font-size: 10pt;">価格変動が似通った共和分性をもつ銘柄A,Bを見つける。共和分性見られるなら元の株価や収益率、ラグなど何でもいい</span></div></li><li><div><span style="font-size: 10pt;">共和分性見つけた銘柄A,Bの値について線形和:A(t)+βB(t) を考えて、線形和の2σの株価になったら、一方は売り、もう一方は買いのポジション持つ（別に2σ出なくていい。要は大きく離れたらポジション持つ）</span></div></li><li><div><span style="font-size: 10pt;">共和分性の線形和の平均回帰性より、いつかA(t)+βB(t)の平均値に値が戻ってくるので、そのタイミングで利確</span></div></li><li><div><span style="font-size: 10pt;">実運用では、平均値に来なければ、いつまでも持ち続けることになるので、期間や株価で損切ラインを決めないといけない</span></div></li></ol><div><a href="https://sigfin.org/?plugin=attach&amp;refer=019-11&amp;openfile=SIG-FIN-019-11.pdf" style="font-size: 10pt;">https://sigfin.org/?plugin=attach&amp;refer=019-11&amp;openfile=SIG-FIN-019-11.pdf</a></div><div><font style="font-size: 10pt;"><br/></font></div></div><div><br/></div></span>
</div></body></html> 