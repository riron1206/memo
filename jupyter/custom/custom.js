// leave at least 2 line with only a star on it below, or doc generation fails
/**
 *
 *
 * Placeholder for custom user javascript
 * mainly to be overridden in profile/static/custom/custom.js
 * This will always be an empty file in IPython
 *
 * User could add any javascript in the `profile/static/custom/custom.js` file.
 * It will be executed by the ipython notebook at load time.
 *
 * Same thing with `profile/static/custom/custom.css` to inject custom css into the notebook.
 *
 *
 * The object available at load time depend on the version of IPython in use.
 * there is no guaranties of API stability.
 *
 * The example below explain the principle, and might not be valid.
 *
 * Instances are created after the loading of this file and might need to be accessed using events:
 *     define([
 *        'base/js/namespace',
 *        'base/js/events'
 *     ], function(IPython, events) {
 *         events.on("app_initialized.NotebookApp", function () {
 *             IPython.keyboard_manager....
 *         });
 *     });
 *
 * __Example 1:__
 *
 * Create a custom button in toolbar that execute `%qtconsole` in kernel
 * and hence open a qtconsole attached to the same kernel as the current notebook
 *
 *    define([
 *        'base/js/namespace',
 *        'base/js/events'
 *    ], function(IPython, events) {
 *        events.on('app_initialized.NotebookApp', function(){
 *            IPython.toolbar.add_buttons_group([
 *                {
 *                    'label'   : 'run qtconsole',
 *                    'icon'    : 'icon-terminal', // select your icon from http://fortawesome.github.io/Font-Awesome/icons
 *                    'callback': function () {
 *                        IPython.notebook.kernel.execute('%qtconsole')
 *                    }
 *                }
 *                // add more button here if needed.
 *                ]);
 *        });
 *    });
 *
 * __Example 2:__
 *
 * At the completion of the dashboard loading, load an unofficial javascript extension
 * that is installed in profile/static/custom/
 *
 *    define([
 *        'base/js/events'
 *    ], function(events) {
 *        events.on('app_initialized.DashboardApp', function(){
 *            require(['custom/unofficial_extension.js'])
 *        });
 *    });
 *
 * __Example 3:__
 *
 *  Use `jQuery.getScript(url [, success(script, textStatus, jqXHR)] );`
 *  to load custom script into the notebook.
 *
 *    // to load the metadata ui extension example.
 *    $.getScript('/static/notebook/js/celltoolbarpresets/example.js');
 *    // or
 *    // to load the metadata ui extension to control slideshow mode / reveal js for nbconvert
 *    $.getScript('/static/notebook/js/celltoolbarpresets/slideshow.js');
 *
 *
 * @module IPython
 * @namespace IPython
 * @class customjs
 * @static
 */

/*
* サンプルのスニペット追加（「My favorites」というスニペット増やす）
* https://qiita.com/_snow_narcissus/items/647a6d3cec0a9bdb8980
*/
require(["nbextensions/snippets_menu/main"], function (snippets_menu) {
  console.log('Loading `snippets_menu` customizations from `custom.js`');
  var my_favorites = {
    'name': 'My favorites',
    'sub-menu': [
      {
        'name': 'first_cell',
        'snippet': [
          "!pwd",
          "import sys",
          "%reload_ext autoreload",
          "%autoreload 2",
          "%matplotlib inline",
          "sys.executable",
        ],
      },
      {
        'name': 'sys_path_append',
        'snippet': [
          "import sys",
          "sys.path.append(r'C:\\Users\\81908\\Git\\xfeat')",
          "import xfeat",
        ],
      },
      {
        'name': 'import_numpy_etc',
        'snippet': [
          "import os",
          "import numpy as np",
          "import pandas as pd",
          "import matplotlib.pyplot as plt",
          "import seaborn as sns",
          "sns.set()",
        ],
      },
      {
        'name': 'warnings_ignore',
        'snippet': [
          "import warnings",
          "warnings.filterwarnings('ignore')",
        ],
      },
      {
        'name': '__main__',
        'snippet': [
          "if __name__ == '__main__':",
        ],
      },
      {
        'name': 'pd.set_option',
        'snippet': [
          "pd.set_option('display.max_columns', None)",
        ],
      },
      {
        'name': 'import titanic',
        'snippet': [
          "import pandas as pd",
          "import seaborn as sns",
          "df = sns.load_dataset(\"titanic\")",
          "df = df.drop([\"alive\"], axis=1)",
          "for col in [\"sex\", \"embarked\", \"who\", \"embark_town\", \"class\", \"adult_male\", \"alone\", \"deck\"]:",
          "    df[col], uni = pd.factorize(df[col])",
        ],
      },
      {
        'name': 'type_hint',
        'snippet': [
          "import numpy as np",
          "import pandas as pd",
          "from typing import Dict, List, Union",
          "",
          "def calc_billing_amount(",
          "    x: np.ndarray,",
          "    df: pd.DataFrame,",
          "    _list: List[int],",
          "    _dict: Dict[str, Union[int, float]] = None,",
          ") -> int:",
          "    pass",
        ],
      },
      {
        'name': 'yaml',
        'snippet': [
          "import yaml",
          "yml = \"tmp.yml\"",
          "args = {'yml': yml}",
          "with open(args[\"yml\"]) as f:",
          "    config = yaml.load(f)",
          "    args.update(config)",
          "print(args)",
        ],
      },
      {
        'name': 'logging',
        'snippet': [
          "import datetime",
          "import logging",
          "import os",
          "import pathlib",
          "import numpy as np",
          "",
          "",
          "class Logger:",
          "    def __init__(self, output_dir=None):",
          "        self.general_logger = logging.getLogger(\"general\")",
          "        self.result_logger = logging.getLogger(\"result\")",
          "        stream_handler = logging.StreamHandler()",
          "",
          "        # ディレクトリ指定無ければカレントディレクトリにログファイル出す",
          "        output_dir = pathlib.Path.cwd() if output_dir is None else output_dir",
          "        file_general_handler = logging.FileHandler(",
          "            os.path.join(output_dir, \"general.log\")",
          "        )",
          "        file_result_handler = logging.FileHandler(",
          "            os.path.join(output_dir, \"result.log\")",
          "        )",
          "",
          "        if len(self.general_logger.handlers) == 0:",
          "            self.general_logger.addHandler(stream_handler)",
          "            self.general_logger.addHandler(file_general_handler)",
          "            self.general_logger.setLevel(logging.INFO)",
          "            self.result_logger.addHandler(stream_handler)",
          "            self.result_logger.addHandler(file_result_handler)",
          "            self.result_logger.setLevel(logging.INFO)",
          "",
          "    def info(self, message):",
          "        \"\"\"時刻をつけてコンソールとgeneral.log（ログファイル）に文字列書き込み\"\"\"",
          "        self.general_logger.info(\"[{}] - {}\".format(self.now_string(), message))",
          "",
          "    def result_scores(self, run_name, scores):",
          "        \"\"\"",
          "        計算結果をコンソールとresult.log（cv結果用ログファイル）に書き込み",
          "        parms: run_name: 実行したcvの名前",
          "        parms: scores: cv scoreのリスト。result.logには平均値も書く",
          "        \"\"\"",
          "        dic = dict()",
          "        dic[\"name\"] = run_name",
          "        dic[\"score\"] = np.mean(scores)",
          "        for i, score in enumerate(scores):",
          "            dic[f\"score{i}\"] = score",
          "        self.result(self.to_ltsv(dic))",
          "        ",
          "    def result(self, message):",
          "        \"\"\"コンソールとresult.logに文字列書き込み\"\"\"",
          "        self.result_logger.info(message)",
          "        ",
          "    def result_ltsv(self, dic):",
          "        \"\"\"コンソールとresult.logに辞書データ書き込み\"\"\"",
          "        self.result(self.to_ltsv(dic))",
          "",
          "    def now_string(self):",
          "        \"\"\"時刻返すだけ\"\"\"",
          "        return str(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))",
          "",
          "    def to_ltsv(self, dic):",
          "        \"\"\"辞書を文字列に変えるだけ\"\"\"",
          "        return \"\\t\".join([\"{}:{}\".format(key, value) for key, value in dic.items()])",
          "",
          "",
          "if __name__ == \"__main__\":",
          "    output_dir = \"tmp\"",
          "    os.makedirs(output_dir, exist_ok=True)",
          "    logger = Logger(output_dir)",
          "",
          "    logger.info(\"hogehoge\")  # general.logに文字列書き込み。実行時間も書いてくれる",
          "    logger.result(\"hogehoge\")  # result.logに文字列書き込み",
          "    logger.result_ltsv({\"hogehoge\": 1234})  # result.logに辞書データ書き込み",
          "    logger.result_scores(\"cv1\", [0.999, 0.5])  # result.logにcvの計算結果書き込み",
        ],
      },
      {
        'name': 'optuna_sample',
        'snippet': [
          "import os",
          "import argparse",
          "import traceback",
          "",
          "import optuna",
          "print(f\"optuna.__version__: {optuna.__version__}\")",
          "",
          "",
          "def get_dataset():",
          "    \"\"\" sample data \"\"\"",
          "    from sklearn.datasets import load_boston",
          "    X, y = load_boston(return_X_y=True)",
          "    return X, y",
          "",
          "",
          "def build_model(params):",
          "    \"\"\" ",
          "    sample model ",
          "    Usage:",
          "        params = {\"lr\": 0.1, \"input_shape\": [13,], \"hidden_node\": 16}",
          "        model = build_model(params)",
          "        model.summary()",
          "    \"\"\"",
          "    from tensorflow import keras",
          "    inputs = keras.layers.Input(params[\"input_shape\"])",
          "    x = inputs",
          "    x = keras.layers.Dense(params[\"hidden_node\"], activation='relu')(x)",
          "    x = keras.layers.Dense(1)(x)",
          "    model = keras.models.Model(inputs, x)",
          "    model.compile(",
          "        optimizer=keras.optimizers.Adam(lr=params[\"lr\"]),",
          "        loss=keras.losses.MeanSquaredError(),",
          "        metrics=['mae', 'mse']",
          "    )",
          "    return model",
          "",
          "",
          "class Objective(object):",
          "    def __init__(self, args):",
          "        self.args = args",
          "        # get dataset",
          "        self.X, self.y = get_dataset()",
          "        self.args[\"input_shape\"] = self.X.shape[1]",
          "",
          "    def set_params(self, trial):",
          "        \"\"\" Hyper parameter setting \"\"\"",
          "        params = self.args.copy()",
          "        params[\"hidden_node\"] = trial.suggest_categorical('hidden_node', ",
          "                                                           self.args[\"hidden_nodes\"])  # リストから選択",
          "        params[\"lr\"] = trial.suggest_loguniform('lr', ",
          "                                                self.args[\"lr_min\"], ",
          "                                                self.args[\"lr_max\"])  # 最大最小範囲から10倍刻みで選択",
          "        ",
          "        trial.set_user_attr('batch_size', self.args[\"batch_size\"])  # study.csvに表示したい属性値追加",
          "        return params",
          "        ",
          "    def trial_train(self, trial):",
          "        \"\"\" Train function for optuna \"\"\"",
          "        # ### モデルの学習処理書く ### #",
          "        params = self.set_params(trial)",
          "        model = build_model(params)",
          "        hist = model.fit(self.X, self.y)",
          "        eval_metric = hist.history[\"loss\"]",
          "        # ###########################",
          "        return min(eval_metric)  # Minimize metrics.",
          "",
          "    def __call__(self, trial):",
          "        \"\"\" Objective function for optuna \"\"\"",
          "        try: # optuna v0.18以上だとtryで囲まないとエラーでtrial落ちる",
          "            min_eval_metric= self.trial_train(trial)",
          "            return min_eval_metric",
          "        except Exception as e:",
          "            traceback.print_exc()  # Exceptionが発生した際に表示される全スタックトレース表示",
          "            return e  # 例外を返さないとstudy.csvにエラー内容が記載されない",
          "",
          "def get_args():",
          "    ap = argparse.ArgumentParser()",
          "    ap.add_argument(\"-o\", \"--output_dir\", type=str, default='output', help=\"output dir path\")",
          "    ap.add_argument(\"-i\", \"--input_dir\", type=str, default='input', help=\"input dir path\")",
          "    ap.add_argument(\"-n_t\", \"--n_trials\", type=int, default=10, help=\"Optuna num trials\")",
          "    ap.add_argument(\"-s\", \"--study_name\", type=str, default='study', help=\"Optuna trials study name\")",
          "    ap.add_argument(\"--hidden_nodes\", nargs=\"+\", type=int, default=[32, 16, 8], help=\"Hidden nodes\")",
          "    ap.add_argument(\"--batch_size\", type=int, default=32, help=\"Batch size\")",
          "    ap.add_argument(\"--lr_min\", type=float, default=0.01, help=\"Learning rate min\")",
          "    ap.add_argument(\"--lr_max\", type=float, default=0.1, help=\"Learning rate max\")",
          "    #args = vars(ap.parse_args())",
          "    args = vars(ap.parse_args(args=[]))  # notebookで argparseそのままで実行する方法",
          "    return args",
          "        ",
          "if __name__ == '__main__':",
          "    args = get_args()",
          "    os.makedirs(args[\"output_dir\"], exist_ok=True)",
          "    study_name = args[\"study_name\"]  # Unique identifier of the study.",
          "    study = optuna.create_study(study_name=study_name, ",
          "                                storage=f\"sqlite:///{args['output_dir']}/{study_name}.db\", ",
          "                                load_if_exists=True)",
          "    study.optimize(Objective(args), n_trials=args[\"n_trials\"])",
          "    study.trials_dataframe().to_csv(f\"{args['output_dir']}/{study_name}_history.csv\", index=False)",
          "    print(f\"\\nstudy.best_params:\\n{study.best_params}\")",
          "    print(f\"\\nstudy.best_trial:\\n{study.best_trial}\")",
        ],
      },
      {
        // 参考: https://employment.en-japan.com/engineerhub/entry/2019/09/10/103000#%E6%AC%A0%E6%90%8D%E5%80%A4%E3%82%92%E3%81%A9%E3%81%86%E5%87%A6%E7%90%86%E3%81%99%E3%82%8B%E3%81%8B
        'name': 'データ分析',
        'sub-menu': [
          {
            'name': '0.データ確認',
            'snippet': [
              "df = pd.read_csv(\"tmp.csv\")  # df = sns.load_dataset(\"titanic\")",
              "df.info()",
              "display(df.head().style.background_gradient(cmap=\"Pastel1\"))",
              "display(df.describe().T.style.background_gradient(cmap=\"Pastel1\"))",
              "",
              "# カラム名 / カラムごとのユニーク値数 / 最も出現頻度の高い値 / 最も出現頻度の高い値の出現回数 / 欠損損値の割合 / 最も多いカテゴリの割合 / dtypes を表示",
              "stats = []",
              "for col in df.columns:",
              "    stats.append(",
              "        (",
              "            col,",
              "            df[col].nunique(),",
              "            df[col].value_counts().index[0],",
              "            df[col].value_counts().values[0],",
              "            df[col].isnull().sum() * 100 / df.shape[0],",
              "            df[col].value_counts(normalize=True, dropna=False).values[0] * 100,",
              "            df[col].dtype,",
              "        )",
              "    )",
              "stats_df = pd.DataFrame(",
              "    stats,",
              "    columns=[",
              "        \"Feature\",",
              "        \"Unique values\",",
              "        \"Most frequent item\",",
              "        \"Freuquence of most frequent item\",",
              "        \"Percentage of missing values\",",
              "        \"Percentage of values in the biggest category\",",
              "        \"Type\",",
              "    ],",
              ")",
              "display(stats_df.sort_values(\"Percentage of missing values\", ascending=False).style.background_gradient(cmap=\"Pastel1\"))",
            ]
          },
          {
            'name': '1.重複,欠損データ確認',
            'snippet': [
              "# 重複データ確認",
              "_df = df.copy()  # 重複削除する場合はcopy不要",
              "print(f\"shape before drop: {_df.shape}\")",
              "_df.drop_duplicates(inplace=True)",
              "_df.reset_index(drop=True, inplace=True)",
              "print(f\"shape after drop: {_df.shape}\")",
              "",
              "# 欠損データ確認",
              "display(pd.DataFrame({\"is_null\": _df.isnull().sum()}))",
            ]
          },
          {
            'name': '2.EDA',
            'sub-menu': [
              {
                'name': '数値列の外れ値確認',
                'snippet': [
                  "# 外れ値確認",
                  "num_cols = df.select_dtypes(include=[\"int\", \"int32\", \"int64\", \"float\", \"float32\", \"float64\"]).columns.to_list()",
                  "plt.figure(figsize=(20, 10))",
                  "sns.boxplot(data=df[num_cols])",
                  "plt.show()",
                ]
              },
              {
                'name': '数値列のヒストグラム',
                'snippet': [
                  "# 数値列のヒストグラム。歪み（Skewness）大きい場合は対数変換した方が良い",
                  "num_cols = df.select_dtypes(include=[\"int\", \"int32\", \"int64\", \"float\", \"float32\", \"float64\"]).columns.to_list()",
                  "for col in num_cols:",
                  "    sns.distplot(df[col], label=f\"Skewness: {round(df[col].skew())}\")",
                  "    plt.legend()",
                  "    plt.show()",
                  "    plt.clf()",
                  "    plt.close()",
                ]
              },
              {
                'name': 'カテゴリ列のレコード数',
                'snippet': [
                  "# カテゴリ列のレコード数",
                  "cate_cols = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.to_list()",
                  "for col in cate_cols:",
                  "    sns.countplot(x=col, data=df, color='salmon')",
                  "    plt.show()",
                  "    plt.clf()",
                  "    plt.close()",
                ]
              },
              {
                'name': 'カテゴリ列に対するカテゴリ列のレコード数確認',
                'snippet': [
                  "# カテゴリ列 vs. カテゴリ列。これは１列ごとに見てかないと理解できない",
                  "df = sns.load_dataset(\"titanic\")",
                  "# countの場合は valuesはどの列選んでも同じ。columnsとindexが一意なレコードの数数える",
                  "df = (",
                  "    df.pivot_table(",
                  "        values=\"parch\",",
                  "        columns=\"survived\",",
                  "        index=[\"sex\"],",
                  "        aggfunc=\"count\",",
                  "    )",
                  ").sort_values(by=0, ascending=False)",
                  "df.plot.bar()",
                  "df"
                ]
              },
              {
                'name': '特徴量同士の相関ヒートマップ',
                'snippet': [
                  "# 特徴量同士の相関ヒートマップ",
                  "fig, ax = plt.subplots(figsize=(12, 9))",
                  "sns.heatmap(df.corr(), square=True, vmax=1, vmin=-1, center=0, annot=True)",
                  "plt.show()",
                ]
              },
              {
                'name': 'カテゴリ列に対する数値列の分布確認',
                'snippet': [
                  "# カテゴリ列 vs. 複数の数値列",
                  "cate_col = \"survived\"",
                  "num_columns = df.select_dtypes(include=[\"int\", \"int32\", \"int64\", \"float\", \"float32\", \"float64\"]).columns.to_list()",
                  "",
                  "for i, n_col in enumerate(num_columns):",
                  "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))",
                  "    fig.suptitle(n_col)",
                  "    sns.boxplot(x=cate_col, y=n_col, data=df, ax=ax[0])",
                  "    sns.violinplot(x=cate_col, y=n_col, data=df, ax=ax[1])",
                  "    plt.tight_layout(rect=[0,0,1,0.96])  # タイトル重ならないようにする",
                  "    plt.show()",
                  "    plt.clf()",
                  "    plt.close()",
                ]
              },
              {
                'name': '各カラムごとのレコード数と数値型の目的変数の平均値を可視化',
                'snippet': [
                  "def grouping(df, cols, agg_dict, prefix=\"\"):",
                  "    \"\"\"特定のカラムについてgroup化された特徴量の作成を行う",
                  "    Args:",
                  "        df (pd.DataFrame): 特徴量作成のもととなるdataframe",
                  "        cols (str or list): group by処理のkeyとなるカラム (listで複数指定可能)",
                  "        agg_dict (dict): 特徴量作成を行いたいカラム/集計方法を指定するdictionary",
                  "        prefix (str): 集約後のカラムに付与するprefix name",
                  "",
                  "    Returns:",
                  "        df (pd.DataFrame): 特定のカラムについてgroup化された特徴量群",
                  "    \"\"\"",
                  "    group_df = df.groupby(cols).agg(agg_dict)",
                  "    group_df.columns = [prefix + c[0] + \"_\" + c[1] for c in list(group_df.columns)]",
                  "    group_df.reset_index(inplace=True)",
                  "",
                  "    return group_df",
                  "",
                  "",
                  "def plot_target_stats(df, col, agg_dict, plot_config, figsize=(15, 8)):",
                  "    \"\"\"指定したカラムとtargetの関係を可視化する",
                  "    Args:",
                  "        df (pd.DataFrame): 可視化したい特徴量作成のもととなるdataframe",
                  "        col (str): group by処理のkeyとなるカラム",
                  "        agg_dict (dict): 特徴量作成を行いたいカラム/集計方法を指定するdictionary",
                  "",
                  "    \"\"\"",
                  "",
                  "    plt_data = grouping(df, col, agg_dict, prefix=\"\")",
                  "",
                  "    target_col = list(agg_dict.keys())[0]",
                  "",
                  "    # 2軸グラフの作成",
                  "    fig, ax1 = plt.subplots(figsize=figsize)",
                  "",
                  "    ax2 = ax1.twinx()",
                  "",
                  "    ax1.bar(",
                  "        plt_data[col],",
                  "        plt_data[f\"{target_col}_count\"],",
                  "        label=f\"{target_col}_count\",",
                  "        color=\"skyblue\",",
                  "        **plot_config[\"bar\"],",
                  "    )",
                  "    ax2.plot(",
                  "        plt_data[col],",
                  "        plt_data[f\"{target_col}_mean\"],",
                  "        label=f\"{target_col}_mean\",",
                  "        color=\"red\",",
                  "        marker=\".\",",
                  "        markersize=10,",
                  "    )",
                  "",
                  "    h1, label1 = ax1.get_legend_handles_labels()",
                  "    h2, label2 = ax2.get_legend_handles_labels()",
                  "",
                  "    ax1.legend(h1 + h2, label1 + label2, loc=2, borderaxespad=0.0)",
                  "    ax1.set_xticks(plt_data[col])",
                  "    ax1.set_xticklabels(plt_data[col], rotation=-90, fontsize=10)",
                  "",
                  "    ax1.set_title(",
                  "        f\"Relationship between {col}, {target_col}_count, and {target_col}_mean\",",
                  "        fontsize=14,",
                  "    )",
                  "    ax1.set_xlabel(f\"{col}\")",
                  "    ax1.tick_params(labelsize=12)",
                  "",
                  "    ax1.set_ylabel(f\"{target_col}_count\")",
                  "    ax2.set_ylabel(f\"{target_col}_mean\")",
                  "",
                  "    ax1.set_ylim([0, plt_data[f\"{target_col}_count\"].max() * 1.2])",
                  "    ax2.set_ylim([0, plt_data[f\"{target_col}_mean\"].max() * 1.1])",
                  "",
                  "    plt.tight_layout(rect=[0, 0, 1, 0.96])  # タイトル重ならないようにする",
                  "    plt.show()",
                  "    plt.clf()",
                  "    plt.close()",
                  "",
                  "# 各カラムごとのレコード数と数値型の目的変数の平均値を可視化",
                  "target_col = \"survived\"",
                  "for col in df.columns.to_list():",
                  "    agg_dict = {target_col: [\"count\", \"mean\"]}",
                  "    ",
                  "    plot_config = {\"bar\": {\"width\": 0.8}}",
                  "    plot_target_stats(df, col, agg_dict, plot_config)"
                ]
              },
              {
                'name': 'カスタムペアプロット',
                'snippet': [
                  "# カスタムペアプロット",
                  "import sys",
                  "sys.path.append(r'C:\\Users\\81908\\Git\\seaborn_analyzer')",
                  "",
                  "from custom_pair_plot import CustomPairPlot",
                  "# 行数列数多いと処理終わらないので注意",
                  "num_columns = df.select_dtypes(include=[\"int\", \"int32\", \"int64\", \"float\", \"float32\", \"float64\"]).columns.to_list()",
                  "CustomPairPlot().pairanalyzer(df[num_columns])  # dfでも実行できる",
                  "plt.show()"
                ]
              }
            ]
          },
          {
            'name': '3.欠損値変更',
            'sub-menu': [
              {
                'name': '欠損列の分布確認',
                'snippet': [
                  "# 欠損値を処理する方法を決めるために、各カラムの分布plot",
                  "_df_miss_sum = pd.DataFrame({\"is_null\": _df.isnull().sum()})",
                  "missing_columns = _df_miss_sum[_df_miss_sum[\"is_null\"] > 0].index.to_list()",
                  "",
                  "missing_columns = df[missing_columns].columns.to_list()",
                  "missing_num_columns = df[missing_columns].select_dtypes(include=[\"int\", \"int32\", \"int64\", \"float\", \"float32\", \"float64\"]).columns.to_list()",
                  "missing_cat_columns = df[missing_columns].select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.to_list()",
                  "",
                  "for col in missing_num_columns:",
                  "    plt.figure(figsize=(12, 5))",
                  "    plt.hist(df[col])",
                  "    plt.xlabel(f\"{col}\")",
                  "    plt.ylabel(\"X_count\")",
                  "    plt.title(f\"{col} - distribution\")",
                  "    plt.show()",
                  "for col in missing_cat_columns:",
                  "    plt.figure(figsize=(12, 5))",
                  "    sns.countplot(x=col, data=df, color='salmon')",
                  "    plt.title(f\"{col} - distribution\")",
                  "    plt.show()",
                ]
              },
              {
                'name': '欠損列削除',
                'snippet': [
                  "# 欠損列削除",
                  "cols_with_missing = [col for col in df.columns if df[col].isnull().any()]",
                  "df = df.drop(cols_with_missing, axis=1)",
                ]
              },
              {
                'name': '平均値で補完',
                'snippet': [
                  "# 平均値で補完",
                  "for col in missing_num_columns:",
                  "    column_mean = df[col].mean()",
                  "    df[col].fillna(column_mean, inplace=True)",
                ]
              },
              {
                'name': '最頻値で補完',
                'snippet': [
                  "# 最頻値で補完",
                  "for col in missing_cat_columns:",
                  "    column_mode = df[col].mode()[0]",
                  "    df[col].fillna(column_mode, inplace=True)",
                ]
              },
              {
                'name': '0で補完',
                'snippet': [
                  "# 0で補完",
                  "for col in missing_num_columns:",
                  "    df[col].fillna(0, inplace=True)",
                ]
              },
              {
                'name': '直前/直後の値を使って補間',
                'snippet': [
                  "method = \"ffill\"  # 直前の値を使って補間",
                  "method = \"bfill\"  # 直後の値を使って補間",
                  "# 直前/直後の値を使って補間。直前/直後の値無い場合は0で補間（※カテゴリ列はエラーになるので注意）",
                  "df[\"age\"] = df[\"age\"].fillna(method=method, axis=0).fillna(0)",
                ]
              },
              {
                // https://www.kaggle.com/alexisbcook/missing-values
                'name': '欠損値を補間して欠損フラグ列を追加',
                'snippet': [
                  "def impute_null_add_flag_col(df, strategy=\"median\", cols_with_missing=None, fill_value=None):",
                  "    \"\"\"欠損値を補間して欠損フラグ列を追加する",
                  "    fill_value はstrategy=\"constant\"の時のみ有効になる補間する定数",
                  "    \"\"\"",
                  "    from sklearn.impute import SimpleImputer",
                  "",
                  "    df_plus = df.copy()",
                  "",
                  "    if cols_with_missing is None:",
                  "        if strategy in [\"median\", \"median\"]:",
                  "            # 数値列で欠損ある列探す",
                  "            cols_with_missing = [col for col in df.columns if (df[col].isnull().any()) and (df[col].dtype.name not in [\"object\", \"category\", \"bool\"])]",
                  "        else:",
                  "            # 欠損ある列探す",
                  "            cols_with_missing = [col for col in df.columns if (df[col].isnull().any())]",
                  "",
                  "    for col in cols_with_missing:",
                  "        # 欠損フラグ列を追加",
                  "        df_plus[col + \"_was_missing\"] = df[col].isnull()",
                  "        df_plus[col + \"_was_missing\"] = df_plus[col + \"_was_missing\"].astype(int)",
                  "        # 欠損値を平均値で補間",
                  "        my_imputer = SimpleImputer(strategy=strategy, fill_value=fill_value)",
                  "        df_plus[col] = my_imputer.fit_transform(df[cols_with_missing])",
                  "",
                  "    return df_plus",
                  "",
                  "nulls = df.isnull().sum().to_frame()",
                  "null_indexs = [index for index, row in nulls.iterrows() if row[0] > 0]",
                  "impute_null_add_flag_col(df, cols_with_missing=null_indexs)  # 平均値で補間（数値列のみ）",
                  "impute_null_add_flag_col(df, strategy='median', cols_with_missing=null_indexs)  # 中央値で補間（数値列のみ）",
                  "impute_null_add_flag_col(df, strategy=\"most_frequent\", cols_with_missing=null_indexs)  # 最頻値で補間",
                  "impute_null_add_flag_col(df, strategy=\"constant\", cols_with_missing=null_indexs, fill_value=-1)  # 定数で補間",
                  "impute_null_add_flag_col(df, cols_with_missing=[\"deck\"], strategy=\"constant\", fill_value=\"X\")  # 指定列を定数で補間",
                ]
              },
            ]
          },
          {
            'name': '4.エンコディング',
            'sub-menu': [
              {
                'name': 'ラベルエンコディング',
                'snippet': [
                  "import sys",
                  "",
                  "sys.path.append(r\"C:\\Users\\81908\\Git\\xfeat\")",
                  "import xfeat",
                  "from xfeat import (",
                  "    Pipeline,",
                  "    SelectNumerical,",
                  "    SelectCategorical,",
                  "    LabelEncoder,",
                  ")",
                  "",
                  "def label_encode_xfeat(df):",
                  "    \"\"\"",
                  "    xfeatでobject型の列すべてラベルエンコディング",
                  "    ※bool型は変換できないので注意",
                  "    \"\"\"",
                  "    df_cate = Pipeline(",
                  "        [SelectCategorical(), LabelEncoder(output_suffix=\"\"),]",
                  "    ).fit_transform(df)",
                  "    df_num = Pipeline([SelectNumerical(),]).fit_transform(df)",
                  "    df = pd.concat([df_cate, df_num], axis=1)",
                  "    return df",
                  "df = label_encode_xfeat(df)",
                ]
              },
              {
                'name': 'one-hotエンコディング',
                'snippet': [
                  "# One-hot encoding for categorical columns with get_dummies",
                  "def one_hot_encoder(df, nan_as_category=True):",
                  "    original_columns = list(df.columns)",
                  "    categorical_columns = [col for col in df.columns if df[col].dtype == \"object\"]",
                  "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)",
                  "    new_columns = [c for c in df.columns if c not in original_columns]",
                  "    return df, new_columns",
                  "df, cat_cols = one_hot_encoder(df)  # 欠損値もダミー化する場合はnan_as_category=Trueにする",
                ]
              },
              {
                'name': 'カウントエンコディング',
                'snippet': [
                  "def count_encoder(train_df, valid_df, cat_features=None):",
                  "    \"\"\"",
                  "    Count_Encoding: カテゴリ列をカウント値に変換する特徴量エンジニアリング（要はgroupby().size()の集計列追加のこと）",
                  "    ※カウント数が同じカテゴリは同じようなデータ傾向になる可能性がある",
                  "    https://www.kaggle.com/matleonard/categorical-encodings",
                  "    \"\"\"",
                  "    # conda install -c conda-forge category_encoders",
                  "    import category_encoders as ce",
                  "    ",
                  "    if cat_features is None:",
                  "        cat_features = train_df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.to_list()",
                  "",
                  "    count_enc = ce.CountEncoder(cols=cat_features)",
                  "",
                  "    # trainだけでfitすること(validationやtest含めるとリークする)",
                  "    count_enc.fit(train_df[cat_features])",
                  "    train_encoded = train_df.join(count_enc.transform(train_df[cat_features]).add_suffix(\"_count\"))",
                  "    valid_encoded = valid_df.join(count_enc.transform(valid_df[cat_features]).add_suffix(\"_count\"))",
                  "    ",
                  "    return train_encoded, valid_encoded",
                  "",
                  "if __name__ == '__main__':",
                  "    # サンプルデータ",
                  "    import seaborn as sns",
                  "    df = sns.load_dataset(\"titanic\")",
                  "    df = df.drop([\"alive\"], axis=1)",
                  "    target_col = \"survived\"",
                  "    feats = df.columns.to_list()",
                  "    feats.remove(target_col)",
                  "    print(df.shape)",
                  "    ",
                  "    from tqdm import tqdm",
                  "    from sklearn.model_selection import *",
                  "    # KFoldで実行する場合",
                  "    folds = KFold(n_splits=4, shuffle=True, random_state=1001)",
                  "    for n_fold, (train_idx, valid_idx) in tqdm(enumerate(folds.split(df[feats], df[target_col]))):",
                  "        train_x, train_y = (df[feats].iloc[train_idx], df[target_col].iloc[train_idx],)",
                  "        valid_x, valid_y = (df[feats].iloc[valid_idx], df[target_col].iloc[valid_idx],)",
                  "        ",
                  "        # Count_Encoding",
                  "        train_x, valid_x = count_encoder(train_x, valid_x)",
                  "        print(f\"fold: {n_fold}:\", train_x.shape, valid_x.shape)",
                  "    ",
                  "    # test setでする場合",
                  "    train_df, test_df = count_encoder(df.iloc[:700,], df.iloc[700:,])",
                  "    print(train_df.shape, test_df.shape)",
                ]
              },
              {
                'name': 'ターゲットエンコディング',
                'snippet': [
                  "def target_encoder(train_df, valid_df, target_col:str, cat_features=None):",
                  "    \"\"\"",
                  "    Target_Encoding: カテゴリ列を目的変数の平均値に変換する特徴量エンジニアリング",
                  "    https://www.kaggle.com/matleonard/categorical-encodings",
                  "    \"\"\"",
                  "    # conda install -c conda-forge category_encoders",
                  "    import category_encoders as ce",
                  "",
                  "    if cat_features is None:",
                  "        cat_features = train_df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.to_list()",
                  "    ",
                  "    target_enc = ce.TargetEncoder(cols=cat_features)",
                  "",
                  "    # trainだけでfitすること(validationやtest含めるとリークする)",
                  "    target_enc.fit(train_df[cat_features], train_df[target_col])",
                  "",
                  "    train_encoded = train_df.join(target_enc.transform(train_df[cat_features]).add_suffix(\"_target\"))",
                  "    valid_encoded = valid_df.join(target_enc.transform(valid_df[cat_features]).add_suffix(\"_target\"))",
                  "    return train_encoded, valid_encoded",
                  "",
                  "",
                  "if __name__ == '__main__':",
                  "    # サンプルデータ",
                  "    import seaborn as sns",
                  "    df = sns.load_dataset(\"titanic\")",
                  "    df = df.drop([\"alive\"], axis=1)",
                  "    target_col = \"survived\"",
                  "    feats = df.columns.to_list()",
                  "    feats.remove(target_col)",
                  "    print(df.shape)",
                  "    ",
                  "    import pandas as pd",
                  "    from tqdm import tqdm",
                  "    from sklearn.model_selection import *",
                  "    # KFoldで実行する場合",
                  "    folds = KFold(n_splits=4, shuffle=True, random_state=1001)",
                  "    for n_fold, (train_idx, valid_idx) in tqdm(enumerate(folds.split(df[feats], df[target_col]))):",
                  "        train_x, train_y = (df[feats].iloc[train_idx], df[target_col].iloc[train_idx],)",
                  "        valid_x, valid_y = (df[feats].iloc[valid_idx], df[target_col].iloc[valid_idx],)",
                  "        train_df = pd.concat([train_x, train_y], axis=1)",
                  "        valid_df = pd.concat([valid_x, valid_y], axis=1)",
                  "        ",
                  "        # Target_Encoding",
                  "        train_df, valid_df = target_encoder(train_df, valid_df, target_col)",
                  "        print(f\"fold: {n_fold}:\", train_df.shape, valid_df.shape)",
                  "    ",
                  "    # test setでする場合",
                  "    train_df, test_df = target_encoder(df.iloc[:700,], df.iloc[700:,], target_col)",
                  "    print(train_df.shape, test_df.shape)",
                ]
              },
              {
                'name': 'CatBoostエンコディング',
                'snippet': [
                  "def catboost_encoder(train_df, valid_df, target_col:str, cat_features=None):",
                  "    \"\"\"",
                  "    CatBoost_Encoding: カテゴリ列を目的変数の1行前の行からのみに変換する特徴量エンジニアリング",
                  "    CatBoost使ったターゲットエンコーディング",
                  "    https://www.kaggle.com/matleonard/categorical-encodings",
                  "    \"\"\"",
                  "    # conda install -c conda-forge category_encoders",
                  "    import category_encoders as ce",
                  "",
                  "    if cat_features is None:",
                  "        cat_features = train_df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.to_list()",
                  "    ",
                  "    cb_enc = ce.CatBoostEncoder(cols=cat_features, random_state=7)",
                  "",
                  "    # trainだけでfitすること(validationやtest含めるとリークする)",
                  "    cb_enc.fit(train_df[cat_features], train_df[target_col])",
                  "",
                  "    train_encoded = train_df.join(cb_enc.transform(train_df[cat_features]).add_suffix(\"_cb\"))",
                  "    valid_encoded = valid_df.join(cb_enc.transform(valid_df[cat_features]).add_suffix(\"_cb\"))",
                  "    return train_encoded, valid_encoded",
                  "",
                  "",
                  "if __name__ == '__main__':",
                  "    # サンプルデータ",
                  "    import seaborn as sns",
                  "    df = sns.load_dataset(\"titanic\")",
                  "    df = df.drop([\"alive\"], axis=1)",
                  "    target_col = \"survived\"",
                  "    feats = df.columns.to_list()",
                  "    feats.remove(target_col)",
                  "    print(df.shape)",
                  "    ",
                  "    import pandas as pd",
                  "    from tqdm import tqdm",
                  "    from sklearn.model_selection import *",
                  "    # KFoldで実行する場合",
                  "    folds = KFold(n_splits=4, shuffle=True, random_state=1001)",
                  "    for n_fold, (train_idx, valid_idx) in tqdm(enumerate(folds.split(df[feats], df[target_col]))):",
                  "        train_x, train_y = (df[feats].iloc[train_idx], df[target_col].iloc[train_idx],)",
                  "        valid_x, valid_y = (df[feats].iloc[valid_idx], df[target_col].iloc[valid_idx],)",
                  "        train_df = pd.concat([train_x, train_y], axis=1)",
                  "        valid_df = pd.concat([valid_x, valid_y], axis=1)",
                  "        ",
                  "        # CatBoost_Encoding",
                  "        train_df, valid_df = catboost_encoder(train_df, valid_df, target_col)",
                  "        print(f\"fold: {n_fold}:\", train_df.shape, valid_df.shape)",
                  "    ",
                  "    # test setでする場合",
                  "    train_df, test_df = catboost_encoder(df.iloc[:700,], df.iloc[700:,], target_col)",
                  "    print(train_df.shape, test_df.shape)",
                ]
              },
              {
                'name': 'カウント,ターゲット,CatBoost,ラベルエンコディング一気にやる',
                'snippet': [
                  "import numpy as np",
                  "import pandas as pd",
                  "from tqdm import tqdm",
                  "from sklearn.model_selection import *",
                  "",
                  "class Encoder():",
                  "    def __init__(self, ",
                  "                 encoder_flags={\"count\": True, ",
                  "                                \"target\": True, ",
                  "                                \"catboost\": True, ",
                  "                                \"label\": True, ",
                  "                                \"impute_null\": True}) -> None:",
                  "        self.encoder_flags = encoder_flags",
                  "    ",
                  "    @staticmethod",
                  "    def count_encoder(train_df, valid_df, cat_features=None):",
                  "        \"\"\"",
                  "        Count_Encoding: カテゴリ列をカウント値に変換する特徴量エンジニアリング（要はgroupby().size()の集計列追加のこと）",
                  "        ※カウント数が同じカテゴリは同じようなデータ傾向になる可能性がある",
                  "        https://www.kaggle.com/matleonard/categorical-encodings",
                  "        \"\"\"",
                  "        # conda install -c conda-forge category_encoders",
                  "        import category_encoders as ce",
                  "",
                  "        if cat_features is None:",
                  "            cat_features = train_df.select_dtypes(",
                  "                include=[\"object\", \"category\", \"bool\"]",
                  "            ).columns.to_list()",
                  "",
                  "        count_enc = ce.CountEncoder(cols=cat_features)",
                  "",
                  "        # trainだけでfitすること(validationやtest含めるとリークする)",
                  "        count_enc.fit(train_df[cat_features])",
                  "        train_encoded = train_df.join(",
                  "            count_enc.transform(train_df[cat_features]).add_suffix(\"_count\")",
                  "        )",
                  "        valid_encoded = valid_df.join(",
                  "            count_enc.transform(valid_df[cat_features]).add_suffix(\"_count\")",
                  "        )",
                  "",
                  "        return train_encoded, valid_encoded",
                  "",
                  "    @staticmethod",
                  "    def target_encoder(train_df, valid_df, target_col: str, cat_features=None):",
                  "        \"\"\"",
                  "        Target_Encoding: カテゴリ列を目的変数の平均値に変換する特徴量エンジニアリング",
                  "        https://www.kaggle.com/matleonard/categorical-encodings",
                  "        \"\"\"",
                  "        # conda install -c conda-forge category_encoders",
                  "        import category_encoders as ce",
                  "",
                  "        if cat_features is None:",
                  "            cat_features = train_df.select_dtypes(",
                  "                include=[\"object\", \"category\", \"bool\"]",
                  "            ).columns.to_list()",
                  "",
                  "        target_enc = ce.TargetEncoder(cols=cat_features)",
                  "",
                  "        # trainだけでfitすること(validationやtest含めるとリークする)",
                  "        target_enc.fit(train_df[cat_features], train_df[target_col])",
                  "",
                  "        train_encoded = train_df.join(",
                  "            target_enc.transform(train_df[cat_features]).add_suffix(\"_target\")",
                  "        )",
                  "        valid_encoded = valid_df.join(",
                  "            target_enc.transform(valid_df[cat_features]).add_suffix(\"_target\")",
                  "        )",
                  "        return train_encoded, valid_encoded",
                  "",
                  "    @staticmethod",
                  "    def catboost_encoder(train_df, valid_df, target_col: str, cat_features=None):",
                  "        \"\"\"",
                  "        CatBoost_Encoding: カテゴリ列を目的変数の1行前の行からのみに変換する特徴量エンジニアリング",
                  "        CatBoost使ったターゲットエンコーディング",
                  "        https://www.kaggle.com/matleonard/categorical-encodings",
                  "        \"\"\"",
                  "        # conda install -c conda-forge category_encoders",
                  "        import category_encoders as ce",
                  "",
                  "        if cat_features is None:",
                  "            cat_features = train_df.select_dtypes(",
                  "                include=[\"object\", \"category\", \"bool\"]",
                  "            ).columns.to_list()",
                  "",
                  "        cb_enc = ce.CatBoostEncoder(cols=cat_features, random_state=7)",
                  "",
                  "        # trainだけでfitすること(validationやtest含めるとリークする)",
                  "        cb_enc.fit(train_df[cat_features], train_df[target_col])",
                  "",
                  "        train_encoded = train_df.join(",
                  "            cb_enc.transform(train_df[cat_features]).add_suffix(\"_cb\")",
                  "        )",
                  "        valid_encoded = valid_df.join(",
                  "            cb_enc.transform(valid_df[cat_features]).add_suffix(\"_cb\")",
                  "        )",
                  "        return train_encoded, valid_encoded",
                  "",
                  "    @staticmethod",
                  "    def impute_null_add_flag_col(df, strategy=\"median\", cols_with_missing=None, fill_value=None):",
                  "        \"\"\"欠損値を補間して欠損フラグ列を追加する",
                  "        fill_value はstrategy=\"constant\"の時のみ有効になる補間する定数",
                  "        \"\"\"",
                  "        from sklearn.impute import SimpleImputer",
                  "",
                  "        df_plus = df.copy()",
                  "",
                  "        if cols_with_missing is None:",
                  "            if strategy in [\"median\", \"median\"]:",
                  "                # 数値列で欠損ある列探す",
                  "                cols_with_missing = [col for col in df.columns if (df[col].isnull().any()) and (df[col].dtype.name not in [\"object\", \"category\", \"bool\"])]",
                  "            else:",
                  "                # 欠損ある列探す",
                  "                cols_with_missing = [col for col in df.columns if (df[col].isnull().any())]",
                  "",
                  "        for col in cols_with_missing:",
                  "            # 欠損フラグ列を追加",
                  "            df_plus[col + \"_was_missing\"] = df[col].isnull()",
                  "            df_plus[col + \"_was_missing\"] = df_plus[col + \"_was_missing\"].astype(int)",
                  "            # 欠損値を平均値で補間",
                  "            my_imputer = SimpleImputer(strategy=strategy, fill_value=fill_value)",
                  "            df_plus[col] = my_imputer.fit_transform(df[cols_with_missing])",
                  "",
                  "        return df_plus",
                  "    ",
                  "    def run_encoders(self, X, y, train_index, val_index):",
                  "        \"\"\"カウント,ターゲット,CatBoost,ラベルエンコディング一気にやる。cvの処理のforの中に書きやすいように\"\"\"",
                  "        train_df = pd.concat([X, y], axis=1)",
                  "        t_fold_df, v_fold_df = train_df.iloc[train_index], train_df.iloc[val_index]",
                  "",
                  "        if self.encoder_flags[\"count\"]:",
                  "            # カウントエンコディング",
                  "            t_fold_df, v_fold_df = Encoder().count_encoder(t_fold_df, v_fold_df, cat_features=None)",
                  "        if self.encoder_flags[\"target\"]:",
                  "            # ターゲットエンコディング",
                  "            t_fold_df, v_fold_df = Encoder().target_encoder(t_fold_df, v_fold_df, target_col=target_col, cat_features=None)",
                  "        if self.encoder_flags[\"catboost\"]:",
                  "            # CatBoostエンコディング",
                  "            t_fold_df, v_fold_df = Encoder().catboost_encoder(t_fold_df, v_fold_df, target_col=target_col, cat_features=None)",
                  "        ",
                  "        if self.encoder_flags[\"label\"]:",
                  "            # ラベルエンコディング",
                  "            train_df = t_fold_df.append(v_fold_df)  # trainとval再連結",
                  "            cate_cols = t_fold_df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.to_list()",
                  "            for col in cate_cols:",
                  "                train_df[col], uni = pd.factorize(train_df[col])",
                  "                ",
                  "        if self.encoder_flags[\"impute_null\"]:",
                  "            # 欠損置換（ラベルエンコディングの後じゃないと処理遅くなる）",
                  "            nulls = train_df.drop(y.name, axis=1).isnull().sum().to_frame()",
                  "            null_indexs = [index for index, row in nulls.iterrows() if row[0] > 0]",
                  "            train_df = Encoder().impute_null_add_flag_col(train_df, cols_with_missing=null_indexs, strategy=\"most_frequent\")  # 最頻値で補間",
                  "        ",
                  "        t_fold_df, v_fold_df = train_df.iloc[train_index], train_df.iloc[val_index]",
                  "        print(",
                  "            \"run encoding Train shape: {}, valid shape: {}\".format(",
                  "                t_fold_df.shape, v_fold_df.shape",
                  "            )",
                  "        )",
                  "        feats = t_fold_df.columns.to_list()",
                  "        feats.remove(target_col)",
                  "        X_train, y_train = (t_fold_df[feats], t_fold_df[target_col])",
                  "        X_val, y_val = (v_fold_df[feats], v_fold_df[target_col])",
                  "        return X_train, y_train, X_val, y_val",
                  "    ",
                  "",
                  "if __name__ == '__main__':",
                  "    # サンプルデータ",
                  "    import seaborn as sns",
                  "    df = sns.load_dataset(\"titanic\")",
                  "    df = df.drop([\"alive\"], axis=1)",
                  "    target_col = \"survived\"",
                  "    feats = df.columns.to_list()",
                  "    feats.remove(target_col)",
                  "    print(df.shape)",
                  "    ",
                  "    # KFoldで実行する場合",
                  "    folds = KFold(n_splits=4, shuffle=True, random_state=1001)",
                  "    for n_fold, (train_idx, valid_idx) in tqdm(enumerate(folds.split(df[feats], df[target_col]))):",
                  "        # カウント,ターゲット,CatBoost,ラベルエンコディング一気にやる",
                  "        X_train, y_train, X_val, y_val = Encoder().run_encoders(df[feats], df[target_col], train_idx, valid_idx)",
                  "        train_df = pd.concat([X_train, y_train], axis=1)",
                  "        valid_df = pd.concat([X_val, y_val], axis=1)",
                  "        print(f\"fold: {n_fold}:\", train_df.shape, valid_df.shape)",
                  "    ",
                  "    # test setでする場合",
                  "    X_train, y_train, X_test, y_test = Encoder().run_encoders(df[feats], df[target_col], list(range(700)), list(range(700, df.shape[0])))",
                  "    train_df = pd.concat([X_train, y_train], axis=1)",
                  "    test_df = pd.concat([X_test, y_test], axis=1)",
                  "    print(train_df.shape, test_df.shape)",
                ]
              },
            ],
          },
          {
            'name': '5.特徴量の作成',
            'sub-menu': [
              {
                'name': '標準化',
                'snippet': [
                  "from sklearn.preprocessing import StandardScaler",
                  "",
                  "num_cols = df.select_dtypes(",
                  "    include=[\"float\", \"float32\", \"float64\"]",
                  ").columns.to_list()",
                  "",
                  "scaler = StandardScaler()",
                  "scaler.fit(df[num_cols])",
                  "",
                  "df[num_cols] = scaler.transform(df[num_cols])",
                  "#df_test[num_cols] = scaler.transform(df_test[num_cols])",
                  "df",
                ]
              },
              {
                'name': 'ビン化',
                'snippet': [
                  "def bin(df, num_col, bins=10):",
                  "    df[f\"{num_col}_bin{str(bins)}\"] = pd.cut(df[num_col], bins=bins, labels=False)",
                  "    return df",
                  "",
                  "df = bin(df, \"age\", bins=3)",
                  "df = bin(df, \"fare\", bins=8)",
                ]
              },
              {
                'name': 'クリッピング',
                'snippet': [
                  "def clipping(df, num_cols, min_clip=0.01, max_clip=0.99,):",
                  "    p_min = df[num_cols].quantile(min_clip)",
                  "    p_max = df[num_cols].quantile(max_clip)",
                  "    print(f\"### {min_clip * 100}% clip ###\\n{p_min}\")",
                  "    print(f\"### {max_clip * 100}% clip ###\\n{p_max}\")",
                  "    # 1％点以下の値は1％点に、99％点以上の値は99％点にclippingする",
                  "    df[num_cols] = df[num_cols].clip(p_min, p_max, axis=1)",
                  "    return df",
                  "",
                  "df = clipping_cols(df, [\"age\", \"fare\"])",
                ]
              },
              {
                'name': '順位に変換した列追加',
                'snippet': [
                  "def add_rank(df, cols):",
                  "    \"\"\"pandasのrank関数で順位に変換して列追加\"\"\"",
                  "    rank = df[cols].rank()",
                  "    rank.columns = [f\"{col}_rank\" for col in cols]",
                  "    return pd.concat([df, rank], axis=1)",
                  "    ",
                  "import pandas as pd",
                  "import seaborn as sns",
                  "df = sns.load_dataset(\"titanic\")",
                  "df = df.drop([\"alive\"], axis=1)",
                  "add_rank(df, [\"age\", \"fare\"])",
                ]
              },
              {
                'name': 'ランクガウスに変換',
                'snippet': [
                  "def rank_gauss(train_x, num_cols=None):",
                  "    \"\"\"指定列にランクガウス（数値を順位に変換した後無理やり正規分布にする変換）適用する\"\"\"",
                  "    from sklearn.preprocessing import QuantileTransformer",
                  "    ",
                  "    # 数値列のみ取り出す",
                  "    num_cols = num_cols if num_cols is not None else train_x.select_dtypes(include=[\"int\", \"int32\", \"int64\", \"float\", \"float32\", \"float64\"]).columns.to_list()",
                  "    ",
                  "    # 学習データに基づいて複数列のRankGaussによる変換を定義",
                  "    transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution='normal')",
                  "    transformer.fit(train_x[num_cols])",
                  "",
                  "    # 変換後のデータで各列を置換",
                  "    train_x[num_cols] = transformer.transform(train_x[num_cols])",
                  "    # test_x[num_cols] = transformer.transform(test_x[num_cols])",
                  "",
                  "    return train_x",
                  "        ",
                  "if __name__ == '__main__':",
                  "    import pandas as pd",
                  "    import seaborn as sns",
                  "    df = sns.load_dataset(\"titanic\")",
                  "    df = df.drop([\"alive\"], axis=1)",
                  "    for col in [\"sex\", \"embarked\", \"who\", \"embark_town\", \"class\", \"adult_male\", \"alone\", \"deck\"]:",
                  "        df[col], uni = pd.factorize(df[col])",
                  "    train_x = df.drop(\"survived\", axis=1)",
                  "    ",
                  "    train_x = rank_gauss(train_x)",
                  "    train_x[\"age\"].plot.hist()",
                ]
              },
              {
                'name': 'パーセント列(比率)追加',
                'snippet': [
                  "num_cols = df.select_dtypes(include=[\"float\", \"float32\", \"float64\"]).columns.to_list()",
                  "for col in num_cols:",
                  "    df[f\"{col}_perc\"] = df[col] / df[col].max() # df[\"tmp_max\"]",
                  "",
                  "df",
                ]
              },
              {
                'name': '平均からの差列追加',
                'snippet': [
                  "num_cols = df.select_dtypes(include=[\"float\", \"float32\", \"float64\"]).columns.to_list()",
                  "for col in num_cols:",
                  "    df[f\"{col}_diff_mean\"] = df[col] / df[col].mean() # df[\"tmp_max\"]",
                  "",
                  "df",
                ]
              },
              {
                'name': '対数列追加',
                'snippet': [
                  "def log1p(df, num_cols):",
                  "    for col in num_cols:",
                  "        df[f\"{col}_log1p\"] = np.log(df[col] + 1)",
                  "    return df",
                  "",
                  "df = log1p(df, [\"age\", \"fare\"])",
                ]
              },
              {
                'name': '歪度>0.75のカラムを対数変換',
                'snippet': [
                  "def skew_log1p(df, num_cols=None):",
                  "    \"\"\"歪度>0.75のカラムを対数変換\"\"\"",
                  "    import numpy as np",
                  "    from scipy.stats import skew",
                  "    ",
                  "    if num_cols is None:",
                  "        num_cols = df.select_dtypes(include=[\"float\", \"float32\", \"float64\"]).columns.to_list()  # 列指定なければfloat列にだけ行う",
                  "    ",
                  "    skewd_cols = df[num_cols].apply(lambda x: skew(x.dropna()))  # 欠損値除いて歪度求める",
                  "    skewd_cols = skewd_cols[skewd_cols > 0.75].index",
                  "    print(f\"[INFO] skew>0.75 cols:\\n {skewd_cols}\")",
                  "    df[skewd_cols] = np.log1p(df[skewd_cols])",
                  "    return df",
                  "",
                  "_df = skew_log1p(df.head(1000))",
                ]
              },
              {
                'name': 'sin_cos列追加',
                'snippet': [
                  "def sin_cos(df, num_cols):",
                  "    for col in num_cols:",
                  "        df[col + '_cos'] = np.cos(2 * np.pi * df[col] / df[col].max())",
                  "        df[col + '_sin'] = np.sin(2 * np.pi * df[col] / df[col].max())",
                  "    return df",
                  "",
                  "df = sin_cos(df, [\"age\", \"fare\"])",
                ]
              },
              {
                'name': '文字型の列を連結した列追加',
                'snippet': [
                  "import itertools",
                  "from sklearn import preprocessing",
                  "",
                  "import pandas as pd",
                  "import seaborn as sns",
                  "df = sns.load_dataset(\"titanic\")",
                  "df = df.drop([\"alive\"], axis=1)",
                  "cat_features = df.select_dtypes(include=[\"object\"]).columns.to_list()",
                  "",
                  "df[\"new_feat\"] = (df[\"class\"].astype(str) + \"_\" + df[\"sex\"].astype(str))",
                  "",
                  "# 文字列を2列結合してラベルエンコディング",
                  "for col1, col2 in itertools.combinations(cat_features, 2):",
                  "    new_col_name = '_'.join([col1, col2])",
                  "    new_values = df[col1].map(str) + \"_\" + df[col2].map(str)",
                  "    encoder = preprocessing.LabelEncoder()",
                  "    df[new_col_name] = encoder.fit_transform(new_values)",
                  "",
                  "def add_cates(df, cols, new_col_name):",
                  "    \"\"\"複数列一気に連結\"\"\"",
                  "    df[cols] = df[cols].astype(str)",
                  "    df[new_col_name] = df[cols[0]].str.cat(df[cols[1:]], sep='_')",
                  "    return df",
                  "",
                  "df",
                ]
              },
              {
                'name': '行単位で統計量列追加',
                'snippet': [
                  "# 行単位で統計量とる",
                  "def add_num_row_agg(df_all, agg_num_cols):",
                  "    \"\"\"行単位の統計量列追加",
                  "    agg_num_cols は数値列だけでないとエラー\"\"\"",
                  "    import warnings",
                  "",
                  "    warnings.filterwarnings(\"ignore\")",
                  "",
                  "    df = df_all[agg_num_cols]",
                  "    cols = df.columns.to_list()",
                  "    cols = map(str, cols)  # 文字列にする",
                  "    col_name = \"_\".join(cols)",
                  "",
                  "    df_all[f\"{col_name}_sum\"] = df.sum(axis=1)",
                  "    df_all[f\"{col_name}_mean\"] = df.mean(axis=1)",
                  "    df_all[f\"{col_name}_median\"] = df.median(axis=1)",
                  "    df_all[f\"{col_name}_var\"] = df.var(axis=1)",
                  "    df_all[f\"{col_name}_std\"] = df.std(axis=1)",
                  "    df_all[f\"{col_name}_sem\"] = df.sem(axis=1)  # 標準誤差",
                  "    df_all[f\"{col_name}_mad\"] = df.mad(axis=1)  # 中央値の絶対偏差",
                  "    df_all[f\"{col_name}_skew\"] = df.skew(axis=1)  # 歪度",
                  "    df_all[f\"{col_name}_ratio_range\"] = df.max(axis=1) / df.min(axis=1)  # 最大/最小",
                  "    df_all[f\"{col_name}_mean_var\"] = df.std(axis=1) / df.mean(axis=1)  # 平均分散",
                  "    df_all[f\"{col_name}_percentile_75\"] = df.apply(",
                  "        lambda x: np.percentile(x, 75), axis=1",
                  "    )  # 75パーセンタイル",
                  "    df_all[f\"{col_name}_percentile_25\"] = df.apply(",
                  "        lambda x: np.percentile(x, 25), axis=1",
                  "    )  # 25パーセンタイル",
                  "    df_all[f\"{col_name}_diff_percentile_75-25\"] = df.apply(",
                  "        lambda x: np.percentile(x, 75) - np.percentile(x, 25), axis=1",
                  "    )  # 75,25パーセンタイルの差",
                  "    df_all[f\"{col_name}_ratio_percentile_75-25\"] = df.apply(",
                  "        lambda x: np.percentile(x, 75) / np.percentile(x, 25), axis=1",
                  "    )  # 75,25パーセンタイルの比",
                  "    df_all[f\"{col_name}_ptp\"] = df.apply(",
                  "        lambda x: np.ptp(x), axis=1",
                  "    )  # peak to peak: 最大値と最小値との差",
                  "    df_all[f\"{col_name}_kurt\"] = df.apply(lambda x: pd.DataFrame.kurt(x), axis=1)  # 尖度",
                  "",
                  "    def _hl_ratio(x):",
                  "        \"\"\"平均より高いサンプル数と低いサンプル数の比率\"\"\"",
                  "        x = x.dropna()",
                  "        n_high = x[x >= np.mean(x)].shape[0]",
                  "        n_low = x[x < np.mean(x)].shape[0]",
                  "        return 1.0 if n_low == 0 else n_high / n_low",
                  "    df_all[f\"{col_name}_hl_ratio\"] = df.apply(",
                  "        _hl_ratio, axis=1",
                  "    )  # 平均より高いサンプル数と低いサンプル数の比率",
                  "",
                  "    def _beyond1std(x):",
                  "        \"\"\"1stdを超える比率\"\"\"",
                  "        x = x.dropna()",
                  "        return (",
                  "            1.0",
                  "            if x.shape[0] == 0",
                  "            else x[np.abs(x) > np.abs(np.std(x))].shape[0] / x.shape[0]",
                  "        )",
                  "    df_all[f\"{col_name}_beyond1std\"] = df.apply(_beyond1std, axis=1)  # 1stdを超える比率",
                  "",
                  "    return df_all",
                  "",
                  "",
                  "def add_cate_row_agg(df_all, agg_cate_cols):",
                  "    \"\"\"行単位の統計量列追加",
                  "    agg_cate_cols は文字列でないとエラー\"\"\"",
                  "    from scipy.stats import entropy",
                  "    import warnings",
                  "",
                  "    warnings.filterwarnings(\"ignore\")",
                  "",
                  "    df = df_all[agg_cate_cols]",
                  "    cols = df.columns.to_list()",
                  "    cols = map(str, cols)  # 文字列にする",
                  "    col_name = \"_\".join(cols)",
                  "",
                  "    df_all[f\"{col_name}_freq_entropy\"] = df.apply(lambda x: entropy(x.value_counts().values), axis=1)  # 出現頻度のエントロピー",
                  "    ",
                  "    def _freq1name(x):",
                  "        x = x.dropna()",
                  "        return np.nan if x.shape[0] == 0 else x.value_counts().sort_values(ascending=False)[0] ",
                  "    df_all[f\"{col_name}_freq1name\"] = df.apply(_freq1name, axis=1)  # 最も頻繁に出現するカテゴリの数",
                  "",
                  "    def _freq1ratio(x):",
                  "        x = x.dropna()",
                  "        frq = x.value_counts().sort_values(ascending=False)",
                  "        return np.nan if frq.shape[0] == 0 else frq[0] / frq.shape[0] ",
                  "    df_all[f\"{col_name}_freq1ratio\"] = df.apply(_freq1ratio, axis=1)  # 最も頻繁に出現するカテゴリ/グループの数",
                  "    ",
                  "    return df_all",
                  "",
                  "",
                  "import pandas as pd",
                  "import seaborn as sns",
                  "df = sns.load_dataset(\"titanic\")",
                  "df_all = df.drop([\"alive\"], axis=1)",
                  "",
                  "agg_num_cols = df_all.select_dtypes(include=[\"int\", \"int32\", \"int64\", \"float\", \"float32\", \"float64\"]).columns.to_list()",
                  "df_all = add_num_row_agg(df_all, agg_num_cols)",
                  "",
                  "agg_cate_cols = df_all.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.to_list()",
                  "df_all = add_cate_row_agg(df_all, agg_cate_cols)",
                  "",
                  "display(df_all)",
                ]
              },
              {
                'name': 'groupbyで統計量列追加',
                'snippet': [
                  "# A列でグループして集計したB列は意味がありそうと仮説たててから統計値列作ること",
                  "# 目的変数をキーにして集計するとリークしたターゲットエンコーディングになるため説明変数同士で行うこと",
                  "def grouping(df, cols, agg_dict, prefix=\"\"):",
                  "    \"\"\"特定のカラムについてgroup化された特徴量の作成を行う",
                  "    Args:",
                  "        df (pd.DataFrame): 特徴量作成のもととなるdataframe",
                  "        cols (str or list): group by処理のkeyとなるカラム (listで複数指定可能)",
                  "        agg_dict (dict): 特徴量作成を行いたいカラム/集計方法を指定するdictionary",
                  "        prefix (str): 集約後のカラムに付与するprefix name",
                  "",
                  "    Returns:",
                  "        df (pd.DataFrame): 特定のカラムについてgroup化された特徴量群",
                  "    \"\"\"",
                  "    group_df = df.groupby(cols).agg(agg_dict)",
                  "    group_df.columns = [prefix + c[0] + \"_\" + c[1] for c in list(group_df.columns)]",
                  "    group_df.reset_index(inplace=True)",
                  "",
                  "    return group_df",
                  "",
                  "class AggUtil():",
                  "    ############## カテゴリ列 vs. 数値列について ##############",
                  "    @staticmethod",
                  "    def percentile(n):",
                  "        \"\"\"パーセンタイル\"\"\"",
                  "        def percentile_(x):",
                  "            return np.percentile(x, n)",
                  "        percentile_.__name__ = \"percentile_%s\" % n",
                  "        return percentile_",
                  "",
                  "    @staticmethod",
                  "    def diff_percentile(n1, n2):",
                  "        \"\"\"パーセンタイルの差\"\"\"",
                  "        def diff_percentile_(x):",
                  "            p1 = np.percentile(x, n1)",
                  "            p2 = np.percentile(x, n2)",
                  "            return p1 - p2",
                  "        diff_percentile_.__name__ = f\"diff_percentile_{n1}-{n2}\"",
                  "        return diff_percentile_",
                  "",
                  "    @staticmethod",
                  "    def ratio_percentile(n1, n2):",
                  "        \"\"\"パーセンタイルの比\"\"\"",
                  "        def ratio_percentile_(x):",
                  "            p1 = np.percentile(x, n1)",
                  "            p2 = np.percentile(x, n2)",
                  "            return p1 / p2",
                  "        ratio_percentile_.__name__ = f\"ratio_percentile_{n1}-{n2}\"",
                  "        return ratio_percentile_",
                  "    ",
                  "    @staticmethod",
                  "    def mean_var():",
                  "        \"\"\"平均分散\"\"\"",
                  "        def mean_var_(x):",
                  "            x = x.dropna()",
                  "            return np.std(x) / np.mean(x)",
                  "        mean_var_.__name__ = f\"mean_var\"",
                  "        return mean_var_",
                  "    ",
                  "    @staticmethod",
                  "    def diff_mean():",
                  "        \"\"\"平均との差の中央値(aggは集計値でないとエラーになるから中央値をとる)\"\"\"",
                  "        def diff_mean_(x):",
                  "            x = x.dropna()",
                  "            return np.median(x - np.mean(x))",
                  "        diff_mean_.__name__ = f\"diff_mean\"",
                  "        return diff_mean_",
                  "    ",
                  "    @staticmethod",
                  "    def ratio_mean():",
                  "        \"\"\"平均との比の中央値(aggは一意な値でないとエラーになるから中央値をとる)\"\"\"",
                  "        def ratio_mean_(x):",
                  "            x = x.dropna()",
                  "            return np.median(x / np.mean(x))",
                  "        ratio_mean_.__name__ = f\"ratio_mean\"",
                  "        return ratio_mean_",
                  "    ",
                  "    @staticmethod",
                  "    def hl_ratio():",
                  "        \"\"\"平均より高いサンプル数と低いサンプル数の比率\"\"\"",
                  "        def hl_ratio_(x):",
                  "            x = x.dropna()",
                  "            n_high = x[x >= np.mean(x)].shape[0]",
                  "            n_low = x[x < np.mean(x)].shape[0]",
                  "            if n_low == 0:",
                  "                return 1.0",
                  "            else:",
                  "                return n_high / n_low",
                  "        hl_ratio_.__name__ = f\"hl_ratio\"",
                  "        return hl_ratio_",
                  "    ",
                  "    @staticmethod",
                  "    def ratio_range():",
                  "        \"\"\"最大/最小\"\"\"",
                  "        def ratio_range_(x):",
                  "            x = x.dropna()",
                  "            if np.min(x) == 0:",
                  "                return 1.0",
                  "            else:",
                  "                return np.max(x) / np.min(x)",
                  "        ratio_range_.__name__ = f\"ratio_range\"",
                  "        return ratio_range_",
                  "    ",
                  "    @staticmethod",
                  "    def beyond1std():",
                  "        \"\"\"1stdを超える比率\"\"\"",
                  "        def beyond1std_(x):",
                  "            x = x.dropna()",
                  "            return x[np.abs(x) > np.abs(np.std(x))].shape[0] / x.shape[0]",
                  "        beyond1std_.__name__ = \"beyond1std\"",
                  "        return beyond1std_",
                  "    ",
                  "    @staticmethod",
                  "    def zscore():",
                  "        \"\"\"Zスコアの中央値(aggは一意な値でないとエラーになるから中央値をとる)\"\"\"",
                  "        def zscore_(x):",
                  "            x = x.dropna()",
                  "            return np.median((x - np.mean(x)) / np.std(x))",
                  "        zscore_.__name__ = \"zscore\"",
                  "        return zscore_",
                  "    ######################################################",
                  "    ",
                  "    ############## カテゴリ列 vs. カテゴリ列について ##############",
                  "    @staticmethod",
                  "    def freq_entropy():",
                  "        \"\"\"出現頻度のエントロピー\"\"\"",
                  "        from scipy.stats import entropy",
                  "        def freq_entropy_(x):",
                  "            return entropy(x.value_counts().values)",
                  "        freq_entropy_.__name__ = \"freq_entropy\"",
                  "        return freq_entropy_",
                  "    ",
                  "    @staticmethod",
                  "    def freq1name():",
                  "        \"\"\"最も頻繁に出現するカテゴリの数\"\"\"",
                  "        def freq1name_(x):",
                  "            return x.value_counts().sort_values(ascending=False)[0]",
                  "        freq1name_.__name__ = \"freq1name\"",
                  "        return freq1name_",
                  "    ",
                  "    @staticmethod",
                  "    def freq1ratio():",
                  "        \"\"\"最も頻繁に出現するカテゴリ/グループの数\"\"\"",
                  "        def freq1ratio_(x):",
                  "            frq = x.value_counts().sort_values(ascending=False)",
                  "            return frq[0] / frq.shape[0]",
                  "        freq1ratio_.__name__ = \"freq1ratio\"",
                  "        return freq1ratio_",
                  "    #########################################################",
                  "",
                  "",
                  "if __name__ == '__main__':",
                  "    import seaborn as sns",
                  "    df = sns.load_dataset(\"titanic\")",
                  "    # df = pd.DataFrame({\"a\": [1, 2, 3, np.nan, 5], \"b\": [np.nan, 7, 8, 9, 0], \"c\": [\"a\", \"b\", np.nan, \"a\", \"b\"], \"d\": [\"c\", \"c\", np.nan, \"d\", \"d\"], \"e\": [\"f\", \"g\", np.nan, \"i\", \"j\"],})",
                  "    ",
                  "    import numpy as np",
                  "    import pandas as pd",
                  "    ",
                  "    # 集計する数値列指定",
                  "    value_agg = {",
                  "        \"age\": [",
                  "            \"max\",",
                  "            \"min\",",
                  "            \"sum\",",
                  "            \"mean\",",
                  "            \"median\",",
                  "            \"prod\",  # 積",
                  "            \"var\",  # 分散",
                  "            \"std\",  # 標準偏差",
                  "            \"sem\",  # 標準誤差",
                  "            \"mad\",  # 中央値の絶対偏差",
                  "            \"skew\",  # 歪度",
                  "            pd.DataFrame.kurt,  # 尖度",
                  "            np.ptp,  # peak to peak: 最大値と最小値との差",
                  "            AggUtil().percentile(25),  # 25パーセンタイル",
                  "            AggUtil().percentile(75),  # 75パーセンタイル",
                  "            AggUtil().diff_percentile(75, 25),  # パーセンタイルの差",
                  "            AggUtil().mean_var(),  # 平均分散",
                  "            AggUtil().diff_mean(),  # 平均との差の中央値",
                  "            AggUtil().ratio_mean(),  # 平均との比の中央値",
                  "            AggUtil().ratio_range(),  # 最大/最小",
                  "            AggUtil().hl_ratio(),  # 平均より高いサンプル数と低いサンプル数の比率",
                  "            AggUtil().beyond1std(),  # 1stdを超える比率",
                  "            AggUtil().zscore(),  # Zスコアの中央値",
                  "        ],",
                  "        \"sex\": [",
                  "           \"count\",  # カウント数",
                  "            AggUtil().freq_entropy(),  # 出現頻度のエントロピー",
                  "            AggUtil().freq1name(),  # 最も頻繁に出現するカテゴリの数",
                  "            AggUtil().freq1ratio(),  # 最も頻繁に出現するカテゴリ/グループの数",
                  "        ],        ",
                  "    }",
                  "    # グループ化するカテゴリ列でループ",
                  "    for key in [\"pclass\", \"deck\"]:",
                  "        feature_df = grouping(df, key, value_agg, prefix=key + \"_\")",
                  "        df = pd.merge(df, feature_df, how=\"left\", on=key)",
                  "    pd.set_option('display.max_columns', None)",
                  "    display(df)",
                ]
              },
              {
                'name': '算術列追加',
                'snippet': [
                  "import sys",
                  "",
                  "sys.path.append(r\"C:\\Users\\81908\\Git\\xfeat\")",
                  "import xfeat",
                  "from xfeat import (",
                  "    Pipeline,",
                  "    SelectNumerical,",
                  "    ArithmeticCombinations,",
                  ")",
                  "",
                  "",
                  "def arithmetic_num_xfeat(df, operator=\"*\", n_order=2, num_cols=None):",
                  "    \"\"\"",
                  "    xfeatで数値列同士を算術計算",
                  "    operatorは「+,-,*」のいずれか",
                  "    n_orderは次数。2なら2列の組み合わせになる",
                  "    列膨大になるので掛け合わせる列num_colsで指定したほうがいい",
                  "    参考: https://megane-man666.hatenablog.com/entry/xfeat",
                  "    \"\"\"",
                  "    if num_cols is None:",
                  "        df_num = Pipeline([SelectNumerical(),]).fit_transform(df)",
                  "        num_cols = df_num.columns.tolist()",
                  "",
                  "    if operator == \"+\":",
                  "        output_suffix = \"_plus\"",
                  "    elif operator == \"*\":",
                  "        output_suffix = \"_mul\"",
                  "    elif operator == \"-\":",
                  "        output_suffix = \"_minus\"",
                  "",
                  "    df = Pipeline(",
                  "        [",
                  "            ArithmeticCombinations(",
                  "                input_cols=num_cols,",
                  "                drop_origin=False,",
                  "                operator=operator,",
                  "                r=n_order,",
                  "                output_suffix=output_suffix,",
                  "            ),",
                  "        ]",
                  "    ).fit_transform(df)",
                  "    return df",
                  "",
                  "df = arithmetic_num_xfeat(df)",
                ]
              },
              {
                'name': 'kNNの近傍までの距離を特徴量に追加',
                'snippet': [
                  "def add_knn_dist_feat(df, target_col: str, k=1):",
                  "    def knn_feat(X: np.ndarray, y: np.ndarray, k=1) -> np.ndarray:",
                  "        \"\"\"",
                  "        kNNの近傍までの距離を特徴量にする",
                  "        Kaggleのソリューションの1つ",
                  "        Xに欠損あれば距離も欠損になるので注意",
                  "        参考: ",
                  "            https://upura.hatenablog.com/entry/2018/06/23/165855  # 元コードはここからgit cloneした",
                  "            https://blog.amedama.jp/entry/knn-feature-extraction  # 解説がわかりやすい",
                  "        Args:",
                  "            X: 特徴量",
                  "            y: ラベル列",
                  "            k: knnでとる近傍の数。この数でreturnする特徴量の列数きまる",
                  "               k=1なら最近棒1個との距離*クラス数分のデータを返す。k=2ならk=1の列に加えて、第1と第2近傍までの距離の和*クラス数分のデータを返す",
                  "        \"\"\"",
                  "        import sys",
                  "        sys.path.append(r'C:\\Users\\81908\\Git\\knnFeat')",
                  "        from knnFeat import knnExtract",
                  "        # とりあえずcv_fold=5にしておく",
                  "        return knnExtract(X, y, k=k, folds=5)",
                  "    ",
                  "    y = df[target_col].values",
                  "    X = df.drop(target_col, axis=1).values",
                  "    kNNdist = knn_feat(X, y, k=k)",
                  "    kNNdist = pd.DataFrame(kNNdist, columns=[f\"kNNdist_{y_u}_k{i}\" for y_u in np.unique(y) for i in range(k)])",
                  "    return pd.concat([df, kNNdist], axis=1)",
                  "",
                  "if __name__ == '__main__':",
                  "    import numpy as np",
                  "    import matplotlib.pyplot as plt",
                  "    import pandas as pd",
                  "    import seaborn as sns",
                  "    df = sns.load_dataset(\"titanic\")",
                  "    df = df.drop([\"alive\"], axis=1)",
                  "    for col in [\"sex\", \"embarked\", \"who\", \"embark_town\", \"class\", \"adult_male\", \"alone\", \"deck\"]:",
                  "        df[col], uni = pd.factorize(df[col])",
                  "    ",
                  "    df = add_knn_dist_feat(df, \"survived\", k=1)",
                  "    display(df)",
                  "    plt.scatter(df[\"kNNdist_0_k0\"], df[\"kNNdist_1_k0\"], c=df[\"survived\"])  # うまく線形分離できそうなplotなら良い特徴量",
                ]
              },
              {
                'name': 'kNNの近傍点でtarget列平均した特徴量追加',
                'snippet': [
                  "def add_knn_y_mean(df, target_col: str, k=500):",
                  "    def knn_y_mean(X: np.ndarray, y: np.ndarray, k=500) -> np.ndarray:",
                  "        \"\"\"",
                  "        kNNで近傍k点のtargetの平均を特徴量として加える",
                  "        KaggleのHome creditの1位ソリューションの1つ",
                  "        Xに欠損あるとエラーになる",
                  "        Args:",
                  "            X: 特徴量",
                  "            y: ラベル列",
                  "            k: knnでとる近傍の数",
                  "        \"\"\"",
                  "        from sklearn.neighbors import NearestNeighbors",
                  "        ",
                  "        knn_model = NearestNeighbors(n_neighbors=k).fit(X) ",
                  "        y_means = []",
                  "        for x_i in X:",
                  "            dists, idxs = knn_model.kneighbors([x_i])",
                  "            y_means.append(np.mean(y[idxs[0]]))",
                  "        return y_means",
                  "    ",
                  "    y = df[target_col].values",
                  "    X = df.drop(target_col, axis=1).values",
                  "    knn_y_mean = knn_y_mean(X, y, k=k)",
                  "    knn_y_mean = pd.DataFrame(knn_y_mean, columns=[f\"knn_y_mean_k{k}\"])",
                  "    return pd.concat([df, knn_y_mean], axis=1)",
                  "",
                  "if __name__ == '__main__':",
                  "    import pandas as pd",
                  "    import seaborn as sns",
                  "    df = sns.load_dataset(\"titanic\")",
                  "    df = df.drop([\"alive\"], axis=1)",
                  "    for col in [\"sex\", \"embarked\", \"who\", \"embark_town\", \"class\", \"adult_male\", \"alone\", \"deck\"]:",
                  "        df[col], uni = pd.factorize(df[col])",
                  "        ",
                  "    # 欠損列最頻値で補完",
                  "    cols_with_missing = [col for col in df.columns if df[col].isnull().any()]",
                  "    for col in cols_with_missing:",
                  "        column_mode = df[col].mode()[0]",
                  "        df[col].fillna(column_mode, inplace=True)",
                  "    ",
                  "    df = add_knn_y_mean(df, \"survived\", k=500)",
                  "    display(df)",
                ]
              },
              {
                'name': 'DenoisingAutoEncoderで特長量作成',
                'snippet': [
                  "import os",
                  "import random",
                  "import joblib",
                  "import numpy as np",
                  "import pandas as pd",
                  "",
                  "import tensorflow as tf",
                  "from tensorflow.keras.layers import *",
                  "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau",
                  "from tensorflow.keras.models import Model, load_model",
                  "from tensorflow.keras.regularizers import l2",
                  "from sklearn.preprocessing import StandardScaler, MinMaxScaler",
                  "",
                  "",
                  "",
                  "def pre_scale(tr_x, te_x):",
                  "    \"\"\"NNに入れる前に行うデータ前処理のスケーリング\"\"\"",
                  "    def rank_gauss(train_x, num_cols=None):",
                  "        \"\"\"指定列にランクガウス（数値を順位に変換した後無理やり正規分布にする変換）適用する\"\"\"",
                  "        from sklearn.preprocessing import QuantileTransformer",
                  "",
                  "        # 数値列のみ取り出す",
                  "        num_cols = num_cols if num_cols is not None else train_x.select_dtypes(include=[\"int\", \"int32\", \"int64\", \"float\", \"float32\", \"float64\"]).columns.to_list()",
                  "",
                  "        # 学習データに基づいて複数列のRankGaussによる変換を定義",
                  "        transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution='normal')",
                  "        transformer.fit(train_x[num_cols])",
                  "",
                  "        return transformer",
                  "    ",
                  "    # ランクガウスでスケーリング",
                  "    scaler = rank_gauss(tr_x)",
                  "    scaler.fit(tr_x)",
                  "    tr_x = scaler.transform(tr_x)",
                  "    te_x = scaler.transform(te_x)",
                  "    #print(tr_x, te_x)",
                  "    ",
                  "    # 0-1も範囲にしたいのでMinMaxScalerもかける",
                  "    scaler = MinMaxScaler()",
                  "    scaler.fit(tr_x)",
                  "    tr_x = scaler.transform(tr_x)",
                  "    te_x = scaler.transform(te_x)",
                  "    #print(tr_x, te_x)",
                  "    ",
                  "    return tr_x, te_x",
                  "    ",
                  "def swap_noise(array, pb=0.15):",
                  "    \"\"\"swap noise",
                  "    ランダムに10%程度の確率で、\"同じ列\"で\"他の行\"と入れ替えてノイズを作る",
                  "    https://github.com/GINK03/kaggle-dae",
                  "    https://speakerdeck.com/hoxomaxwell/home-credit-default-risk-2nd-place-solutions?slide=43",
                  "    \"\"\"",
                  "    height = len(array)",
                  "    width = len(array[0])",
                  "    rands = np.random.uniform(0, 1, (height, width))",
                  "    copy = np.copy(array)",
                  "    for h in range(height):",
                  "        for w in range(width):",
                  "            if rands[h, w] <= pb:",
                  "                swap_target_h = random.randint(0, h)",
                  "                copy[h, w] = array[swap_target_h, w]",
                  "    return copy",
                  "",
                  "class DenoisingAutoEncoder:",
                  "    \"\"\"DenoisingAutoEncoder",
                  "    テーブルデータのAutoEncoder。データにノイズを付与し、ノイズ除去するように学習させて特徴を取り出す",
                  "    テーブルデータも何らかの確率的な振る舞いをしていて、事象の例外などのノイズ的な影響を受けるとき、Denosing AutoEncoder使えば一般的で、汎用的な表現に変換できるのかもしれない",
                  "    lightGBMだと精度下がった。。。",
                  "    \"\"\"",
                  "    def __init__(self, tr_x, te_x, run_fold_name=\"auto_enc\", params={\"nodes\": [4096, 4096, 4096]}) -> None:",
                  "        \"\"\"コンストラクタ",
                  "        :param run_fold_name: ランの名前とfoldの番号を組み合わせた名前",
                  "        :param params: ハイパーパラメータ",
                  "        \"\"\"",
                  "        self.run_fold_name = run_fold_name",
                  "        self.params = params",
                  "        self.model = None",
                  "        # NNに入れる前に行うデータ前処理のスケーリング",
                  "        self.tr_x, self.te_x = pre_scale(tr_x, te_x)",
                  "        #print(self.tr_x, self.te_x)",
                  "        # ノイズ追加データ",
                  "        self.tr_x_noise = swap_noise(self.tr_x, pb=0.15)",
                  "        self.te_x_noise = swap_noise(self.te_x, pb=0.15)",
                  "        #print(self.tr_x_noise, self.te_x_noise)",
                  "    ",
                  "    def build_model(self, n_feature, nodes, l2_val=1e-4):",
                  "        \"\"\"encoder+decoderのモデル構築",
                  "        パラメ参考: https://speakerdeck.com/hoxomaxwell/home-credit-default-risk-2nd-place-solutions?slide=43",
                  "        \"\"\"",
                  "        # encoder",
                  "        inputs = Input(shape=(n_feature, ))",
                  "        x = inputs",
                  "        for i, node in enumerate(nodes):",
                  "            x = Dense(node, activation='relu', kernel_regularizer=l2(l2_val), name=f'encoder_layer{i}')(x)",
                  "        ",
                  "        # decoder",
                  "        for i, node in enumerate(nodes[::-1]):",
                  "            x = Dense(node, activation='relu', kernel_regularizer=l2(l2_val), name=f'decoder_layer{i}')(x)",
                  "        outputs = Dense(n_feature, activation='sigmoid', kernel_regularizer=l2(l2_val))(x)",
                  "",
                  "        # autoencoder",
                  "        return Model(inputs, outputs, name=\"autoencoder\")",
                  "    ",
                  "    def train(self):",
                  "        \"\"\"モデル学習\"\"\"",
                  "        # 出力ディレクトリ作成",
                  "        os.makedirs(self.params[\"out_dir\"], exist_ok=True)",
                  "        ",
                  "        cb = []",
                  "        cb.append(ModelCheckpoint(filepath=os.path.join(self.params[\"out_dir\"], f\"best_val_loss_{self.run_fold_name}.h5\"), monitor=\"val_loss\", save_best_only=True, verbose=2,))",
                  "        cb.append(EarlyStopping(monitor=\"val_loss\", patience=self.params[\"epochs\"] // 3, verbose=1))",
                  "        cb.append(ReduceLROnPlateau(monitor='val_loss', factor=0.001, patience=5, min_lr=0.00001))  # https://speakerdeck.com/hoxomaxwell/home-credit-default-risk-2nd-place-solutions?slide=43",
                  "        ",
                  "        self.model = self.build_model(self.tr_x.shape[1], self.params[\"nodes\"])",
                  "        self.model.compile(optimizer='adam', loss='mse')",
                  "        hist = self.model.fit(self.tr_x_noise, self.tr_x, ",
                  "                              validation_data=(self.te_x_noise, self.te_x), ",
                  "                              epochs=self.params[\"epochs\"], ",
                  "                              batch_size=self.params[\"batch_size\"],",
                  "                              callbacks=cb,",
                  "                             )",
                  "        ",
                  "        self.model = load_model(os.path.join(self.params[\"out_dir\"], f\"best_val_loss_{self.run_fold_name}.h5\"), compile=False)",
                  "        ",
                  "        return hist",
                  "    ",
                  "    def predict_encoder(self):",
                  "        \"\"\"出力するlayerの名前指定して中間層(encoder)出力",
                  "        エンコーダの全layerの列concatする",
                  "        参考: https://speakerdeck.com/hoxomaxwell/home-credit-default-risk-2nd-place-solutions?slide=44",
                  "        \"\"\"",
                  "        df_enc_con = None",
                  "        for i in range(len(params[\"nodes\"])):",
                  "            encoder = Model(inputs=self.model.input, outputs=self.model.get_layer(f'encoder_layer{i}').output)   ",
                  "            ",
                  "            # train/testでそれぞれpredict",
                  "            #df_enc_tr = pd.DataFrame(encoder.predict(self.tr_x_noise))",
                  "            #df_enc_te = pd.DataFrame(encoder.predict(self.te_x_noise))",
                  "            df_enc_tr = pd.DataFrame(encoder.predict(self.tr_x))",
                  "            df_enc_te = pd.DataFrame(encoder.predict(self.te_x))",
                  "            df_enc = pd.concat([df_enc_tr, df_enc_te])",
                  "            #print(df_enc_tr.shape, df_enc_te.shape, df_enc.shape)",
                  "            ",
                  "            # すべて同じ値の列削除",
                  "            df_enc = df_enc.loc[:, ~(df_enc.nunique()==1)]  ",
                  "            df_enc.columns = [f'encoder_layer{i}_{ii}' for ii in range(df_enc.shape[1])]",
                  "            ",
                  "            # 横積み",
                  "            df_enc_con = df_enc if df_enc_con is None else pd.concat([df_enc_con, df_enc], axis=1)  ",
                  "        ",
                  "        return df_enc_con.iloc[:df_enc_tr.shape[0]], df_enc_con.iloc[df_enc_tr.shape[0]:]",
                  "        ",
                  "",
                  "if __name__ == '__main__':",
                  "    def sample_data():",
                  "        import seaborn as sns",
                  "        from sklearn.model_selection import train_test_split",
                  "",
                  "        df = sns.load_dataset(\"titanic\")",
                  "        df = df.drop([\"alive\"], axis=1)",
                  "",
                  "        # 欠損最頻値で補完",
                  "        for col in [col for col in df.columns if df[col].isnull().any()]:",
                  "            column_mode = df[col].mode()[0]",
                  "            df[col].fillna(column_mode, inplace=True)",
                  "",
                  "        for col in [",
                  "            \"sex\",",
                  "            \"embarked\",",
                  "            \"who\",",
                  "            \"embark_town\",",
                  "            \"class\",",
                  "            \"adult_male\",",
                  "            \"alone\",",
                  "            \"deck\",",
                  "        ]:",
                  "            df[col], uni = pd.factorize(df[col])",
                  "        target_col = \"survived\"",
                  "        feats = df.columns.to_list()",
                  "        feats.remove(target_col)",
                  "",
                  "        (X_train, X_test, y_train, y_test) = train_test_split(df.drop(target_col, axis=1), df[target_col], test_size=0.3, random_state=71)",
                  "        ",
                  "        return X_train, X_test, y_train, y_test",
                  "    ",
                  "    X_train, X_test, y_train, y_test = sample_data()",
                  "",
                  "    params={\"out_dir\": \"tmp\",",
                  "            \"nodes\": [128, 64, 32], ",
                  "            \"batch_size\": 256,",
                  "            \"epochs\": 1000",
                  "           }",
                  "    auto_enc = DenoisingAutoEncoder(X_train, X_test, params=params)",
                  "    ",
                  "    hist = auto_enc.train()",
                  "    auto_enc.model.summary()",
                  "",
                  "    df_enc_tr_con, df_enc_te_con = auto_enc.predict_encoder()",
                  "    print(df_enc_tr_con.shape)",
                  "    print(df_enc_te_con.shape)",
                  "",
                  "    #df_enc_tr_con[\"y\"] = y_train",
                  "    #df_enc_te_con[\"y\"] = y_test",
                  "    display(df_enc_tr_con)",
                  "    display(df_enc_te_con)",
                ]
              },
            ]
          },
          {
            'name': '5.特徴量の作成_時系列',
            'sub-menu': [
              {
                'name': '日付・時間の列分解',
                'snippet': [
                  "def time_cols(df, date_col):",
                  "    df[date_col] = pd.to_datetime(df[date_col]) # dtype を datetime64 に変換",
                  "    df['year'] = df[date_col].dt.year",
                  "    df['month'] = df[date_col].dt.month",
                  "    df['day'] = df[date_col].dt.day",
                  "    df['dayofyear'] = df[date_col].dt.dayofyear",
                  "    df['dayofweek'] = df[date_col].dt.dayofweek",
                  "    df['weekend'] = (df[date_col].dt.dayofweek.values >= 5).astype(int)",
                  "    df['hour'] = df[date_col].dt.hour",
                  "    df['minute'] = df[date_col].dt.minute",
                  "    df['second'] = df[date_col].dt.second",
                  "    return df",
                  "",
                  "import pandas as pd",
                  "import seaborn as sns",
                  "df = sns.load_dataset(\"titanic\")",
                  "df[\"datetime\"] = pd.date_range(start='2018-2-1', periods=df.shape[0], freq='H')  # 疑似datetime列作成",
                  "df = time_cols(df, \"datetime\")",
                  "df",
                ]
              },
              {
                'name': '季節列追加',
                'snippet': [
                  "def add_season(df, date_col):",
                  "    def _to_season(month_num):",
                  "        season = \"winter\"",
                  "        if 3 <= month_num <= 5:",
                  "            season = \"spring\"",
                  "        elif 6 <= month_num <= 8:",
                  "            season = \"summer\"",
                  "        elif 9 <= month_num <= 11:",
                  "            season = \"autumn\"",
                  "        return season",
                  "        ",
                  "    df[date_col] = pd.to_datetime(df[date_col]) # dtype を datetime64 に変換",
                  "    df['season'] = df[date_col].dt.month.apply(_to_season)",
                  "    return df",
                  "",
                  "import pandas as pd",
                  "import seaborn as sns",
                  "df = sns.load_dataset(\"titanic\")",
                  "df[\"datetime\"] = pd.date_range(start='2018-2-1', periods=df.shape[0], freq='H')  # 疑似datetime列作成",
                  "df = add_season(df, \"datetime\")",
                  "df",
                ]
              },
              {
                'name': '3時間ごとの時間帯列追加',
                'snippet': [
                  "def add_timezone(df, date_col):",
                  "    def _to_timezone(hour_num):",
                  "        \"\"\"",
                  "        3時間おきの時間帯",
                  "        参考: https://jpnculture.net/jikan/",
                  "        \"\"\"",
                  "        timezone = \"early_dawn\"  # 未明(am 0-3)",
                  "        if 3 <= hour_num < 6:",
                  "            timezone = \"dawn\"  # 明け方",
                  "        elif 6 <= hour_num < 9:",
                  "            timezone = \"morning\"  # 朝",
                  "        elif 9 <= hour_num < 12:",
                  "            timezone = \"before_noon\"  # 昼前",
                  "        elif 12 <= hour_num < 15:",
                  "            timezone = \"afternoon\"  # 昼過ぎ",
                  "        elif 15 <= hour_num < 18:",
                  "            timezone = \"evening\"  # 夕方",
                  "        elif 18 <= hour_num < 21:",
                  "            timezone = \"night\"  # 夜",
                  "        elif 21 <= hour_num < 24:",
                  "            timezone = \"midnight\"  # 夜遅く",
                  "        return timezone",
                  "        ",
                  "    df[date_col] = pd.to_datetime(df[date_col]) # dtype を datetime64 に変換",
                  "    df['timezone'] = df[date_col].dt.hour.apply(_to_timezone)",
                  "    return df",
                  "",
                  "import pandas as pd",
                  "import seaborn as sns",
                  "df = sns.load_dataset(\"titanic\")",
                  "df[\"datetime\"] = pd.date_range(start='2018-2-1', periods=df.shape[0], freq='H')  # 疑似datetime列作成",
                  "df = add_timezone(df, \"datetime\")",
                  "df",
                ]
              },
              {
                'name': '1レコード前との差分列追加',
                'snippet': [
                  "def diff(df, num_cols):",
                  "    for col in num_cols:",
                  "        df[f\"{col}_diff\"] = df[col].diff()",
                  "    return df",
                  "",
                  "df = diff(df, [\"age\", \"fare\"])",
                ]
              },
              {
                'name': 'nレコード前のラグ特徴量追加',
                'snippet': [
                  "def prev_hour(df, num_col, n):",
                  "    # fillna(method='bfill')は欠損値を直後の値を使って埋める",
                  "    df[f\"prev_hour_{n}_{num_col}\"] = df[num_col].shift(n).fillna(method='bfill')",
                  "    return df",
                  "",
                  "df = prev_hour(df, \"age\", 1)",
                  "df = prev_hour(df, \"age\", 2)",
                  "df = prev_hour(df, \"age\", 4)",
                  "df = prev_hour(df, \"age\", 8)",
                ]
              },
              {
                'name': '1日毎に平均した1つ前のラグ特徴量追加',
                'snippet': [
                  "def prev_day_mean(df, num_col):",
                  "    # freq='D'にすることで1日単位でまとめられる。月ならfreq='M'",
                  "    # https://qiita.com/wakame1367/items/fafc1b8e2406156a5e4c",
                  "    # fillna(method='bfill')は欠損値を直後の値を使って埋める",
                  "    df[f\"prev_day_{num_col}\"] = df.groupby(pd.Grouper(freq='D'))[num_col].mean().shift(1).fillna(method='bfill')",
                  "    return df",
                  "",
                  "df.index = pd.date_range(start='2018-2-1', periods=df.shape[0], freq='H')  # 疑似datetime列作成",
                  "df = prev_day_mean(df, \"age\")",
                ]
              },
              {
                'name': '移動平均特徴量追加',
                'snippet': [
                  "df = pd.read_csv(\"https://raw.githubusercontent.com/facebook/prophet/master/examples/example_air_passengers.csv\")",
                  "df['ds'] = pd.to_datetime(df['ds'], format='%Y-%m-%d')",
                  "df = df.set_index('ds')",
                  "",
                  "df[\"y_rolling_mean\"] = df[\"y\"].rolling(3).mean().fillna(method='bfill')",
                  "df[\"y_rolling_sum\"] = df[\"y\"].rolling(3).sum().fillna(method='bfill')",
                  "df[\"y_rolling_count\"] = df[\"y\"].rolling(3).count() - 1  # 自身の行はカウントしたくない場合は1を引く必要があり",
                  "display(df.head(5))",
                ]
              },
              {
                'name': 'クリスマスに休日フラグつける',
                'snippet': [
                  "extra_holidays = [\"2011-12-25\", \"2011-12-26\", \"2012-12-24\", \"2012-12-26\"]",
                  "for holiday in extra_holidays:",
                  "    df.loc[holiday, \"holiday\"] = 1",
                  "    df.loc[holiday, \"workingday\"] = 0"
                ]
              },
            ]
          },
          {
            'name': '6.特徴量の選択',
            'sub-menu': [
              {
                'name': 'すべて同じ値の列削除',
                'snippet': [
                  "df.loc[:,~(df.nunique()==1)]",
                ]
              },
              {
                'name': '相関高い列削除',
                'snippet': [
                  "# 相関高い列削除",
                  "corr_matrix = df.corr().abs()",
                  "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))",
                  "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]",
                  "df.drop(df.columns[to_drop], axis=1)",
                ]
              },
              {
                'name': '不要な列削除',
                'snippet': [
                  "import sys",
                  "",
                  "sys.path.append(r\"C:\\Users\\81908\\Git\\xfeat\")",
                  "import xfeat",
                  "from xfeat import Pipeline",
                  "from xfeat.selector import DuplicatedFeatureEliminator",
                  "from xfeat.selector import ConstantFeatureEliminator",
                  "from xfeat.selector import SpearmanCorrelationEliminator",
                  "from xfeat.utils import compress_df",
                  "",
                  "",
                  "def remove_useless_features(df, cols=None, threshold=0.95):",
                  "    \"\"\"",
                  "    xfeatで不要な特徴量削除",
                  "    - 列の内容が重複している列削除",
                  "    - すべて同じ値の列削除",
                  "    - スピマンの相関係数が高い列（多重共変性ある列）削除.相関係数がthresholdより高い列が消される",
                  "    https://github.com/pfnet-research/xfeat/blob/master/examples/remove_useless_features.py",
                  "    \"\"\"",
                  "    # データ型を変換してメモリ使用量を削減",
                  "    cols = df.columns.tolist() if cols is None else cols",
                  "    df = compress_df(pd.DataFrame(data=df, columns=cols))",
                  "",
                  "    encoder = Pipeline(",
                  "        [",
                  "            DuplicatedFeatureEliminator(),",
                  "            ConstantFeatureEliminator(),",
                  "            SpearmanCorrelationEliminator(threshold=threshold),  # 相関係数>thresholdの特長削除",
                  "        ]",
                  "    )",
                  "    df_reduced = encoder.fit_transform(df)",
                  "    # print(\"Selected columns: {}\".format(df_reduced.columns.tolist()))",
                  "    return df_reduced",
                  "",
                  "",
                  "remove_useless_features(df, threshold=0.4)",
                ]
              },
              {
                'name': 'lgbのfeature_importanceで特徴量選択',
                'snippet': [
                  "import pandas as pd",
                  "",
                  "import sys",
                  "sys.path.append(r\"C:\\Users\\81908\\Git\\xfeat\")",
                  "import xfeat",
                  "from xfeat import GBDTFeatureSelector",
                  "",
                  "def run_feature_selection(",
                  "    df, target_col, params,",
                  "):",
                  "    \"\"\"feature_importanceの閾値固定して特徴量選択\"\"\"",
                  "    # 特徴量の列名（取捨選択前）",
                  "    input_cols = df.columns.tolist()",
                  "    n_before_selection = len(input_cols)",
                  "    input_cols.remove(target_col)",
                  "",
                  "    # 特徴量選択用モデル取得",
                  "    lgbm_params = {",
                  "        \"objective\": params[\"objective\"],",
                  "        \"metric\": params[\"metric\"],",
                  "        \"verbosity\": -1,",
                  "    }",
                  "    selector = GBDTFeatureSelector(",
                  "        input_cols=input_cols,",
                  "        target_col=target_col,",
                  "        threshold=params[\"threshold\"],",
                  "        lgbm_params=lgbm_params,",
                  "    )",
                  "    selector.fit(df)",
                  "",
                  "    # 選択をした特徴量を返す",
                  "    selected_cols = selector.get_selected_cols()",
                  "    print(f\" - {n_before_selection - len(selected_cols)} features are removed.\")",
                  "    return df[selected_cols]",
                  "",
                  "",
                  "if __name__ == \"__main__\":",
                  "    # サンプルデータ",
                  "    import seaborn as sns",
                  "    df = sns.load_dataset(\"titanic\")",
                  "    df = df.drop([\"alive\"], axis=1)",
                  "    for col in [\"sex\", \"embarked\", \"who\", \"embark_town\"]:",
                  "        df[col], uni = pd.factorize(df[col])",
                  "    ",
                  "    # 分類",
                  "    target_col = \"survived\"",
                  "    # metric=roc_aucでも可能",
                  "    # feature_importance高い順に列数を 列数*threshold にする",
                  "    params = {\"metric\": \"binary_logloss\", \"objective\": \"binary\", \"threshold\": 0.95}",
                  "    select_df = run_feature_selection(df, target_col, params)",
                  "    print(df.shape, select_df.shape)",
                  "    print(df.columns)",
                  "    print(select_df.columns)",
                  "    ",
                  "    # 回帰",
                  "    target_col = \"fare\"",
                  "    # feature_importance高い順に列数を 列数*threshold にする",
                  "    params = {\"metric\": \"rmse\", \"objective\": \"regression\", \"threshold\": 0.95}",
                  "    select_df = run_feature_selection(df, target_col, params)",
                  "    print(df.shape, select_df.shape)",
                  "    print(df.columns)",
                  "    print(select_df.columns)",
                ]
              },
              {
                'name': '再帰的特徴除去',
                'snippet': [
                  "import numpy as np",
                  "import pandas as pd",
                  "from sklearn.feature_selection import RFECV",
                  "from sklearn.model_selection import *",
                  "from sklearn.metrics import *",
                  "import lightgbm as lgb",
                  "",
                  "def plot_rfecv(selector):",
                  "    import seaborn as sns",
                  "    import matplotlib.pyplot as plt",
                  "    sns.set()",
                  "",
                  "    plt.xlabel(\"Number of features selected\")",
                  "    plt.ylabel(\"Cross validation score (nb of correct classifications)\")",
                  "    plt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_)",
                  "    plt.show()",
                  "    plt.clf()",
                  "    plt.close()",
                  "",
                  "",
                  "if __name__ == \"__main__\":",
                  "    from sklearn.linear_model import Ridge, RidgeClassifier",
                  "    ",
                  "    ############### 分類 ###############",
                  "    # サンプルデータ",
                  "    from sklearn.datasets import load_breast_cancer",
                  "    X_train, y_train = load_breast_cancer(return_X_y=True)",
                  "    X_train = pd.DataFrame(X_train)",
                  "    y_train = pd.Series(y_train)",
                  "",
                  "    # リッジ分類",
                  "    clf = RidgeClassifier()",
                  "    # clf = lgb.LGBMClassifier(n_jobs=-1, seed=71)  # 欠損ある場合はGBM使う（リッジより遅い）",
                  "",
                  "    # RFECVは交差検証+再帰的特徴除去。データでかいとメモリ死ぬので注意",
                  "    # RFE（再帰的特徴除去=recursive feature elimination: すべての特徴量を使う状態から、1つずつ特徴量を取り除いていく）で特徴量選択",
                  "    selector = RFECV(clf, cv=KFold(3, shuffle=True), scoring=\"roc_auc\", n_jobs=-1)",
                  "    selector.fit(X_train, y_train)",
                  "",
                  "    # 選択した特徴量",
                  "    select_cols = X_train.columns[selector.get_support()].to_list()",
                  "    print(\"\\nselect_cols:\\n\", select_cols)",
                  "    # 捨てた特徴量",
                  "    print(\"not select_cols:\\n\", X_train.columns[~selector.get_support()].to_list())",
                  "    plot_rfecv(selector)",
                  "    ####################################",
                  "",
                  "    ############### 回帰 ###############",
                  "    # サンプルデータ",
                  "    n_samples, n_features = 1000, 50",
                  "    rng = np.random.RandomState(0)",
                  "    X_train = pd.DataFrame(rng.randn(n_samples, n_features))",
                  "    y_train = pd.Series(rng.randn(n_samples))",
                  "",
                  "    # リッジ回帰",
                  "    reg = Ridge(alpha=1.0)",
                  "    # reg = lgb.LGBMRegressor(n_jobs=-1, seed=71)  # 欠損ある場合はGBM使う（リッジより遅い）",
                  "",
                  "    # RFECVは交差検証+再帰的特徴除去。データでかいとメモリ死ぬので注意",
                  "    # RFE（再帰的特徴除去=recursive feature elimination: すべての特徴量を使う状態から、1つずつ特徴量を取り除いていく）で特徴量選択",
                  "    selector = RFECV(reg, cv=KFold(3, shuffle=True), scoring=\"neg_root_mean_squared_error\", n_jobs=-1)",
                  "    selector.fit(X_train, y_train)",
                  "",
                  "    # 選択した特徴量",
                  "    select_cols = X_train.columns[selector.get_support()].to_list()",
                  "    print(\"\\nselect_cols:\\n\", select_cols)",
                  "    # 捨てた特徴量",
                  "    print(\"not select_cols:\\n\", X_train.columns[~selector.get_support()].to_list())",
                  "    plot_rfecv(selector)",
                  "    ####################################",
                ]
              },
            ]
          },
          {
            'name': '7.データの分割',
            'sub-menu': [
              {
                'name': 'trainとtestの縦積みして再分割',
                'snippet': [
                  "def process_train_test(df_train, df_test):",
                  "    \"\"\"train/testのデータフレーム連結して、データセット全体への処理して、再度train/testに分ける",
                  "    行数で再分割すればいい。この書き方良く忘れる",
                  "    \"\"\"",
                  "    train_num = len(df_train)",
                  "    df = pd.concat([df_train, df_test])",
                  "    ",
                  "    # train/testのデータフレームの処理を書く!!!",
                  "    ",
                  "    return df[:train_num], df[train_num:]",
                  "",
                  "",
                  "import pandas as pd",
                  "import seaborn as sns",
                  "from sklearn.model_selection import train_test_split",
                  "df = sns.load_dataset(\"titanic\")",
                  "df_train, df_test = train_test_split(df, test_size=0.3, random_state=71)",
                  "",
                  "df_train, df_test = process_train_test(df_train, df_test)",
                  "display(df_train)",
                  "display(df_test)",
                ]
              },
              {
                'name': 'trainとtestの分布同じか確認',
                'snippet': [
                  "import lightgbm as lgb",
                  "import numpy as np",
                  "import pandas as pd",
                  "import seaborn as sns",
                  "from sklearn.model_selection import train_test_split",
                  "from sklearn.metrics import roc_auc_score",
                  "",
                  "",
                  "def adversarial_validation(X_train, X_test):",
                  "    \"\"\"",
                  "    adversarial_validation:学習データとテストデータの分布が同じか判断する手法",
                  "    学習データとテストデータを結合してテストデータか否かを目的変数とする二値分類",
                  "    同じ分布なら見分けられないのでauc=0.5に近くなる。こうなるのが理想",
                  "    0.5上回るなら違う分布",
                  "    \"\"\"",
                  "    # 学習データのラベル=1, テストデータのラベル=0とする",
                  "    y_train = np.array([1 for i in range(X_train.shape[0])])",
                  "    y_test = np.array([0 for i in range(X_test.shape[0])])",
                  "",
                  "    # 学習データとテストデータ結合",
                  "    y = np.concatenate([y_train, y_test])",
                  "    X = pd.concat([X_train, X_test])",
                  "",
                  "    # ラベル付け替えたデータでtrain/testに分ける",
                  "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)",
                  "",
                  "    # lightGBM 用のデータに変形",
                  "    lgb_train = lgb.Dataset(X_train, y_train)",
                  "    lgb_val = lgb.Dataset(X_test, y_test)",
                  "",
                  "    # 二値分類モデル作成",
                  "    params = {",
                  "        \"boosting_type\": \"gbdt\",",
                  "        \"objective\": \"binary\",",
                  "        \"metric\": \"binary_logloss\",",
                  "        \"n_jobs\": -1,",
                  "        \"seed\": 236,",
                  "    }",
                  "    model_lgb = lgb.train(",
                  "        params,",
                  "        lgb_train,",
                  "        num_boost_round=500,",
                  "        early_stopping_rounds=50,",
                  "        valid_sets=[lgb_train, lgb_val],",
                  "        verbose_eval=-1,",
                  "    )",
                  "",
                  "    # AUC計算",
                  "    pred = model_lgb.predict(X_test)",
                  "    score = roc_auc_score(y_test, pred)",
                  "    print(f\"AUC: {round(score, 3)}\")",
                  "",
                  "    ",
                  "if __name__ == \"__main__\":",
                  "    df = sns.load_dataset(\"titanic\")",
                  "    df = df.drop([\"sex\", \"embarked\", \"who\", \"embark_town\", \"alive\"], axis=1)",
                  "",
                  "    y_col = \"survived\"",
                  "    drop_cols = [y_col]  # 取り除きたいカラムのリスト",
                  "    cols = [c for c in df.columns if c not in drop_cols]",
                  "    df_x = df[cols]",
                  "    X_train, X_test, y_train, y_test = train_test_split(",
                  "        df_x, df[y_col], random_state=42",
                  "    )",
                  "    ",
                  "    adversarial_validation(X_train, X_test)",
                ]
              },
            ]
          },
          {
            'name': '8.モデル関連',
            'sub-menu': [
              {
                'name': 'AUCの場合アンサンブル平均は確信度をランクに変換してから平均取ること',
                'snippet': [
                  "# AUCで評価する場合は確信度の平均取るでは効果でない。ランクに変換してランクを平均すること",
                  "# https://mlwave.com/kaggle-ensembling-guide/",
                  "",
                  "from sklearn.metrics import roc_auc_score",
                  "",
                  "def add_rank(df, cols):",
                  "    \"\"\"pandasのrank関数で順位に変換して列追加\"\"\"",
                  "    rank = df[cols].rank()",
                  "    rank.columns = [f\"{col}_rank\" for col in cols]",
                  "    return pd.concat([df, rank], axis=1)",
                  "    ",
                  "# 2つの予測結果をアンサンブルする場合",
                  "df_pred1 = pd.DataFrame({\"id\": [1, 2, 3, 4, 5], \"y\": [0.8, 0.7, 0.8, 0.2, 0.4]})",
                  "df_pred2 = pd.DataFrame({\"id\": [1, 2, 3, 4, 5], \"y\": [0.5, 0.3, 0.7, 0.1, 0.9]})",
                  "",
                  "# 確信度をランクにする",
                  "df_pred1 = add_rank(df_pred1, [\"y\"])",
                  "df_pred2 = add_rank(df_pred2, [\"y\"])",
                  "display(df_pred1)",
                  "display(df_pred2)",
                  "",
                  "# ランクでアンサンブル平均",
                  "ensemble = (df_pred1[\"y_rank\"] + df_pred2[\"y_rank\"]) / 2",
                  "print(ensemble)",
                  "print(roc_auc_score([1, 1, 0, 0, 0], df_pred1[\"y\"]))",
                  "print(roc_auc_score([1, 1, 0, 0, 0], ensemble))",
                ]
              },
              {
                'name': 'ネルダーミードで閾値探す',
                'snippet': [
                  "def nelder_mead_th(true_y, pred_y):",
                  "    \"\"\"ネルダーミードでf1スコアから2値分類のbestな閾値見つける\"\"\"",
                  "    from scipy.optimize import minimize",
                  "",
                  "    def f1_opt(x):",
                  "        return -f1_score(true_y, pred_y >= x)",
                  "",
                  "    result = minimize(f1_opt, x0=np.array([0.5]), method=\"Nelder-Mead\")",
                  "    best_threshold = result[\"x\"].item()",
                  "    return best_threshold",
                  "",
                  "",
                  "if __name__ == \"__main__\":",
                  "    import numpy as np",
                  "    import pandas as pd",
                  "    from sklearn.metrics import f1_score",
                  "",
                  "    # サンプルデータ生成の準備",
                  "    rand = np.random.RandomState(seed=71)",
                  "    train_y_prob = np.linspace(0, 1.0, 10000)",
                  "",
                  "    # 真の値と予測値が以下のtrain_y, train_pred_probであったとする",
                  "    train_y = pd.Series(rand.uniform(0.0, 1.0, train_y_prob.size) < train_y_prob)",
                  "    train_pred_prob = np.clip(",
                  "        train_y_prob * np.exp(rand.standard_normal(train_y_prob.shape) * 0.3), 0.0, 1.0",
                  "    )",
                  "    print(train_y_prob, train_pred_prob)",
                  "",
                  "    # 閾値を0.5とすると、F1は0.722",
                  "    init_threshold = 0.5",
                  "    init_score = f1_score(train_y, train_pred_prob >= init_threshold)",
                  "    print(\"init_threshold, init_score:\", init_threshold, init_score)",
                  "",
                  "    best_threshold = nelder_mead_th(train_y, train_pred_prob)",
                  "    best_score = f1_score(train_y, train_pred_prob >= best_threshold)",
                  "    print(\"best_threshold, best_score:\", best_threshold, best_score)",
                ]
              },
              {
                'name': 'ネルダーミードで予測値アンサンブル',
                'snippet': [
                  "def nelder_mead_func(trues, preds):",
                  "    \"\"\"",
                  "    ネルダーミードで任意の関数のbestな重み見つける",
                  "    - func_opt関数の中身変更すること",
                  "    - 複数モデルの結果ブレンドするのに使える",
                  "    \"\"\"",
                  "    from sklearn.metrics import f1_score",
                  "    from scipy.optimize import minimize",
                  "    from sklearn.metrics import mean_squared_error",
                  "",
                  "    def func_opt(x):",
                  "        # y = a*x1 + b*x2 * c*x3 みたいな式のa,b,cのbest重み最適化",
                  "        blend_preds = 0",
                  "        for x_i, p_i in zip(x, preds):",
                  "            blend_preds += p_i * x_i",
                  "        # 正解との平均2乗誤差返す",
                  "        return mean_squared_error(trues, blend_preds)",
                  "",
                  "    result = minimize(func_opt, x0=np.array([1.0 for i in range(len(preds))]), method=\"Nelder-Mead\")",
                  "    # print(result)",
                  "    best_thresholds = result[\"x\"]",
                  "    return best_thresholds",
                  "",
                  "if __name__ == '__main__':",
                  "    import numpy as np",
                  "    import pandas as pd",
                  "",
                  "    train_y_prob = np.linspace(0, 1.0, 10000)",
                  "    rand = np.random.RandomState(seed=71)",
                  "    train_y = pd.Series(rand.uniform(0.0, 1.0, train_y_prob.size) < train_y_prob)",
                  "    train_pred_prob = np.clip(train_y_prob * np.exp(rand.standard_normal(train_y_prob.shape) * 0.3), 0.0, 1.0)",
                  "    ",
                  "    # y = a*x1 + b*x2 * c*x3 の式のa,b,cのbest重み最適化",
                  "    best_thresholds = nelder_mead_func(train_y, [train_pred_prob, train_pred_prob, train_pred_prob])",
                  "    best_score = (",
                  "        (train_pred_prob * best_thresholds[0])",
                  "        + (train_pred_prob * best_thresholds[1])",
                  "        + (train_pred_prob * best_thresholds[2])",
                  "    )",
                  "    ",
                  "    print(\"best_thresholds, best_score:\", best_thresholds, best_score)",
                ]
              },
            ]
          },
          {
            'name': '999.その他',
            'sub-menu': [
              {
                'name': 'データフレームのメモリ使用量を削減',
                'snippet': [
                  "def reduce_mem_usage(df, verbose=True):",
                  "    \"\"\"メモリ使用量を減らすためにデータ型を変更\"\"\"",
                  "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']",
                  "    start_mem = df.memory_usage().sum() / 1024**2",
                  "    for col in df.columns:",
                  "        col_type = df[col].dtypes",
                  "        if col_type in numerics:",
                  "            c_min = df[col].min()",
                  "            c_max = df[col].max()",
                  "            if str(col_type)[:3] == 'int':",
                  "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:",
                  "                    df[col] = df[col].astype(np.int8)",
                  "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:",
                  "                    df[col] = df[col].astype(np.int16)",
                  "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:",
                  "                    df[col] = df[col].astype(np.int32)",
                  "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:",
                  "                    df[col] = df[col].astype(np.int64)",
                  "            else:",
                  "                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()",
                  "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max and c_prec == np.finfo(np.float16).precision:",
                  "                    df[col] = df[col].astype(np.float16)",
                  "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float64).precision:",
                  "                    df[col] = df[col].astype(np.float32)",
                  "                else:",
                  "                    df[col] = df[col].astype(np.float64)",
                  "    end_mem = df.memory_usage().sum() / 1024**2",
                  "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))",
                  "    return df",
                  "",
                  "import numpy as np",
                  "import pandas as pd",
                  "import seaborn as sns",
                  "reduce_mem_usage(sns.load_dataset(\"titanic\"))",
                ]
              },
              {
                'name': 'ガーベジコレクション',
                'snippet': [
                  "import gc",
                  "",
                  "import pandas as pd",
                  "import seaborn as sns",
                  "df = sns.load_dataset(\"titanic\")",
                  "",
                  "del df",
                  "gc.collect()",
                ]
              },
            ]
          },
        ]
      },
      {
        'name': 'jupyter',
        'sub-menu': [
          {
            'name': 'データフレーム色付け',
            'snippet': [
              "display(df.style.background_gradient(cmap=\"Blues\"))",
            ]
          },
          {
            'name': 'print文色付け',
            'snippet': [
              "### Make prettier the prints ###",
              "from colorama import Fore",
              "c_ = Fore.CYAN",
              "m_ = Fore.MAGENTA",
              "r_ = Fore.RED",
              "b_ = Fore.BLUE",
              "y_ = Fore.YELLOW",
              "g_ = Fore.GREEN",
              "",
              "print(c_,\"FOLDS: \")",
              "print(m_, '*' * 60)",
              "print(r_, '*' * 60)",
              "print(b_, '*' * 60)",
              "print(y_, '*' * 60)",
              "print(g_, '*' * 60)",
            ]
          },
          {
            'name': '1セルの実行結果を複数表示するための便利設定',
            'snippet': [
              "from IPython.core.interactiveshell import InteractiveShell",
              "InteractiveShell.ast_node_interactivity = \"all\"",
              "",
              "# 元の設定に戻す場合は",
              "# InteractiveShell.ast_node_interactivity = \"last_expr\"",
            ]
          },
          {
            'name': '入力セルの表示/非表示を切り替えるjavascript',
            'snippet': [
              "from IPython.display import HTML",
              "",
              "HTML(\"\"\"",
              "<button id=\"code-show-switch-btn\">スクリプトを非表示にする</button>",
              "",
              "<script>",
              "var code_show = true;",
              "",
              "function switch_display_setting() {",
              "    var switch_btn = $(\"#code-show-switch-btn\");",
              "    if (code_show) {",
              "        $(\"div.input\").hide();",
              "        code_show = false;",
              "        switch_btn.text(\"スクリプトを表示する\");",
              "    }else {",
              "        $(\"div.input\").show();",
              "        code_show = true;",
              "        switch_btn.text(\"スクリプトを非表示にする\");",
              "    }",
              "}",
              "",
              "$(\"#code-show-switch-btn\").click(switch_display_setting);",
              "</script>",
              "\"\"\")",
            ]
          },
        ]
      },
      {
        'name': 'plt',
        'sub-menu': [
          {
            'name': 'plt_figure',
            'snippet': [
              "plt.figure(figsize=(10,8)) # グラフのサイズ変更\n",
              "fig, ax = plt.subplots(1, 2, figsize=(10, 3))  # 2行1列で並べる",
              "df.plot(ax=ax[0])",
              "df.plot(ax=ax[1])\n",
              "fig, ax = plt.subplots(2, 2, figsize=(10, 3))  # 2行2列で並べる",
              "df.plot(ax=ax[0, 0])",
              "df.plot(ax=ax[1, 0])",
              "df.plot(ax=ax[0, 1])",
              "df.plot(ax=ax[1, 1])\n",
              "plt.tight_layout(rect=[0,0,1,0.96])  # タイトル重ならないようにする",
              "plt.savefig(\"result.png\", bbox_inches='tight', pad_inches=0)  # bbox_inchesなどは余白削除オプション\n",
              "plt.clf()  # メモリ解放",
              "plt.close()",
            ],
          },
          {
            'name': 'CustomPairPlot',
            'snippet': [
              "import sys",
              "sys.path.append(r'C:\\Users\\81908\\Git\\seaborn_analyzer')",
              "",
              "from custom_pair_plot import CustomPairPlot",
              "",
              "#CustomPairPlot().pairanalyzer(df_train)  # カテゴリ型の列れるといつまでたっても処理終わらない",
              "#CustomPairPlot().pairanalyzer(df_train, hue='count')  # hueで目的変数指定するといつまでたっても処理終わらない",
              "",
              "CustomPairPlot().pairanalyzer(df_train[['temp', 'atemp', 'humidity', 'windspeed', 'casual', 'registered', 'count']])",
              "#CustomPairPlot().pairanalyzer(df_train[['temp', 'atemp', 'humidity', 'windspeed', 'casual', 'registered', 'count']], hue='count')  # hueで目的変数指定するといつまでたっても処理終わらない",
              "",
              "plt.show()",
            ],
          },
        ],
      },
      {
        'name': 'df',
        'sub-menu': [
          {
            'name': 'df_object_col',
            'snippet': [
              "df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.to_list()",
            ],
          },
          {
            'name': 'df_int_float_col',
            'snippet': [
              "df.select_dtypes(",
              "    include=[\"int\", \"int32\", \"int64\", \"float\", \"float32\", \"float64\"]",
              ").columns.to_list()"
            ],
          },
          {
            'name': 'df_dtype',
            'snippet': [
              "pd.set_option('display.max_rows', None)",
              "display(pd.DataFrame({\"column\": df.columns, \"dtype\": [df[col].dtype for col in df.columns]}))",
            ],
          },
          {
            'name': 'df_col_split',
            'snippet': [
              "df[[\"year\", \"month\", \"day\"]] = df[\"date\"].str.split(\"/\", expand=True)",
            ],
          },
          {
            'name': 'dict2df',
            'snippet': [
              "dict = {\"a\": \"aaa\", \"b\": \"bbb\"}",
              "df = pd.DataFrame(dict.values(), index=dict.keys())",
            ],
          },
          {
            'name': 'df_drop_cols',
            'snippet': [
              "drop_cols = ['survived', 'pclass'] # 取り除きたいカラムのリスト",
              "cols = [c for c in df.columns if c not in drop_cols]",
              "df = df[cols]"
            ],
          },
          {
            'name': 'df_dropna',
            'snippet': [
              "df = df.dropna()",
            ],
          },
          {
            'name': 'df_not_null',
            'snippet': [
              "train = df[df[y_col].notnull()]  # 欠損ではない行のみ",
              "test = df[df[y_col].isnull()]  # 欠損行のみ",
            ],
          },
          {
            'name': 'df_null_check',
            'snippet': [
              "pd.set_option('display.max_rows', None)",
              "pd.DataFrame(df_train.isnull().sum())",
            ],
          },
          {
            'name': 'df_progress_apply',
            'snippet': [
              "from tqdm import tqdm",
              "tqdm.pandas()",
              "",
              "import pandas as pd",
              "import seaborn as sns",
              "df = sns.load_dataset(\"titanic\")",
              "df = df.drop([\"alive\"], axis=1)",
              "for col in [\"sex\", \"embarked\", \"who\", \"embark_town\", \"class\", \"adult_male\", \"alone\", \"deck\"]:",
              "    df[col], uni = pd.factorize(df[col])",
              "",
              "df.progress_apply(lambda x: x ** 2, axis=1)",
            ],
          },
          {
            'name': 'df_agg',
            'snippet': [
              "agg = {\"sex\": [\"max\", \"min\", \"size\"]}",
              "df.groupby(\"fare\").agg(agg).reset_index()",
            ],
          },
          {
            'name': 'df_label_encoder',
            'snippet': [
              "import seaborn as sns",
              "from sklearn.preprocessing import LabelEncoder",
              "df = sns.load_dataset('titanic')",
              "display(df)",
              "",
              "num_cols = [\"age\", \"fare\"]",
              "cat_cols = list(set(df.columns.to_list()) - set(num_cols))",
              "enc_cols = list(set(cat_cols) - set([\"survived\", \"pclass\", \"sibsp\", \"parch\"]))",
              "df[enc_cols] = df[enc_cols].apply(lambda x: LabelEncoder().fit_transform(x.astype(str)))",
              "display(df)",
            ],
          },
          {
            'name': 'df_label_encoder_factorize',
            'snippet': [
              "for col in [\"sex\", \"class\"]:",
              "    df[col], uni = pd.factorize(df[col])",
            ],
          },
          {
            'name': 'df_dummy',
            'snippet': [
              "def dummy_df(df, col: str):",
              "    \"\"\"指定列をダミー変数にする\"\"\"",
              "    import pandas as pd",
              "    dummy = pd.get_dummies(df[col], drop_first=True)",
              "    df = pd.concat([df, dummy], axis=1)",
              "    return df.drop([col], axis=1)",
            ],
          },
          {
            'name': 'カテゴリ型の列をobject型に変換',
            'snippet': [
              "df = df.apply(lambda x: x.astype(str) if x.dtype.name == \"category\" else x)",
              "print(df.dtypes)",
            ],
          },
          {
            'name': 'Parquet(パーケィ)で入出力',
            'snippet': [
              "# poetry add pyarrow 必要",
              "df.to_parquet('tmp/df.parq')",
              "pd.read_parquet('tmp/df.parq')",
            ],
          },
          {
            'name': 'featherで入出力',
            'snippet': [
              "# poetry add pyarrow 必要",
              "df.to_feather('tmp/df.ftr')",
              "pd.read_feather('tmp/df.ftr')",
            ],
          },
          {
            'name': '列名の空白を「_」で埋める',
            'snippet': [
              "df.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in df.columns]"
            ],
          },
          {
            'name': 'df色付ける',
            'snippet': [
              "display(df.describe().style.background_gradient(cmap=\"Pastel1\"))"
            ],
          },
          {
            'name': 'dfをplotly化',
            'snippet': [
              "# pandas version 0.25 から使える",
              "",
              "import pandas as pd",
              "pd.options.plotting.backend = \"plotly\"",
              "",
              "df = pd.DataFrame(dict(a=[1,3,2], b=[3,2,1]))",
              "fig = df.plot()",
              "fig.show()",
            ],
          },
        ]
      },
      {
        'name': 'list',
        'sub-menu': [
          {
            'name': 'list_flatten',
            'snippet': [
              "def deep_flatten(xs):",
              "    \"\"\"入れ子のリストを1次元にする\"\"\"",
              "    flat_list = []",
              "    [flat_list.extend(deep_flatten(x)) for x in xs] if isinstance(xs, list) else flat_list.append(xs)",
              "    return flat_list",
              "",
              "deep_flatten([1, [2], [[3], 4], 5]) # [1,2,3,4,5]",
            ]
          },
          {
            'name': 'has_duplicates',
            'snippet': [
              "def has_duplicates(lst):",
              "    \"\"\"リストに重複する値があるか\"\"\"",
              "    return len(lst) != len(set(lst))",
              "    ",
              "x = [1,2,3,4,5,5]",
              "y = [1,2,3,4,5]",
              "has_duplicates(x) # True",
              "has_duplicates(y) # False",
            ]
          },
          {
            'name': 'to_dictionary',
            'snippet': [
              "def to_dictionary(keys, values):",
              "    \"\"\"2つのリストをkey,valueとして辞書に変換する\"\"\"",
              "    return dict(zip(keys, values))",
              "    ",
              "keys = [\"a\", \"b\", \"c\"]    ",
              "values = [2, 3, 4]",
              "print(to_dictionary(keys, values)) # {'a': 2, 'c': 4, 'b': 3}",
            ]
          },
        ]
      },
      {
        'name': 'lightGBM',
        'sub-menu': [
          {
            'name': 'lightGBM_importance',
            'snippet': [
              "import lightgbm as lgb",
              "import seaborn as sns",
              "from sklearn.model_selection import train_test_split",
              "from sklearn.metrics import roc_auc_score",
              "",
              "def save_plot_importance(",
              "    model_path, png_path=None, is_Agg=True, height=0.5, figsize=(8, 20),",
              "):",
              "    \"\"\"        ",
              "    lgbのモデルファイルからモデルロードしてfeature importance plot",
              "    ※lgbのデフォルトのfeature importanceの出し方はsplit(その特徴量が決定木の分岐に現れた回数)",
              "    \"\"\"",
              "    import matplotlib.pyplot as plt",
              "",
              "    if isinstance(model_path, str):",
              "        model = Util.load(model_path)",
              "    else:",
              "        model = model_path",
              "",
              "    if is_Agg:",
              "        import matplotlib",
              "",
              "        matplotlib.use(\"Agg\")",
              "",
              "    print(\"feature importance plot:\", model)",
              "    lgb.plot_importance(model, height=height, figsize=figsize)",
              "    if png_path is not None:",
              "        plt.savefig(",
              "            png_path, bbox_inches=\"tight\", pad_inches=0,",
              "        )  # bbox_inchesなどは余白削除オプション",
              "    plt.show()",
              "    plt.clf()",
              "    plt.close()",
              "",
              "if __name__ == '__main__':",
              "    df = sns.load_dataset(\"titanic\")",
              "    df = df.drop([\"sex\", \"embarked\", \"who\", \"embark_town\", \"alive\"], axis=1)",
              "",
              "    y_col = \"survived\"",
              "    drop_cols = [y_col]  # 取り除きたいカラムのリスト",
              "    cols = [c for c in df.columns if c not in drop_cols]",
              "    df_x = df[cols]",
              "    X_train, X_test, y_train, y_test = train_test_split(df_x, df[y_col], random_state=42)",
              "",
              "    params = {",
              "        \"boosting_type\": \"gbdt\",",
              "        \"objective\": \"binary\",",
              "        \"metric\": \"binary_logloss\",",
              "        \"n_jobs\": -1,",
              "        \"seed\": 236,",
              "        \"learning_rate\": 0.1,",
              "        \"bagging_fraction\": 0.75,",
              "        \"bagging_freq\": 10,",
              "        \"colsample_bytree\": 0.75,",
              "    }",
              "",
              "    # lightGBM 用のデータに変形",
              "    lgb_train = lgb.Dataset(X_train, y_train)",
              "    lgb_val = lgb.Dataset(X_test, y_test)",
              "",
              "    # モデルの作成",
              "    model_lgb = lgb.train(",
              "        params,",
              "        lgb_train,",
              "        num_boost_round=100,",
              "        early_stopping_rounds=30,",
              "        valid_sets=[lgb_train, lgb_val],",
              "        verbose_eval=1,",
              "    )",
              "    pred = model_lgb.predict(X_test)",
              "    score = roc_auc_score(y_test, pred)",
              "    print(f\"\\ntest set AUC: {round(score, 3)}\")",
              "",
              "    # feature importance",
              "    save_plot_importance(model_lgb, \"tmp/importance.png\", is_Agg=False)",
            ],
          },
          {
            'name': 'lightGBM_cv',
            'snippet': [
              "import lightgbm as lgb",
              "import seaborn as sns",
              "from sklearn.model_selection import train_test_split",
              "from sklearn.model_selection import KFold, TimeSeriesSplit",
              "",
              "df = sns.load_dataset(\"titanic\")",
              "df = df.drop([\"sex\", \"embarked\", \"who\", \"embark_town\", \"alive\"], axis=1)",
              "",
              "y_col = \"age\"",
              "drop_cols = [y_col]  # 取り除きたいカラムのリスト",
              "cols = [c for c in df.columns if c not in drop_cols]",
              "df_x = df[cols]",
              "X_train, X_test, y_train, y_test = train_test_split(df_x, df[y_col], random_state=42)",
              "",
              "params = {",
              "    \"boosting_type\": \"gbdt\",",
              "    \"objective\": \"regression\",",
              "    \"metric\": \"rmse\",",
              "    \"n_jobs\": -1,",
              "    \"seed\": 236,",
              "}",
              "",
              "# lightGBM 用のデータに変形",
              "lgb_train = lgb.Dataset(X_train, y_train)",
              "lgb_val = lgb.Dataset(X_test, y_test)",
              "",
              "# モデルの作成",
              "cv_results = lgb.cv(",
              "    params,",
              "    lgb_train,",
              "    num_boost_round=100,",
              "    early_stopping_rounds=30,",
              "    folds=KFold(),",
              "    # folds=TimeSeriesSplit(),",
              "    nfold=3,",
              "    verbose_eval=1,",
              ")",
              "cv_results",
            ],
          },
          {
            'name': 'lightGBM_sklearn_clf_cv_target_enc_importance',
            'snippet': [
              "import os",
              "import gc",
              "import joblib",
              "import warnings",
              "",
              "import numpy as np",
              "import pandas as pd",
              "from lightgbm import *",
              "from sklearn.metrics import *",
              "from sklearn.model_selection import *",
              "from sklearn.inspection import permutation_importance",
              "import matplotlib.pyplot as plt",
              "import seaborn as sns",
              "from tqdm import tqdm",
              "",
              "warnings.filterwarnings('ignore')",
              "",
              "",
              "class Model:",
              "    def __init__(",
              "        self,",
              "        output_dir=None,",
              "        dict_enc_flag={\"count\": True, \"target\": True, \"catboost\": True},",
              "    ):",
              "        self.output_dir = output_dir",
              "        self.dict_enc_flag = dict_enc_flag",
              "",
              "    def _encoding(self, tr_df, te_df, target_col):",
              "        \"\"\"ターゲットエンコディングとか一括実行\"\"\"",
              "        if self.dict_enc_flag[\"count\"]:",
              "            # カウントエンコディング",
              "            tr_df, te_df = Model().count_encoder(tr_df, te_df, cat_features=None)",
              "        if self.dict_enc_flag[\"target\"]:",
              "            # ターゲットエンコディング",
              "            tr_df, te_df = Model().target_encoder(",
              "                tr_df, te_df, target_col=target_col, cat_features=None",
              "            )",
              "        if self.dict_enc_flag[\"catboost\"]:",
              "            # CatBoostエンコディング",
              "            tr_df, te_df = Model().catboost_encoder(",
              "                tr_df, te_df, target_col=target_col, cat_features=None",
              "            )",
              "        # ラベルエンコディング",
              "        cate_cols = tr_df.select_dtypes(",
              "            include=[\"object\", \"category\", \"bool\"]",
              "        ).columns.to_list()",
              "        for col in cate_cols:",
              "            tr_df[col], uni = pd.factorize(tr_df[col])",
              "            te_df[col], uni = pd.factorize(te_df[col])",
              "",
              "        return tr_df, te_df",
              "",
              "    # LightGBM GBDT with KFold or Stratified KFold",
              "    # Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code",
              "    def kfold_cv_LGBMClassifier(",
              "        self,",
              "        lgb_params: dict,",
              "        df: pd.DataFrame,",
              "        num_folds: int,",
              "        target_col: str,",
              "        del_cols=None,",
              "        select_cols=None,",
              "        eval_metric=\"error\",",
              "        stratified=True,  # StratifiedKFoldにするか",
              "        is_submission=False,  # Home_Credit_Default_Risk の submission.csv作成するか",
              "        is_plot_perm_importance=False,  # permutation importanceも出すか. feature_importance はデフォルトでだす",
              "    ):",
              "        \"\"\"",
              "        LGBMClassifierでcross validation + feature_importance/permutation importance plot",
              "        \"\"\"",
              "        # データフレームからID列など不要な列削除",
              "        if del_cols is not None:",
              "            df = df.drop(del_cols, axis=1)",
              "",
              "        # 特徴量の列のみ保持",
              "        feats = df.columns.to_list()",
              "        feats.remove(target_col)",
              "",
              "        # Divide in training/validation and test data",
              "        train_df = df[df[target_col].notnull()].reset_index(drop=True)",
              "        test_df = df[df[target_col].isnull()].reset_index(drop=True)",
              "        print(",
              "            f\"INFO: Starting LightGBM. Train shape: {train_df.shape}, test shape: {test_df.shape}\"",
              "        )",
              "        del df",
              "        gc.collect()",
              "",
              "        ###################################### cross validation ######################################",
              "        # Cross validation model",
              "        if stratified:",
              "            folds = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=1001)",
              "        else:",
              "            folds = KFold(n_splits=num_folds, shuffle=True, random_state=1001)",
              "",
              "        # Create arrays and dataframes to store results",
              "        oof_preds = np.zeros(train_df.shape[0])",
              "        sub_preds = np.zeros(test_df.shape[0])",
              "        feature_importance_df = pd.DataFrame()",
              "        permutation_importance_df = pd.DataFrame()",
              "        result_scores = {}",
              "        train_probas = {}",
              "        test_probas = {}",
              "",
              "        for n_fold, (train_idx, valid_idx) in enumerate(",
              "            folds.split(train_df[feats], train_df[target_col])",
              "        ):",
              "            print(",
              "                f\"\\n------------------------------------ n_fold={n_fold + 1} ------------------------------------\"",
              "            )",
              "            ############################ create fold ############################",
              "            t_fold_df = train_df.iloc[train_idx]",
              "            v_fold_df = train_df.iloc[valid_idx]",
              "",
              "            # ターゲットエンコディングとか一括実行",
              "            t_fold_df, v_fold_df = self._encoding(t_fold_df, v_fold_df, target_col)",
              "            print(",
              "                f\"INFO: run encoding Train shape: {t_fold_df.shape}, valid shape: {v_fold_df.shape}\"",
              "            )",
              "",
              "            # 指定の列あればそれだけにする",
              "            feats = t_fold_df.columns.to_list() if select_cols is None else select_cols",
              "            if target_col in feats:",
              "                feats.remove(target_col)",
              "            print(f\"INFO: len(feats): {len(feats)}\\n\")",
              "",
              "            train_x, train_y = (",
              "                t_fold_df[feats],",
              "                t_fold_df[target_col],",
              "            )",
              "            valid_x, valid_y = (",
              "                v_fold_df[feats],",
              "                v_fold_df[target_col],",
              "            )",
              "",
              "            ############################ train fit ############################",
              "            # LightGBM parameters found by Bayesian optimization",
              "            clf = LGBMClassifier(**lgb_params)",
              "            clf.fit(",
              "                train_x,",
              "                train_y,",
              "                eval_set=[(train_x, train_y), (valid_x, valid_y)],",
              "                eval_metric=eval_metric,",
              "                verbose=200,",
              "                early_stopping_rounds=200,",
              "            )",
              "            # モデル保存",
              "            joblib.dump(clf, f\"{self.output_dir}/lgb-{n_fold + 1}.model\", compress=True)",
              "",
              "            ############################ valid pred ############################",
              "            oof_preds[valid_idx] = clf.predict_proba(",
              "                valid_x, num_iteration=clf.best_iteration_",
              "            )[:, 1]",
              "            if eval_metric == \"auc\":",
              "                fold_auc = roc_auc_score(valid_y, oof_preds[valid_idx])",
              "                print(\"\\nINFO: Fold %2d AUC : %.6f\" % (n_fold + 1, fold_auc))",
              "                result_scores[f\"fold_auc_{str(n_fold + 1)}\"] = fold_auc",
              "            elif eval_metric == \"error\":",
              "                # intにしないとaccuracy_score()エラーになる",
              "                _pred = oof_preds[valid_idx]",
              "                _pred[_pred >= 0.5] = 1",
              "                _pred[_pred < 0.5] = 0",
              "                fold_err = 1.0 - accuracy_score(valid_y, _pred)",
              "                print(\"\\nINFO: Fold %2d error : %.6f\" % (n_fold + 1, fold_err))",
              "                result_scores[f\"fold_err_{str(n_fold + 1)}\"] = fold_err",
              "",
              "            ############################ test pred ############################",
              "            if test_df.shape[0] > 0:",
              "                # ターゲットエンコディングとか一括実行",
              "                tr_df, te_df = self._encoding(train_df, test_df, target_col)",
              "",
              "                # testの確信度",
              "                test_probas[f\"fold_{str(n_fold + 1)}\"] = clf.predict_proba(",
              "                    te_df[feats], num_iteration=clf.best_iteration_",
              "                )[:, 1]",
              "                sub_preds += test_probas[f\"fold_{str(n_fold + 1)}\"] / folds.n_splits",
              "",
              "                # 一応trainの確信度も出しておく",
              "                train_probas[f\"fold_{str(n_fold + 1)}\"] = clf.predict_proba(",
              "                    tr_df[feats], num_iteration=clf.best_iteration_",
              "                )[:, 1]",
              "",
              "            ############################ importance 計算 ############################",
              "            # feature_importance",
              "            fold_importance_df = pd.DataFrame()",
              "            fold_importance_df[\"feature\"] = feats",
              "            fold_importance_df[\"importance\"] = clf.feature_importances_",
              "            fold_importance_df[\"fold\"] = n_fold + 1",
              "            feature_importance_df = pd.concat(",
              "                [feature_importance_df, fold_importance_df], axis=0",
              "            )",
              "",
              "            if is_plot_perm_importance:",
              "                # permutation_importance",
              "                # 時間かかるからifで制御する",
              "                # scoringはsklearnのスコアリングパラメータ",
              "                # accuracy や neg_mean_squared_log_error とか",
              "                # https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter",
              "                fold_importance_df = pd.DataFrame()",
              "                fold_permutation = permutation_importance(",
              "                    clf, valid_x, valid_y, scoring=\"roc_auc\"",
              "                )",
              "                fold_permutation_df = pd.DataFrame(",
              "                    {",
              "                        \"feature\": valid_x.columns,",
              "                        \"importance\": np.abs(",
              "                            fold_permutation[\"importances_mean\"]",
              "                        ),  # マイナスとるのもあるので絶対値にする",
              "                        \"fold\": n_fold + 1,",
              "                    },",
              "                )",
              "                permutation_importance_df = pd.concat(",
              "                    [permutation_importance_df, fold_permutation_df], axis=0",
              "                )",
              "",
              "            del clf, train_x, train_y, valid_x, valid_y",
              "            gc.collect()",
              "",
              "        print(",
              "            \"\\n------------------------------------ mean fold ------------------------------------\"",
              "        )",
              "        mean_fold_score = None",
              "        if eval_metric == \"auc\":",
              "            mean_fold_score = roc_auc_score(train_df[target_col], oof_preds)",
              "            print(\"INFO: Mean valid AUC score %.6f\" % mean_fold_score)",
              "            result_scores[\"mean_fold_auc\"] = mean_fold_score",
              "        elif eval_metric == \"error\":",
              "            # intにしないとaccuracy_score()エラーになる",
              "            _pred = oof_preds",
              "            _pred[_pred >= 0.5] = 1",
              "            _pred[_pred < 0.5] = 0",
              "            mean_fold_score = 1.0 - accuracy_score(train_df[target_col], _pred)",
              "            print(\"INFO: Mean valid error score %.6f\" % mean_fold_score)",
              "            result_scores[\"mean_fold_err\"] = mean_fold_score",
              "",
              "        # モデルの評価指標出力",
              "        result_scores_df = pd.DataFrame(",
              "            result_scores.values(), index=result_scores.keys()",
              "        )",
              "        result_scores_df.to_csv(f\"{self.output_dir}/result_scores.tsv\", sep=\"\\t\")",
              "",
              "        # 確信度ファイル出力",
              "        train_probas_df = pd.DataFrame(train_probas)",
              "        train_probas_df.to_csv(f\"{self.output_dir}/train_probas.tsv\", index=False)",
              "        if test_df.shape[0] > 0:",
              "            test_probas_df = pd.DataFrame(test_probas)",
              "            test_probas_df.to_csv(f\"{self.output_dir}/test_probas.tsv\", index=False)",
              "",
              "            # Write submission file",
              "            if is_submission:",
              "                # bestの閾値探索",
              "                tr_mean = train_probas_df.apply(lambda x: np.mean(x), axis=1).values",
              "                best_threshold = Model().nelder_mead_th(train_df[target_col], tr_mean)",
              "                print(f\"INFO: submission best_threshold: {best_threshold}\")",
              "                # best_thresholdで2値化",
              "                te_mean = test_probas_df.apply(lambda x: np.mean(x), axis=1).values",
              "                te_mean[te_mean >= best_threshold] = 1",
              "                te_mean[te_mean < best_threshold] = 0",
              "                output_csv = f\"{self.output_dir}/submission_kernel.csv\"",
              "                pd.DataFrame({\"id\": range(len(te_mean)), \"y\": te_mean}).to_csv(",
              "                    output_csv, index=False",
              "                )",
              "                print(f\"INFO: save csv {output_csv}\")",
              "",
              "        # Plot feature importance",
              "        png_path = f\"{self.output_dir}/lgbm_feature_importances.png\"",
              "        Model().display_importances(",
              "            feature_importance_df, png_path=png_path, title=\"feature_importance\",",
              "        )",
              "        # print(f\"INFO: save png {png_path}\")",
              "        if is_plot_perm_importance:",
              "            png_path = f\"{self.output_dir}/lgbm_permutation_importances.png\"",
              "            Model().display_importances(",
              "                permutation_importance_df,",
              "                png_path=png_path,",
              "                title=\"permutation_importance\",",
              "            )",
              "            # print(f\"INFO: save png {png_path}\")",
              "",
              "        return mean_fold_score, feature_importance_df, permutation_importance_df",
              "    ",
              "    @staticmethod",
              "    def count_encoder(train_df, valid_df, cat_features=None):",
              "        \"\"\"",
              "        Count_Encoding: カテゴリ列をカウント値に変換する特徴量エンジニアリング（要はgroupby().size()の集計列追加のこと）",
              "        ※カウント数が同じカテゴリは同じようなデータ傾向になる可能性がある",
              "        https://www.kaggle.com/matleonard/categorical-encodings",
              "        \"\"\"",
              "        # conda install -c conda-forge category_encoders",
              "        import category_encoders as ce",
              "",
              "        if cat_features is None:",
              "            cat_features = train_df.select_dtypes(",
              "                include=[\"object\", \"category\", \"bool\"]",
              "            ).columns.to_list()",
              "",
              "        count_enc = ce.CountEncoder(cols=cat_features)",
              "",
              "        # trainだけでfitすること(validationやtest含めるとリークする)",
              "        count_enc.fit(train_df[cat_features])",
              "        train_encoded = train_df.join(",
              "            count_enc.transform(train_df[cat_features]).add_suffix(\"_count\")",
              "        )",
              "        valid_encoded = valid_df.join(",
              "            count_enc.transform(valid_df[cat_features]).add_suffix(\"_count\")",
              "        )",
              "",
              "        return train_encoded, valid_encoded",
              "",
              "    @staticmethod",
              "    def target_encoder(train_df, valid_df, target_col: str, cat_features=None):",
              "        \"\"\"",
              "        Target_Encoding: カテゴリ列を目的変数の平均値に変換する特徴量エンジニアリング",
              "        https://www.kaggle.com/matleonard/categorical-encodings",
              "        \"\"\"",
              "        # conda install -c conda-forge category_encoders",
              "        import category_encoders as ce",
              "",
              "        if cat_features is None:",
              "            cat_features = train_df.select_dtypes(",
              "                include=[\"object\", \"category\", \"bool\"]",
              "            ).columns.to_list()",
              "",
              "        target_enc = ce.TargetEncoder(cols=cat_features)",
              "",
              "        # trainだけでfitすること(validationやtest含めるとリークする)",
              "        target_enc.fit(train_df[cat_features], train_df[target_col])",
              "",
              "        train_encoded = train_df.join(",
              "            target_enc.transform(train_df[cat_features]).add_suffix(\"_target\")",
              "        )",
              "        valid_encoded = valid_df.join(",
              "            target_enc.transform(valid_df[cat_features]).add_suffix(\"_target\")",
              "        )",
              "        return train_encoded, valid_encoded",
              "",
              "    @staticmethod",
              "    def catboost_encoder(train_df, valid_df, target_col: str, cat_features=None):",
              "        \"\"\"",
              "        CatBoost_Encoding: カテゴリ列を目的変数の1行前の行からのみに変換する特徴量エンジニアリング",
              "        CatBoost使ったターゲットエンコーディング",
              "        https://www.kaggle.com/matleonard/categorical-encodings",
              "        \"\"\"",
              "        # conda install -c conda-forge category_encoders",
              "        import category_encoders as ce",
              "",
              "        if cat_features is None:",
              "            cat_features = train_df.select_dtypes(",
              "                include=[\"object\", \"category\", \"bool\"]",
              "            ).columns.to_list()",
              "",
              "        cb_enc = ce.CatBoostEncoder(cols=cat_features, random_state=7)",
              "",
              "        # trainだけでfitすること(validationやtest含めるとリークする)",
              "        cb_enc.fit(train_df[cat_features], train_df[target_col])",
              "",
              "        train_encoded = train_df.join(",
              "            cb_enc.transform(train_df[cat_features]).add_suffix(\"_cb\")",
              "        )",
              "        valid_encoded = valid_df.join(",
              "            cb_enc.transform(valid_df[cat_features]).add_suffix(\"_cb\")",
              "        )",
              "        return train_encoded, valid_encoded",
              "",
              "    # Display/plot feature/permutation importance",
              "    @staticmethod",
              "    def display_importances(",
              "        importance_df_,",
              "        png_path,",
              "        title,",
              "    ):",
              "        cols = (",
              "            importance_df_[[\"feature\", \"importance\"]]",
              "            .groupby(\"feature\")",
              "            .mean()",
              "            .sort_values(by=\"importance\", ascending=False)[:40]",
              "            .index",
              "        )",
              "        best_features = importance_df_.loc[importance_df_.feature.isin(cols)]",
              "        plt.figure(figsize=(8, 10))",
              "        sns.barplot(",
              "            x=\"importance\",",
              "            y=\"feature\",",
              "            data=best_features.sort_values(by=\"importance\", ascending=False),",
              "        )",
              "        plt.title(f\"LightGBM {title} (avg over folds)\")",
              "        plt.tight_layout()",
              "        plt.savefig(png_path)",
              "",
              "    @staticmethod",
              "    def nelder_mead_th(true_y, pred_y):",
              "        \"\"\"ネルダーミードでf1スコアから2値分類のbestな閾値見つける\"\"\"",
              "        from scipy.optimize import minimize",
              "",
              "        def opt(x):",
              "            return 1.0 - f1_score(true_y, pred_y >= x)  # F1値",
              "            #return 1.0 - accuracy_score(true_y, pred_y >= x)  # 正解率",
              "",
              "        result = minimize(opt, x0=np.array([0.5]), method=\"Nelder-Mead\")",
              "        best_threshold = result[\"x\"].item()",
              "        return best_threshold",
              "",
              "",
              "if __name__ == \"__main__\":",
              "    # サンプルデータ",
              "    import pandas as pd",
              "    import seaborn as sns",
              "",
              "    df = sns.load_dataset(\"titanic\")",
              "    df.iloc[700:, 0] = np.nan  # test set用意",
              "    display(df)",
              "",
              "    OUTPUT_DIR = \"tmp\"",
              "    os.makedirs(OUTPUT_DIR, exist_ok=True)",
              "",
              "    lgb_params = dict(",
              "        n_estimators=10000,",
              "        learning_rate=0.02,",
              "        num_leaves=34,",
              "        colsample_bytree=0.9497036,",
              "        subsample=0.8715623,",
              "        max_depth=8,",
              "        reg_alpha=0.041545473,",
              "        reg_lambda=0.0735294,",
              "        min_split_gain=0.0222415,",
              "        min_child_weight=39.3259775,",
              "        silent=-1,",
              "        random_state=71,",
              "        importance_type=\"gain\",",
              "    )",
              "    params = dict(",
              "        lgb_params=lgb_params,",
              "        df=df,",
              "        num_folds=4,",
              "        target_col=\"survived\",",
              "        del_cols=[\"alive\"],",
              "        select_cols=None,",
              "        eval_metric=\"error\",  # \"auc\",",
              "        stratified=False,  #True",
              "        is_submission=True,  # False",
              "        is_plot_perm_importance=True,",
              "    )",
              "    model = Model(",
              "        output_dir=OUTPUT_DIR,",
              "        dict_enc_flag={\"count\": True, \"target\": True, \"catboost\": True},",
              "        #dict_enc_flag={\"count\": False, \"target\": False, \"catboost\": False},",
              "    )",
              "    mean_fold_score, feat_importance, perm_importance = model.kfold_cv_LGBMClassifier(**params)",
              "    ",
              "print(mean_fold_score)",
            ],
          },
          {
            'name': 'lightGBM_optuna',
            'snippet': [
              "import optuna",
              "from optuna.integration import LightGBMPruningCallback",
              "",
              "import numpy as np",
              "import pandas as pd",
              "import lightgbm as lgb",
              "import seaborn as sns",
              "from sklearn.metrics import mean_squared_error",
              "from sklearn.model_selection import train_test_split",
              "",
              "",
              "def train_optuna():",
              "    def objectives(trial):",
              "        params = {",
              "            \"boosting_type\": \"gbdt\",",
              "            \"objective\": \"binary\",",
              "            \"metric\": \"binary_logloss\",",
              "            \"n_jobs\": -1,",
              "            \"seed\": 236,",
              "            \"learning_rate\": 0.1,",
              "            \"bagging_fraction\": 0.75,",
              "            \"bagging_freq\": 10,",
              "            \"colsample_bytree\": 0.75,",
              "        }",
              "        train_params = {",
              "            \"num_boost_round\": 100,",
              "            \"early_stopping_rounds\": 30,",
              "            \"callbacks\": [LightGBMPruningCallback(trial, params[\"metric\"])],  # 枝刈り",
              "            \"verbose_eval\": 1,",
              "        }",
              "",
              "        # LightGBMPruningCallbackがあるとvalid_names指定できないしtrainのスコアも出せない",
              "        callbacks = (",
              "            train_params.pop(\"callbacks\") if \"callbacks\" in train_params else None",
              "        )",
              "        if callbacks is not None and isinstance(callbacks[0], LightGBMPruningCallback):",
              "            train_params[\"valid_sets\"] = lgb_val",
              "        else:",
              "            train_params[\"valid_names\"] = [\"train\", \"valid\"]",
              "            train_params[\"valid_sets\"] = [lgb_train, lgb_val]",
              "",
              "        # モデルの作成",
              "        model_lgb = lgb.train(params, lgb_train, callbacks=callbacks, **train_params)",
              "        # 予測",
              "        teat_pred = model_lgb.predict(X_test)",
              "        score = np.sqrt(mean_squared_error(y_test, teat_pred))",
              "",
              "        return score",
              "",
              "    df = sns.load_dataset(\"titanic\")",
              "    df = df.drop([\"sex\", \"embarked\", \"who\", \"embark_town\", \"alive\"], axis=1)",
              "",
              "    y_col = \"survived\"",
              "    drop_cols = [y_col]  # 取り除きたいカラムのリスト",
              "    cols = [c for c in df.columns if c not in drop_cols]",
              "    df_x = df[cols]",
              "    X_train, X_test, y_train, y_test = train_test_split(",
              "        df_x, df[y_col], random_state=42",
              "    )",
              "",
              "    # lightGBM 用のデータに変形",
              "    lgb_train = lgb.Dataset(X_train, y_train)",
              "    lgb_val = lgb.Dataset(X_test, y_test)",
              "",
              "    study = optuna.create_study()",
              "    study.optimize(",
              "        objectives, n_trials=2, timeout=60,  # 1分間だけ最適化",
              "    )",
              "",
              "    print(study.best_params)",
              "    print(study.best_value)",
              "    print(study.best_trial.user_attrs)",
              "",
              "    df = study.trials_dataframe()",
              "    # df.to_csv(\"optuna_lgb.csv\")",
              "    print(df)",
              "",
              "",
              "train_optuna()",
            ],
          },
          {
            'name': 'lightGBM_sklearn_optuna_feature_selection',
            'snippet': [
              "import os",
              "import numpy as np",
              "import pandas as pd",
              "import gc",
              "from joblib import Parallel, delayed",
              "import lightgbm as lgb",
              "import optuna",
              "from sklearn.feature_selection import RFECV",
              "from sklearn.model_selection import *",
              "from sklearn.metrics import *",
              "import seaborn as sns",
              "",
              "def exec_study(X, y, cv_index_kv, n_trials, output_dir, t_args):",
              "    def objective(trial):",
              "        gc.collect()",
              "        # ハイパーパラメータ",
              "        max_depth = trial.suggest_int(\"max_depth\", 1, 8)",
              "        num_leaves = trial.suggest_int(\"num_leaves\", 2, 2**max_depth)",
              "        min_child_samples = trial.suggest_int(",
              "            \"min_child_samples\", 1, max(1, int(len(cv_index_kv[1][0]) / num_leaves)))",
              "        tuning_params = dict(",
              "            max_depth=max_depth,",
              "            num_leaves=num_leaves,",
              "            min_child_samples=min_child_samples,",
              "            min_child_weight=trial.suggest_loguniform('min_child_weight', 0.001, 1000),",
              "            feature_fraction=trial.suggest_discrete_uniform('colsample_bytree', 0.1, 0.95, 0.05),  # feature_fractionのこと",
              "            bagging_fraction=trial.suggest_discrete_uniform('subsample', 0.4, 0.95, 0.05),  # bagging_fractionのこと",
              "            bagging_freq=trial.suggest_int('subsample_freq', 1, 10),  # bagging_freqのこと",
              "            reg_alpha=trial.suggest_loguniform('reg_alpha', 1e-09, 10.0),",
              "            reg_lambda=trial.suggest_loguniform('reg_lambda', 1e-09, 10.0),",
              "        )",
              "        if t_args['objective'] == \"regression\":",
              "            tuning_params[\"reg_sqrt\"] = trial.suggest_categorical(\"reg_sqrt\", [True, False])",
              "        print(tuning_params)",
              "",
              "        # クロスバリデーション",
              "        def calc_score(train_index, val_index):",
              "            X_train = X.iloc[train_index]  # TODO df,using_cols,gkfはグローバル変数にしないといけない？",
              "            y_train = y.iloc[train_index]",
              "            X_val   = X.iloc[val_index]",
              "            y_val   = y.iloc[val_index]",
              "            if t_args['objective'] == \"regression\":",
              "                # model = lgb.LGBMRegressor(n_jobs=1, seed=71, n_estimators=10000, learning_rate=0.1, verbose=-1, **tuning_params)",
              "                model = lgb.LGBMRegressor(n_jobs=-1, seed=71, n_estimators=10000, learning_rate=0.1, verbose=-1, **tuning_params)",
              "            else:",
              "                # model = lgb.LGBMClassifier(n_jobs=1, seed=71, n_estimators=10000, learning_rate=0.1, verbose=-1, **tuning_params)",
              "                model = lgb.LGBMClassifier(n_jobs=-1, seed=71, n_estimators=10000, learning_rate=0.1, verbose=-1, **tuning_params)",
              "            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=100, eval_metric=t_args['eval_metric'], verbose=False)",
              "            ",
              "            if t_args[\"objective\"] == \"regression\":",
              "                score = mean_squared_error(y_val, model.predict(X_val))",
              "            else:",
              "                score = 1.0 - roc_auc_score(y_val, model.predict(X_val))",
              "                # score = 1.0 - accuracy_score(y_val, model.predict(X_val))",
              "            return score",
              "        ",
              "        # scores = Parallel(n_jobs=-1)([delayed(calc_score)(train_index, valid_index) for train_index, valid_index in cv_index_kv])",
              "        scores = []",
              "        for train_index, valid_index in cv_index_kv:",
              "            scores = calc_score(train_index, valid_index)",
              "        return np.mean(scores)",
              "",
              "    # 学習実行",
              "    study = optuna.create_study(study_name=\"study\",",
              "                                storage=f\"sqlite:///{output_dir}/study.db\",",
              "                                load_if_exists=True,",
              "                                direction=\"minimize\", ",
              "                                sampler=optuna.samplers.TPESampler(seed=1))",
              "    study.optimize(objective, n_trials=n_trials, n_jobs=1, gc_after_trial=True)",
              "    ",
              "    # 学習履歴保存",
              "    study.trials_dataframe().to_csv(f\"{output_dir}/study_history.csv\", index=False)",
              "    ",
              "    # 最適化されたハイパーパラメータ",
              "    return study.best_params.copy()",
              "",
              "if __name__ == '__main__':  ",
              "    # サンプルデータ",
              "    df = sns.load_dataset(\"titanic\")",
              "    df = df.drop(['class', 'sex', 'embarked', 'who', 'embark_town', 'alive', 'adult_male', 'alone', \"deck\"], axis=1)",
              "    train = df.iloc[:600]",
              "    test = df.iloc[600:]",
              "    y_col = \"survived\"",
              "    using_cols = df.columns.to_list()",
              "    using_cols = list(set(using_cols) - set([y_col]))",
              "    ",
              "    n_trials = 10",
              "    ",
              "    output_dir = \"tmp\"",
              "    os.makedirs(output_dir, exist_ok=True)",
              "    ",
              "    t_args = dict(objective=\"binary\",",
              "                  eval_metric=\"binary_logloss\",",
              "                  #objective=\"regression\",",
              "                  #eval_metric=\"rmse\",",
              "                 )",
              "    ",
              "    X_train = df.loc[train.index][using_cols]",
              "    y_train = df.loc[train.index][y_col]",
              "    n_fold = 5",
              "    cv_index_kv = list(KFold(n_fold).split(X_train, y_train))",
              "    #cv_index_kv = list(StratifiedKFold(n_fold).split(X_train, y_train))",
              "    ",
              "    # 学習実行",
              "    best_params = exec_study(X_train, y_train, cv_index_kv, n_trials, output_dir, t_args)",
              "    print(\"best_params:\\n\", best_params)",
              "    best_params_df = pd.DataFrame(best_params.values(), index=best_params.keys())",
              "    best_params_df.to_csv(f\"{output_dir}/best_params.tsv\", sep=\"\\t\")",
              "    ",
              "    ",
              "    # RFE（recursive feature elimination: すべての特徴量を使う状態から、1つずつ特徴量を取り除いていく）で特徴量選択",
              "    # 本番だから学習率下げてる",
              "    selector = RFECV(",
              "        lgb.LGBMClassifier(n_jobs=1, seed=71, n_estimators=10000, learning_rate=0.01, importance_type=\"gain\", **best_params),",
              "        cv=StratifiedKFold(n_fold, shuffle=True),",
              "        scoring=\"roc_auc\",",
              "        n_jobs=-1",
              "    )",
              "    selector.fit(X_train, y_train)",
              "    ",
              "    # 選択した特徴量",
              "    select_cols = X_train.columns[selector.get_support()].to_list()",
              "    print(\"\\nselect_cols:\\n\", select_cols)",
              "    # 捨てた特徴量",
              "    print(\"not select_cols:\\n\", X_train.columns[~selector.get_support()].to_list())",
              "",
              "    # 選択した特徴量+best_paramsで学習",
              "    model = lgb.LGBMClassifier(n_jobs=1, seed=71, n_estimators=10000, learning_rate=0.01, importance_type=\"gain\", **best_params)",
              "    model.fit(X_train[select_cols], y_train)",
              "",
              "    pred_pb = model.predict_proba(test[select_cols])[:, 1]",
              "    print(\"\\npred_pb:\\n\", pred_pb)",
            ],
          },
          {
            'name': 'lightGBM_optuna_Objective',
            'snippet': [
              "import optuna",
              "from optuna.integration import LightGBMPruningCallback",
              "import os",
              "import argparse",
              "import traceback",
              "import numpy as np",
              "import pandas as pd",
              "import lightgbm as lgb",
              "import seaborn as sns",
              "from sklearn.metrics import mean_squared_error",
              "from sklearn.model_selection import train_test_split",
              "",
              "def get_dataset():",
              "    df_train = pd.read_csv(\"../data/bike-sharing-demand/train.csv\", ",
              "                           dtype={'season': 'category',",
              "                                  'holiday': 'category',",
              "                                  'workingday': 'category',",
              "                                  'weather': 'category'},",
              "                           parse_dates=['datetime'])",
              "    df_test = pd.read_csv(\"../data/bike-sharing-demand/test.csv\", ",
              "                           dtype={'season': 'category',",
              "                                  'holiday': 'category',",
              "                                  'workingday': 'category',",
              "                                  'weather': 'category'},",
              "                          parse_dates=['datetime'])",
              "    X_train = df_train.drop([\"datetime\", \"registered\", \"casual\", \"count\"], axis=1)",
              "    y_train = df_train[[\"count\"]]",
              "    X_test = df_test.drop([\"datetime\"], axis=1)",
              "    return X_train, y_train, X_test",
              "",
              "",
              "class Objective(object):",
              "    def __init__(self, args):",
              "        self.params = {",
              "            'boosting_type': 'gbdt',",
              "            'metric': 'rmse',",
              "            'objective': 'regression',  # 'binary'",
              "            'n_jobs': -1,",
              "            'seed': 236,",
              "        }",
              "        self.X_train, self.y_train, self.X_test = get_dataset()",
              "        self.n_samples = self.X_train.shape[0]",
              "        ",
              "    def _get_params(self, trial):",
              "        params = self.params.copy()  # type: Dict[str, Any]",
              "        params[\"feature_fraction\"] = trial.suggest_discrete_uniform(",
              "            \"feature_fraction\", 0.1, 1.0, 0.05",
              "        )",
              "        params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 7)",
              "        params[\"num_leaves\"] = trial.suggest_int(",
              "            \"num_leaves\", 2, 2 ** params[\"max_depth\"]",
              "        )",
              "        # See https://github.com/Microsoft/LightGBM/issues/907",
              "        params[\"min_data_in_leaf\"] = trial.suggest_int(",
              "            \"min_data_in_leaf\",",
              "            1,",
              "            max(1, int(self.n_samples / params[\"num_leaves\"])),",
              "        )",
              "        params[\"lambda_l1\"] = trial.suggest_loguniform(",
              "            \"lambda_l1\", 1e-09, 10.0",
              "        )",
              "        params[\"lambda_l2\"] = trial.suggest_loguniform(",
              "            \"lambda_l2\", 1e-09, 10.0",
              "        )",
              "",
              "        if params[\"boosting_type\"] != \"goss\":",
              "            params[\"bagging_fraction\"] = trial.suggest_discrete_uniform(",
              "                \"bagging_fraction\", 0.5, 0.95, 0.05",
              "            )",
              "            params[\"bagging_freq\"] = trial.suggest_int(",
              "                \"bagging_freq\", 1, 10",
              "            )",
              "",
              "        return params",
              "    ",
              "    def objectives(self, trial):",
              "        self.params.update(self._get_params(trial))",
              "        train_params = {",
              "            \"num_boost_round\": 10000,",
              "            \"early_stopping_rounds\": 1000,",
              "            \"nfold\": 4,",
              "            \"verbose_eval\": -1,",
              "        }",
              "        # モデルの作成",
              "        lgb_train = lgb.Dataset(self.X_train, self.y_train)",
              "        cv_results = lgb.cv(self.params,",
              "                            lgb_train,",
              "                            **train_params)",
              "        metric_mean = cv_results[f\"{self.params['metric']}-mean\"][-1]",
              "        return metric_mean",
              "",
              "    def __call__(self, trial):",
              "        \"\"\" Objective function for optuna \"\"\"",
              "        try: # optuna v0.18以上だとtryで囲まないとエラーでtrial落ちる",
              "            min_eval_metric= self.objectives(trial)",
              "            return min_eval_metric",
              "        except Exception as e:",
              "            traceback.print_exc()  # Exceptionが発生した際に表示される全スタックトレース表示",
              "            return e  # 例外を返さないとstudy.csvにエラー内容が記載されない",
              "        ",
              "    ",
              "def get_args():",
              "    ap = argparse.ArgumentParser()",
              "    ap.add_argument(\"-o\", \"--output_dir\", type=str, default='output', help=\"output dir path\")",
              "    ap.add_argument(\"-i\", \"--input_dir\", type=str, default='input', help=\"input dir path\")",
              "    ap.add_argument(\"-n_t\", \"--n_trials\", type=int, default=10, help=\"Optuna num trials\")",
              "    ap.add_argument(\"-s\", \"--study_name\", type=str, default='study', help=\"Optuna trials study name\")",
              "    #args = vars(ap.parse_args())",
              "    args = vars(ap.parse_args(args=[]))  # notebookで argparseそのままで実行する方法",
              "    return args",
              "",
              "    ",
              "if __name__ == '__main__':",
              "    args = get_args()",
              "    os.makedirs(args[\"output_dir\"], exist_ok=True)",
              "    study_name = args[\"study_name\"]  # Unique identifier of the study.",
              "    study = optuna.create_study(study_name=study_name, ",
              "                                storage=f\"sqlite:///{args['output_dir']}/{study_name}.db\", ",
              "                                load_if_exists=True)",
              "    study.optimize(Objective(args), n_trials=args[\"n_trials\"])",
              "    study.trials_dataframe().to_csv(f\"{args['output_dir']}/{study_name}_history.csv\", index=False)",
              "    print(f\"\\nstudy.best_params:\\n{study.best_params}\")",
              "    print(f\"\\nstudy.best_trial:\\n{study.best_trial}\")",
            ],
          },
        ],
      },
      {
        'name': 'sklearn',
        'sub-menu': [
          {
            'name': 'train_test_split',
            'snippet': [
              "import seaborn as sns",
              "from sklearn.model_selection import train_test_split",
              "df = sns.load_dataset('iris')",
              "(X_train, X_test, y_train, y_test) = train_test_split(df.iloc[:, [0, 1, 2, 3]], df['species'], test_size=0.3, random_state=71)",
            ],
          },
          {
            'name': 'RidgeRegression',
            'snippet': [
              "from sklearn.linear_model import Ridge",
              "import numpy as np",
              "n_samples, n_features = 1000, 50",
              "rng = np.random.RandomState(0)",
              "y = rng.randn(n_samples)",
              "X = rng.randn(n_samples, n_features)",
              "reg = Ridge(alpha=1.0)",
              "reg.fit(X, y)",
              "reg.score(X, y)  # 決定係数",
            ],
          },
          {
            'name': 'LogisticRegression',
            'snippet': [
              "from sklearn.linear_model import LogisticRegression",
              "lr = LogisticRegression()",
              "lr.fit(X_train, y_train)",
              "print(lr.score(X_test, y_test))",
              "y_predict = lr.predict(X_test)",
            ],
          },
          {
            'name': 'RidgeClassifier',
            'snippet': [
              "from sklearn.datasets import load_breast_cancer",
              "from sklearn.linear_model import RidgeClassifier",
              "X, y = load_breast_cancer(return_X_y=True)",
              "clf = RidgeClassifier().fit(X, y)",
              "clf.score(X, y)  # mean accuracy ",
            ],
          },
          {
            'name': 'SGDClassifier',
            'snippet': [
              "from sklearn.linear_model import SGDClassifier",
              "sgd = SGDClassifier(loss=\"hinge\", penalty=\"l2\")",
              "sgd.fit(X_train, y_train)",
              "print(sgd.score(X_test, y_test))",
              "y_predict = sgd.predict(X_test)"
            ],
          },
          {
            'name': 'RandomForestClassifier',
            'snippet': [
              "from sklearn.metrics import accuracy_score",
              "from sklearn.ensemble import RandomForestClassifier",
              "clf = RandomForestClassifier(random_state=0)",
              "clf = clf.fit(X_train, y_train)",
              "pred = clf.predict(X_test)",
              "print(accuracy_score(pred, y_test))",
            ],
          },
          {
            'name': 'StackingClassifier',
            'snippet': [
              "from sklearn.ensemble import StackingClassifier, StackingRegressor",
              "",
              "from sklearn.datasets import load_digits",
              "from sklearn.model_selection import train_test_split",
              "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor",
              "from sklearn.linear_model import LogisticRegression, Ridge, RidgeClassifier",
              "from sklearn.svm import LinearSVC",
              "from sklearn.preprocessing import StandardScaler",
              "from sklearn.pipeline import make_pipeline",
              "from lightgbm import LGBMClassifier, LGBMRegressor",
              "",
              "# データの準備",
              "X, y = load_digits(return_X_y=True)",
              "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42,)",
              "",
              "# 1段目モデル",
              "estimators = [",
              "    (\"rf\", RandomForestClassifier(n_estimators=10, random_state=42)),",
              "    (\"svr\", make_pipeline(StandardScaler(), LinearSVC(max_iter=4000, random_state=42))),",
              "    (\"lgb\", LGBMClassifier(random_state=42)),",
              "]",
              "clf = StackingClassifier(",
              "    estimators=estimators,",
              "    final_estimator=LGBMClassifier(random_state=111),  # final_estimatorが2段目モデル",
              "    #final_estimator=LogisticRegression(max_iter=600),  # 初期値の max_iter では収束しなかったので大きめの値を設定",
              "    n_jobs=-1,",
              ")",
              "clf.fit(X_train, y_train)",
              "print(\"正解率:\", clf.score(X_test, y_test))",
            ],
          },
          {
            'name': 'StackingRegressor',
            'snippet': [
              "from sklearn.ensemble import StackingClassifier, StackingRegressor",
              "",
              "from sklearn.datasets import load_digits",
              "from sklearn.model_selection import train_test_split",
              "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor",
              "from sklearn.linear_model import LogisticRegression, Ridge, RidgeClassifier",
              "from sklearn.svm import LinearSVC, LinearSVR",
              "from sklearn.preprocessing import StandardScaler",
              "from sklearn.pipeline import make_pipeline",
              "from lightgbm import LGBMClassifier, LGBMRegressor",
              "",
              "# データの準備",
              "import pandas as pd",
              "import seaborn as sns",
              "df = sns.load_dataset(\"titanic\")",
              "df = df.drop([\"alive\"], axis=1)",
              "for col in [\"sex\", \"embarked\", \"who\", \"embark_town\", \"class\", \"adult_male\", \"alone\", \"deck\"]:",
              "    df[col], uni = pd.factorize(df[col])",
              "# 欠損列削除",
              "cols_with_missing = [col for col in df.columns if df[col].isnull().any()]",
              "df = df.drop(cols_with_missing, axis=1)",
              "y = df[\"fare\"]",
              "X = df.drop([\"fare\"], axis=1)",
              "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42,)",
              "",
              "# 1段目モデル",
              "estimators = [",
              "    (\"rf\", RandomForestRegressor(n_estimators=10, random_state=42)),",
              "    (\"svr\", make_pipeline(StandardScaler(), LinearSVR(max_iter=4000, random_state=42))),",
              "    (\"lgb\", LGBMRegressor(random_state=42)),",
              "]",
              "clf = StackingRegressor(",
              "    estimators=estimators,",
              "    final_estimator=LGBMRegressor(random_state=111),  # final_estimatorが2段目モデル",
              "    n_jobs=-1,",
              ")",
              "clf.fit(X_train, y_train)",
              "print(\"決定係数:\", clf.score(X_test, y_test))",
            ],
          },
          {
            'name': 'MultiOutputClassifier',
            'snippet': [
              "# マルチラベルでモデル作成",
              "import numpy as np",
              "from sklearn.datasets import make_multilabel_classification",
              "from sklearn.multioutput import MultiOutputClassifier  # マルチラベルにできるがfeature_importanceが使えなくなる",
              "from sklearn.neighbors import KNeighborsClassifier",
              "import lightgbm as lgb",
              "from sklearn.pipeline import make_pipeline",
              "from sklearn.preprocessing import StandardScaler",
              "from sklearn.svm import SVC",
              "",
              "X, y = make_multilabel_classification(n_classes=3, random_state=0)",
              "#print(X)",
              "#print(y)",
              "print(X.shape, y.shape)",
              "",
              "#clf = MultiOutputClassifier(KNeighborsClassifier()).fit(X, y)  # この書き方でもいける",
              "",
              "# この書き方でもいける",
              "model = lgb.LGBMClassifier()",
              "clf = MultiOutputClassifier(model, n_jobs=-1)",
              "clf.fit(X, y)",
              "",
              "# この書き方でもいける",
              "model = make_pipeline(StandardScaler(), SVC(probability=True))",
              "clf = MultiOutputClassifier(model, n_jobs=-1)",
              "clf.fit(X, y)",
              "",
              "pred = clf.predict(X[-5:])",
              "print(pred.shape)",
              "print(pred)",
              "",
              "pred = np.array(clf.predict_proba(X[-5:]))[:,:,1].T",
              "print(pred.shape)",
              "print(pred)",
            ],
          },
          {
            'name': 'MultiOutputRegressor',
            'snippet': [
              "import numpy as np",
              "from sklearn.datasets import load_linnerud",
              "from sklearn.multioutput import MultiOutputRegressor  # マルチラベルにできるがfeature_importanceが使えなくなる",
              "from sklearn.linear_model import Ridge",
              "",
              "X, y = load_linnerud(return_X_y=True)",
              "",
              "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(X, y)",
              "pred = clf.predict(X[-5:])",
              "print(pred.shape)",
              "print(pred)",
              "",
              "clf = MultiOutputRegressor(lgb.LGBMRegressor()).fit(X, y)",
              "pred = clf.predict(X[-5:])",
              "print(pred.shape)",
              "print(pred)",
            ],
          },
          {
            'name': 'gridsearch_class_df',
            'snippet': [
              "def gridsearch_class_df(df_x, df_y,",
              "                        sklearn_class,",
              "                        tuned_parameters: list,",
              "                        test_size=0.3,",
              "                        cv=5,",
              "                        random_state=71,",
              "                        is_plot=True",
              "                        ):",
              "    \"\"\"",
              "    sklearnの分類モデルをGridSearch",
              "    Usage:",
              "        import warnings",
              "        warnings.filterwarnings(\"ignore\")",
              "        import numpy as np",
              "        import pandas as pd",
              "        from sklearn.neighbors import KNeighborsClassifier",
              "        from sklearn.linear_model import LogisticRegression",
              "        from sklearn.tree import DecisionTreeClassifier",
              "        from sklearn.svm import SVC",
              "        import lightgbm as lgb",
              "        from sklearn.preprocessing import StandardScaler  # 平均0、分散1に変換する正規化（標準化）",
              "        from sklearn.preprocessing import MinMaxScaler  # 最小値0、最大値1にする正規化",
              "        import seaborn as sns",
              "        iris = sns.load_dataset(\"iris\")",
              "        display(iris.head())",
              "        data_all = iris.copy()",
              "        df_x = data_all.drop(\"species\", axis=1)  # 説明変数",
              "        df_x = pd.DataFrame(StandardScaler().fit_transform(df_x))  # 標準化（決定木ベースのモデルについては規格化必要ない）",
              "        df_y = data_all[[\"species\"]]  # 目的変数",
              "        # K近傍法",
              "        sklearn_class = KNeighborsClassifier()",
              "        tuned_parameters = [{#\"n_neighbors\" : [7],  # 近傍数",
              "                             \"n_neighbors\" : np.arange(1, 20, 2),  # 近傍数",
              "                             \"weights\"     : [\"distance\"]  # データ間の距離の指標",
              "                            }]",
              "        # ロジスティック回帰",
              "        sklearn_class = LogisticRegression()",
              "        tuned_parameters = [{#\"C\"                 : [1.0],  # Cは正則化(penalty)の度合いのパラメータ",
              "                             \"C\"                 : pow(10, 1.0 * np.arange(-5, 5)),  # Cは正則化(penalty)の度合いのパラメータ",
              "                             \"penalty\"           : [\"l2\"],  # penaltyは正則化項。l1がLasso回帰(L1正則化)、l2ならRidge回帰(L2正則化)",
              "                             \"fit_intercept\"     : [False]",
              "                            }]",
              "        # 決定木",
              "        sklearn_class = DecisionTreeClassifier()",
              "        #tuned_parameters = [{\"max_depth\" : [3]}]  # 木の深さ",
              "        tuned_parameters = [{\"max_depth\" : np.arange(2, 10)}]  # 木の深さ",
              "        # SVM",
              "        sklearn_class = SVC(probability=True)",
              "        # パラメータのカーネルは RBFカーネル で固定とする",
              "        # コストが小さい場合には数点の誤分類は許容し、コストが大きい場合にはなるべく誤分類がないように分類していきます。",
              "        # 複雑さが小さい場合には簡単な境界線(直線)になり、複雑さが大きい場合には複雑な境界面になります。",
              "        tuned_parameters = [{#\"C\"     : [1],  # C:誤分類に対するコスト(ペナルティ)",
              "                             \"C\"     : pow(10 , 1.0 * np.arange(-4, 6, 2) ),",
              "                             #\"gamma\" : [0.01]  # gamma:分割面の複雑さを表す(大きいほど複雑)",
              "                             \"gamma\" : pow(10 , 1.0 * np.arange(-4, 6, 2))",
              "                            }]",
              "        # lightGBM",
              "        tuned_parameters = [{",
              "            'learning_rate': [0.1, 0.01],",
              "            #'feature_fraction':  np.linspace(0.1, 1.0, 1.0 // 0.05),",
              "            #'max_depth': np.linspace(1, 7, 7),",
              "            #'num_leaves ': np.linspace(2, 50, 10),",
              "            #'min_data_in_leaf': np.linspace(1, 50, 10),",
              "            #'lambda_l1': np.linspace(1e-09, 10.0, 11),",
              "            #'lambda_l2': np.linspace(1e-09, 10.0, 11),",
              "        }]",
              "        sklearn_class = lgb.LGBMClassifier(random_state=0)",
              "",
              "        gs_clf = gridsearch_class_df(df_x, df_y, sklearn_class, tuned_parameters)",
              "    \"\"\"",
              "    import numpy as np",
              "    import matplotlib.pyplot as plt",
              "    from sklearn import model_selection, metrics, tree",
              "    from sklearn.model_selection import train_test_split",
              "    from sklearn.metrics import precision_recall_curve, auc",
              "    import pydotplus",
              "    # 教師データとテストデータの分割",
              "    train_x, test_x, train_y, test_y = train_test_split(df_x, df_y,",
              "                                                        test_size=test_size,",
              "                                                        random_state=random_state)",
              "    # パラメータのセット",
              "    # CVは目安ではありますが、分割数の上限としては、擬似的な テストデータ の数が20行(レコード)を下回らないように調整するとよい",
              "    gs_clf = model_selection.GridSearchCV(sklearn_class,  # model指定",
              "                                          param_grid=tuned_parameters,  # ハイパーパラメータ",
              "                                          cv=cv,  # 交差検証を指定",
              "                                          )",
              "    # 学習",
              "    gs_clf.fit(train_x, train_y)",
              "    # スコア(正確度)",
              "    print(\"\スコア\")",
              "    print(gs_clf.best_score_)",
              "    # 評価",
              "    # テストデータ に対する予測値を算出",
              "    # 予測値をpred列, 各クラスの予測確率をpred_proba_列としてtest_yに追加する",
              "    pred = gs_clf.best_estimator_.predict(test_x)",
              "    test_y['pred'] = pred",
              "    for i, class_name in enumerate(gs_clf.classes_):",
              "        test_y[f'pred_proba_{class_name}'] = gs_clf.best_estimator_.predict_proba(test_x)[:, i]",
              "    print(test_y.head())",
              "    # 混同行列",
              "    cm = metrics.confusion_matrix(test_y.iloc[:, 0], test_y[\"pred\"])",
              "    print(\"\混同行列\")",
              "    print(cm)",
              "    # 正確率",
              "    print(\"\accuracy:\")",
              "    print(cm.diagonal().sum() / cm.sum())",
              "    # AUC",
              "    print(\"\AUC\")",
              "    for i, class_name in enumerate(gs_clf.classes_):",
              "        precision, recall, thresholds = precision_recall_curve(test_y.iloc[:, 0],",
              "                                                               test_y[f\"pred_proba_{class_name}\"],",
              "                                                               pos_label=class_name)",
              "        print(class_name + ':', auc(recall, precision))",
              "    # best param",
              "    print(f\"\{sklearn_class.__class__.__name__} best param\")",
              "    print(gs_clf.best_params_)",
              "    if sklearn_class.__class__.__name__ == \"LogisticRegression\":",
              "        # ロジスティクス回帰について",
              "        print(\"\オッズ比の確認\")",
              "        print(list(df_x.columns))",
              "        print(np.exp(gs_clf.best_estimator_.coef_))",
              "    if sklearn_class.__class__.__name__ == \"DecisionTreeClassifier\":",
              "        # 決定木についてGraphviz描画用ファイルの出力",
              "        tree.export_graphviz(gs_clf.best_estimator_,  # model",
              "                             out_file=\"dtree.dot\",  # ファイル名",
              "                             feature_names=train_x.columns,  # 特徴量名",
              "                             class_names=train_y.iloc[:,0].unique(),  # クラス名",
              "                             filled=True  # 色を塗る",
              "                             )",
              "        # dotファイルをpngに変換",
              "        graph = pydotplus.graphviz.graph_from_dot_file('dtree.dot')",
              "        graph.write_png('dtree.png')",
              "        ## jupyterで決定木の画像表示",
              "        #from IPython.display import Image, display_png",
              "        #display_png(Image(graph.create_png()))",
              "    return gs_clf",
            ],
          },
          {
            'name': 'gridsearch_reg_df',
            'snippet': [
              "def gridsearch_reg_df(df_x, df_y,",
              "                      sklearn_reg,",
              "                      tuned_parameters: list,",
              "                      test_size=0.3,",
              "                      cv=5,",
              "                      scoring='neg_mean_squared_error',",
              "                      random_state=17,",
              "                      is_plot=True",
              "                      ):",
              "    \"\"\"",
              "    sklearnの回帰モデルをGridSearch",
              "    Usage:",
              "        import numpy as np",
              "        import pandas as pd",
              "        import lightgbm as lgb",
              "        from sklearn.linear_model import LinearRegression, ElasticNet",
              "        from sklearn.preprocessing import StandardScaler  # 平均0、分散1に変換する正規化（標準化）",
              "        from sklearn.preprocessing import MinMaxScaler  # 最小値0、最大値1にする正規化",
              "        import seaborn as sns",
              "        iris = sns.load_dataset(\"iris\")",
              "        display(iris.head())",
              "        # Species列を取り除く",
              "        data_all = iris.drop(\"species\", axis=1)",
              "        df_x = data_all.drop(\"petal_length\", axis=1)  # 説明変数",
              "        df_x = pd.DataFrame(StandardScaler().fit_transform(df_x))  # 標準化",
              "        df_y = data_all[[\"petal_length\"]]  # 目的変数",
              "        #sklearn_reg = LinearRegression()  # 線形回帰モデル",
              "        #tuned_parameters = [{\"n_jobs\" : [1]}]  # ハイパーパラメータ 使うコア数",
              "        sklearn_reg = ElasticNet()  # Elastic Net（l1,l2正則化加えた線形回帰モデル）",
              "        tuned_parameters = [{\"l1_ratio\" : 0.1 * np.arange(0,11) + 0.1,  # l1正則化の大きさ  ElasticNetなので、l2正則化の大きさは 1 − l1_ratio になる",
              "                             \"alpha\"    : pow(10, -1.0 * np.arange(0,11))  # l1_ratio と 1 − l1_ratio にかける重み。デフォルトは1.0",
              "                            }]",
              "        # lightGBM",
              "        tuned_parameters = [{",
              "            'learning_rate': [0.1, 0.01],",
              "            #'feature_fraction':  np.linspace(0.1, 1.0, 1.0 // 0.05),",
              "            #'max_depth': np.linspace(1, 7, 7),",
              "            #'num_leaves ': np.linspace(2, 50, 10),",
              "            #'min_data_in_leaf': np.linspace(1, 50, 10),",
              "            #'lambda_l1': np.linspace(1e-09, 10.0, 11),",
              "            #'lambda_l2': np.linspace(1e-09, 10.0, 11),",
              "        }]",
              "        sklearn_reg = lgb.LGBMRegressor(random_state=0)",
              "        ",
              "        gs_reg = gridsearch_reg_df(df_x, df_y, sklearn_reg, tuned_parameters)",
              "    \"\"\"",
              "    import numpy as np",
              "    import matplotlib.pyplot as plt",
              "    from sklearn import model_selection",
              "    from sklearn.model_selection import train_test_split",
              "    from sklearn.metrics import mean_squared_error, mean_absolute_error",
              "    # 教師データとテストデータの分割",
              "    train_x, test_x, train_y, test_y = train_test_split(df_x, df_y,",
              "                                                        test_size=test_size,",
              "                                                        random_state=random_state)",
              "    # パラメータのセット",
              "    # CVは目安ではありますが、分割数の上限としては、擬似的な テストデータ の数が20行(レコード)を下回らないように調整するとよい",
              "    gs_reg = model_selection.GridSearchCV(sklearn_reg,  # model指定",
              "                                          param_grid=tuned_parameters,  # ハイパーパラメータ",
              "                                          cv=cv,  # 交差検証を指定",
              "                                          scoring=scoring  # GridSearchCVのscoringでMSE(平均二乗誤差)指定する場合は'neg_mean_squared_error'",
              "                                         )",
              "    # 学習",
              "    gs_reg.fit(train_x, train_y)",
              "    if sklearn_reg.__class__.__name__ in [\"LinearRegression\", \"ElasticNet\"]:",
              "        # 係数の確認",
              "        print(\"回帰式の各重み（係数）\")",
              "        print(list(df_x.columns))",
              "        print(gs_reg.best_estimator_.coef_)",
              "        print(\"Intercept（回帰式の切片）\")",
              "        print(gs_reg.best_estimator_.intercept_)",
              "    # 損失関数のスコア",
              "    print(\"loss\")",
              "    print(gs_reg.best_score_)",
              "    # 評価",
              "    # テストデータ に対する予測値を算出",
              "    # 予測値をpred列としてtest_yに追加する",
              "    pred = gs_reg.best_estimator_.predict(test_x)",
              "    test_y['pred'] = pred",
              "    print(test_y.head())",
              "    # MAE(平均絶対誤差): モデルからデータの平均距離",
              "    # MAEは誤差をそのまま使っているため、予想を外した点があってもRMSEほどは評価値に影響しない",
              "    print(\"MAE(平均絶対誤差)\")",
              "    print(mean_absolute_error(test_y.iloc[:, 0], test_y[\"pred\"]))",
              "    # RMSE(二乗平均平方根誤差): モデルからデータの平均二乗距離の平方根",
              "    # 予測を外す点を厳しく評価したい場合にはRMSEを使用したほうがよい",
              "    print(\"RMSE(二乗平均平方根誤差)\")",
              "    print(np.sqrt(mean_squared_error(test_y.iloc[:, 0], test_y[\"pred\"])))",
              "    if is_plot:",
              "        # 予測値の可視化",
              "        # 正解値と予測値を描画する(予測がぴったりならば線上に乗る)",
              "        test_y.plot.scatter(x=test_y.columns[0], y=\"pred\", figsize=(10,8))",
              "        plt.plot(np.arange(8), np.arange(8))             # y=xの線",
              "        plt.xlim(1, 7)                                   # x軸の範囲",
              "        plt.ylim(1, 7)                                   # y軸の範囲",
              "        plt.show()",
              "        plt.figure(figsize=(10,8)) # グラフのサイズ変更",
              "        plt.clf()  # メモリ解放",
              "        plt.close()",
              "    return gs_reg",
            ],
          },
          {
            'name': '複数カラムでstratified k-fold',
            'snippet': [
              "import sys",
              "sys.path.append(r'C:\\Users\\81908\\Git\\mcs_kfold')",
              "from mcs_kfold import MCSKFold",
              "",
              "import pandas as pd",
              "import matplotlib.pyplot as plt",
              "",
              "global_seed = 1001",
              "num_cv = 5",
              "max_iter = 100",
              "shuffle_mc = True",
              "mcskf = MCSKFold(n_splits=num_cv, max_iter=max_iter, shuffle_mc=shuffle_mc, global_seed=global_seed)",
              "",
              "import seaborn as sns",
              "df = sns.load_dataset(\"titanic\")",
              "df = df.drop([\"alive\"], axis=1)",
              "target_cols = [\"survived\", \"pclass\", \"sex\"]",
              "indices = mcskf.split(df=df, target_cols=target_cols)",
              "",
              "# 複数カラムでstratified k-fold",
              "for fold, (train_index, valid_index) in enumerate(indices):",
              "    df.iloc[train_index][target_cols].hist()",
            ],
          },
        ],
      },
      {
        'name': 'xgboost',
        'sub-menu': [
          {
            'name': 'XGBRegressor',
            'snippet': [
              "from xgboost import XGBRegressor, XGBClassifier",
              "from sklearn.metrics import mean_absolute_error",
              "model = XGBRegressor(random_state=0, n_estimators=100, learning_rate=0.1)",
              "model.fit(X_train, y_train)",
              "pred = model.predict(X_valid)",
              "mae = mean_absolute_error(pred, y_valid)",
            ],
          }
        ]
      },
      {
        'name': 'tf.keras',
        'sub-menu': [
          {
            'name': 'GPU使わない',
            'snippet': [
              "import os",
              "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"",
            ],
          },
          {
            'name': 'テーブルデータの二値分類model',
            'snippet': [
              "import os",
              "import random",
              "import joblib",
              "",
              "# tensorflowの警告抑制",
              "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"",
              "",
              "import numpy as np",
              "import pandas as pd",
              "import tensorflow as tf",
              "from tensorflow.keras import Sequential",
              "from tensorflow.keras.optimizers import SGD, Adam",
              "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint",
              "from tensorflow.keras.layers import (",
              "    ReLU,",
              "    PReLU,",
              "    Activation,",
              "    Dense,",
              "    Dropout,",
              "    BatchNormalization,",
              ")",
              "from tensorflow.keras.models import save_model, load_model",
              "from tensorflow.keras.utils import to_categorical",
              "from sklearn.preprocessing import *",
              "from sklearn.metrics import *",
              "from sklearn.model_selection import *",
              "import matplotlib.pyplot as plt",
              "",
              "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)",
              "",
              "",
              "class ModelNN:",
              "    def __init__(self, run_fold_name=\"\", params={}) -> None:",
              "        \"\"\"コンストラクタ",
              "        :param run_fold_name: ランの名前とfoldの番号を組み合わせた名前",
              "        :param params: ハイパーパラメータ",
              "        \"\"\"",
              "        self.run_fold_name = run_fold_name",
              "        self.params = params",
              "        self.model = None",
              "        self.scaler = None",
              "",
              "    def build_model(self, input_shape):",
              "        \"\"\"モデル構築\"\"\"",
              "        model = Sequential()",
              "        model.add(Dense(self.params[\"units\"][0], input_shape=input_shape))",
              "        model.add(PReLU())",
              "        model.add(BatchNormalization())",
              "        model.add(Dropout(self.params[\"dropout\"][0]))",
              "",
              "        for l_i in range(1, self.params[\"layers\"] - 1):",
              "            model.add(Dense(self.params[\"units\"][l_i]))",
              "            model.add(PReLU())",
              "            model.add(BatchNormalization())",
              "            model.add(Dropout(self.params[\"dropout\"][l_i]))",
              "",
              "        model.add(Dense(self.params[\"nb_classes\"]))",
              "        model.add(Activation(self.params[\"pred_activation\"]))",
              "        if self.params[\"optimizer\"] == \"adam\":",
              "            opt = Adam(learning_rate=self.params[\"learning_rate\"])",
              "        else:",
              "            opt = SGD(",
              "                learning_rate=self.params[\"learning_rate\"], momentum=0.9, nesterov=True",
              "            )",
              "",
              "        model.compile(",
              "            loss=self.params[\"loss\"], metrics=self.params[\"metrics\"], optimizer=opt,",
              "        )",
              "        self.model = model",
              "",
              "    def train(self, tr_x, tr_y, va_x=None, va_y=None):",
              "        # 乱数固定",
              "        ModelNN().set_tf_random_seed()",
              "",
              "        # 出力ディレクトリ作成",
              "        os.makedirs(self.params[\"out_dir\"], exist_ok=True)",
              "",
              "        # データのセット・スケーリング",
              "        validation = va_x is not None",
              "        scaler = self.params[\"scaler\"]  # StandardScaler()",
              "        scaler.fit(tr_x)",
              "        tr_x = scaler.transform(tr_x)",
              "        # ラベルone-hot化",
              "        tr_y = to_categorical(tr_y, num_classes=self.params[\"nb_classes\"])",
              "",
              "        # モデル構築",
              "        self.build_model((tr_x.shape[1],))",
              "        ",
              "        hist = None",
              "        if validation:",
              "            va_x = scaler.transform(va_x)",
              "            va_y = to_categorical(va_y, num_classes=self.params[\"nb_classes\"])",
              "",
              "            cb = []",
              "            cb.append(",
              "                ModelCheckpoint(",
              "                    filepath=os.path.join(",
              "                        self.params[\"out_dir\"], f\"best_val_loss_{self.run_fold_name}.h5\"",
              "                    ),",
              "                    monitor=\"val_loss\",",
              "                    save_best_only=True,",
              "                    #verbose=1,",
              "                    verbose=0,",
              "                )",
              "            )",
              "            # cb.append(ModelCheckpoint(filepath=os.path.join(self.params[\"out_dir\"], f\"best_val_acc_{self.run_fold_name}.h5\"),",
              "            #        monitor=\"val_acc\",",
              "            #        save_best_only=True,",
              "            #        verbose=1,",
              "            #        mode=\"max\",",
              "            #    )",
              "            # )",
              "            cb.append(",
              "                EarlyStopping(",
              "                    monitor=\"val_loss\", patience=self.params[\"patience\"], verbose=1",
              "                )",
              "            )",
              "            hist = self.model.fit(",
              "                tr_x,",
              "                tr_y,",
              "                epochs=self.params[\"nb_epoch\"],",
              "                batch_size=self.params[\"batch_size\"],",
              "                verbose=2,",
              "                validation_data=(va_x, va_y),",
              "                callbacks=cb,",
              "            )",
              "        else:",
              "            hist = self.model.fit(",
              "                tr_x,",
              "                tr_y,",
              "                epochs=self.params[\"nb_epoch\"],",
              "                batch_size=self.params[\"batch_size\"],",
              "                #verbose=2,",
              "                verbose=0,",
              "            )",
              "",
              "        # スケーラー保存",
              "        self.scaler = scaler",
              "        joblib.dump(",
              "            self.scaler,",
              "            os.path.join(self.params[\"out_dir\"], f\"{self.run_fold_name}-scaler.pkl\"),",
              "        )",
              "        ",
              "        # history plot",
              "        self.plot_hist_acc_loss(hist)",
              "        ",
              "        return hist",
              "",
              "    def predict_binary(self, te_x):",
              "        \"\"\"2値分類の1クラスのみ取得\"\"\"",
              "        self.load_model()",
              "        te_x = self.scaler.transform(te_x)",
              "        pred = self.model.predict(te_x)[:, 1]",
              "        return pred",
              "",
              "    def load_model(self):",
              "        model_path = os.path.join(",
              "            self.params[\"out_dir\"], f\"best_val_loss_{self.run_fold_name}.h5\"",
              "        )",
              "        # model_path = os.path.join(self.params['out_dir'], f'best_val_acc_{self.run_fold_name}.h5')",
              "        scaler_path = os.path.join(",
              "            self.params[\"out_dir\"], f\"{self.run_fold_name}-scaler.pkl\"",
              "        )",
              "        self.model = load_model(model_path)",
              "        self.scaler = joblib.load(scaler_path)",
              "        print(f\"INFO: \\nload model:{model_path} \\nload scaler: {scaler_path}\")",
              "",
              "    def plot_hist_acc_loss(self, history):",
              "        \"\"\"学習historyをplot\"\"\"",
              "        acc = history.history['acc']",
              "        val_acc = history.history['val_acc']",
              "        loss = history.history['loss']",
              "        val_loss = history.history['val_loss']",
              "        epochs = range(len(acc))",
              "",
              "        # 1) Accracy Plt",
              "        plt.plot(epochs, acc, 'bo' ,label = 'training acc')",
              "        plt.plot(epochs, val_acc, 'b' , label= 'validation acc')",
              "        plt.title('Training and Validation acc')",
              "        plt.legend()",
              "        plt.savefig(f\"{self.params['out_dir']}/{self.run_fold_name}-acc.png\", bbox_inches='tight', pad_inches=0)",
              "        plt.clf()",
              "        plt.close()",
              "",
              "        # 2) Loss Plt",
              "        plt.plot(epochs, loss, 'bo' ,label = 'training loss')",
              "        plt.plot(epochs, val_loss, 'b' , label= 'validation loss')",
              "        plt.title('Training and Validation loss')",
              "        plt.legend()",
              "        plt.savefig(f\"{self.params['out_dir']}/{self.run_fold_name}-loss.png\", bbox_inches='tight', pad_inches=0)",
              "        plt.clf()",
              "        plt.close()",
              "        ",
              "    @staticmethod",
              "    def set_tf_random_seed(seed=0):",
              "        \"\"\"",
              "        tensorflow v2.0の乱数固定",
              "        https://qiita.com/Rin-P/items/acacbb6bd93d88d1ca1b",
              "        ※tensorflow-determinism が無いとgpuについては固定できないみたい",
              "         tensorflow-determinism はpipでしか取れない($ pip install tensorflow-determinism)ので未確認",
              "        \"\"\"",
              "        ## ソースコード上でGPUの計算順序の固定を記述",
              "        # from tfdeterminism import patch",
              "        # patch()",
              "        # 乱数のseed値の固定",
              "        random.seed(seed)",
              "        np.random.seed(seed)",
              "        tf.random.set_seed(seed)  # v1.0系だとtf.set_random_seed(seed)",
              "    ",
              "def test_func():",
              "    \"\"\"",
              "    テスト駆動開発での関数のテスト関数",
              "    test用関数はpythonパッケージの nose で実行するのがおすすめ($ conda install -c conda-forge nose などでインストール必要)",
              "    →noseは再帰的にディレクトリ探索して「Test」や「test」で始まるクラスとメソッドを実行する",
              "    $ cd <このモジュールの場所>",
              "    $ nosetests -v -s --nologcapture <本モジュール>.py  # 再帰的にディレクトリ探索して「Test」や「test」で始まるクラスとメソッドを実行",
              "      -s付けるとprint()の内容出してくれる",
              "      --nologcapture付けると不要なログ出さない",
              "    \"\"\"",
              "    import pandas as pd",
              "    import seaborn as sns",
              "",
              "    def _test_train(df, feats, target_col, params):",
              "        num_folds = 2",
              "        folds = KFold(n_splits=num_folds, shuffle=True, random_state=1001)",
              "        for n_fold, (train_idx, valid_idx) in enumerate(",
              "            folds.split(df[feats], df[target_col])",
              "        ):",
              "            t_fold_df = df.iloc[train_idx]",
              "            v_fold_df = df.iloc[valid_idx]",
              "            train_x, train_y = (",
              "                t_fold_df[feats],",
              "                t_fold_df[target_col],",
              "            )",
              "            valid_x, valid_y = (",
              "                v_fold_df[feats],",
              "                v_fold_df[target_col],",
              "            )",
              "            model_cls = ModelNN(n_fold, params)",
              "            # 学習",
              "            hist = model_cls.train(train_x, train_y, valid_x, valid_y)",
              "            # 予億",
              "            print(model_cls.predict_binary(valid_x))",
              "",
              "    def _test_pred(df, feats):",
              "        # ファイルからロードして予億",
              "        params = dict(out_dir=\"tmp\")",
              "        model_cls = ModelNN(0, params)",
              "        model_cls.load_model()",
              "        model_cls.predict_binary(df[feats])",
              "",
              "    df = sns.load_dataset(\"titanic\")",
              "    df = df.drop([\"alive\"], axis=1)",
              "    # 欠損最頻値で補完",
              "    for col in [col for col in df.columns if df[col].isnull().any()]:",
              "        column_mode = df[col].mode()[0]",
              "        df[col].fillna(column_mode, inplace=True)",
              "",
              "    for col in [",
              "        \"sex\",",
              "        \"embarked\",",
              "        \"who\",",
              "        \"embark_town\",",
              "        \"class\",",
              "        \"adult_male\",",
              "        \"alone\",",
              "        \"deck\",",
              "    ]:",
              "        df[col], uni = pd.factorize(df[col])",
              "    target_col = \"survived\"",
              "    feats = df.columns.to_list()",
              "    feats.remove(target_col)",
              "",
              "    params = dict(",
              "        out_dir=\"tmp\",",
              "        scaler=StandardScaler(),",
              "        layers=3,",
              "        units=[128, 64, 32],",
              "        dropout=[0.3, 0.3, 0.3],",
              "        nb_classes=2,",
              "        pred_activation=\"softmax\",",
              "        loss=\"categorical_crossentropy\",",
              "        optimizer=\"adam\",",
              "        learning_rate=0.001,",
              "        metrics=[\"acc\"],",
              "        nb_epoch=10,",
              "        patience=5,",
              "        batch_size=256,",
              "    )",
              "",
              "    _test_train(df, feats, target_col, params)",
              "    _test_pred(df, feats)",
              "",
              "test_func()",
            ]
          },
        ]
      },
      {
        'name': 'xfeat',
        'sub-menu': [
          {
            'name': '数値列算術計算',
            'snippet': [
              "import sys",
              "sys.path.append(r\"C:\\Users\\81908\\Git\\xfeat\")",
              "import xfeat",
              "from xfeat import *",
              "from xfeat.selector import *",
              "from xfeat.utils import *",
              "",
              "import os",
              "import numpy as np",
              "import pandas as pd",
              "from sklearn.model_selection import *",
              "# サンプルデータ",
              "import seaborn as sns",
              "df = sns.load_dataset(\"titanic\")",
              "",
              "# 数値列算術計算",
              "df_num = Pipeline(",
              "    [",
              "        SelectNumerical(),  # 数値列のみ選択",
              "        ArithmeticCombinations(  # 数値列同士算術計算",
              "            exclude_cols=[\"survived\"],  # 除外する列",
              "            drop_origin=True,  # 元の列削除。残す場合はFalse",
              "            operator=\"+\",  # 足し算する。*や-もできる",
              "            r=2,  # 2列単位で算術計算。4つの項目から2つを選ぶ場合は　4C2 = 6　6通りが出力",
              "            output_suffix=\"_plus\",  # 列名のサフィックス指定",
              "        ),",
              "    ]",
              ").fit_transform(df)",
              "df = pd.concat([df, df_num], axis=1)",
              "df",
            ]
          },
          {
            'name': '文字列組み合わせ',
            'snippet': [
              "import sys",
              "sys.path.append(r\"C:\\Users\\81908\\Git\\xfeat\")",
              "import xfeat",
              "from xfeat import *",
              "from xfeat.selector import *",
              "from xfeat.utils import *",
              "",
              "import os",
              "import numpy as np",
              "import pandas as pd",
              "from sklearn.model_selection import *",
              "",
              "# サンプルデータ",
              "import seaborn as sns",
              "df = sns.load_dataset(\"titanic\")",
              "",
              "# カテゴリ型の列をobject型に変換（これしないとConcatCombination()エラーになる）",
              "df = df.apply(lambda x: x.astype(str) if x.dtype.name == \"category\" else x)",
              "# 文字列組み合わせ",
              "df_cate = Pipeline(",
              "    [",
              "        SelectCategorical(   # カテゴリ列のみ選択",
              "            exclude_cols=[\"alive\"]  # 除外する列",
              "        ), ",
              "        ConcatCombination(  # カテゴリ列同士組み合わせ",
              "            drop_origin=True,  # 元の列削除。残す場合はFalse",
              "            r=2,  # 2列単位で組み合わせ。4つの項目から2つを選ぶ場合は　4C2 = 6　6通りが出力",
              "            output_suffix=\"\",  # 列名のサフィックス指定",
              "            fillna=np.nan  # 結合した値が欠損のときに入れる値。デフォルトは\"_NAN_\"が入る",
              "        ),",
              "        LabelEncoder(output_suffix=\"\"),  # ラベルエンコディング",
              "    ]",
              ").fit_transform(df)",
              "df = pd.concat([df, df_cate], axis=1)",
              "df",
            ]
          },
          {
            'name': 'ターゲットエンコディング',
            'snippet': [
              "import sys",
              "sys.path.append(r\"C:\\Users\\81908\\Git\\xfeat\")",
              "import xfeat",
              "from xfeat import *",
              "from xfeat.selector import *",
              "from xfeat.utils import *",
              "",
              "import os",
              "import numpy as np",
              "import pandas as pd",
              "from sklearn.model_selection import *",
              "",
              "# サンプルデータ",
              "import seaborn as sns",
              "df = sns.load_dataset(\"titanic\")",
              "",
              "# ターゲットエンコディング",
              "fold = KFold(n_splits=5, shuffle=False)",
              "df = TargetEncoder(input_cols=[\"embarked\", \"age\", \"pclass\"], ",
              "                        target_col=\"survived\",",
              "                        fold=fold).fit_transform(df)",
              "df",
            ]
          },
          {
            'name': 'groupbyで統計量列追加',
            'snippet': [
              "import sys",
              "sys.path.append(r\"C:\\Users\\81908\\Git\\xfeat\")",
              "import xfeat",
              "from xfeat import *",
              "from xfeat.selector import *",
              "from xfeat.utils import *",
              "",
              "import os",
              "import numpy as np",
              "import pandas as pd",
              "from sklearn.model_selection import *",
              "",
              "# サンプルデータ",
              "import seaborn as sns",
              "df = sns.load_dataset(\"titanic\")",
              "",
              "# groupbyで統計量列追加",
              "df = aggregation(df,",
              "                 group_key=\"embarked\",",
              "                 group_values=[\"fare\", \"age\"],",
              "                 agg_methods=[\"sum\", \"min\", \"max\", \"mean\", \"median\", np.ptp]",
              "                 )[0]",
              "df",
            ]
          },
        ]
      },
      {
        'name': '次元削減',
        'sub-menu': [
          {
            'name': 'pca',
            'snippet': [
              "from sklearn.decomposition import PCA",
              "from sklearn.preprocessing import StandardScaler, MinMaxScaler",
              "from sklearn.model_selection import train_test_split",
              "",
              "def standarized(train_x, test_x):",
              "    \"\"\"標準化を行った学習データとテストデータを返す関数\"\"\"",
              "    scaler = StandardScaler()",
              "    scaler.fit(train_x)",
              "    train_x = scaler.transform(train_x)",
              "    test_x = scaler.transform(test_x)",
              "    return pd.DataFrame(train_x), pd.DataFrame(test_x)",
              "",
              "def run_pca(train_x, test_x, n_components=5):",
              "    \"\"\"pca実行。行数*n_components次元のデータフレームを返す\"\"\"",
              "    pca = PCA(n_components=n_components, random_state =71)",
              "    pca.fit(train_x)",
              "    train_x = pca.transform(train_x)",
              "    test_x = pca.transform(test_x)",
              "    return pd.DataFrame(train_x), pd.DataFrame(test_x)",
              "",
              "",
              "if __name__ == '__main__':",
              "    import pandas as pd",
              "    import seaborn as sns",
              "    df = sns.load_dataset(\"titanic\")",
              "    df = df.drop([\"alive\"], axis=1)",
              "    for col in [\"sex\", \"embarked\", \"who\", \"embark_town\", \"class\", \"adult_male\", \"alone\", \"deck\"]:",
              "        df[col], uni = pd.factorize(df[col])",
              "        ",
              "    # 最頻値で欠損補完",
              "    cols_with_missing = [col for col in df.columns if df[col].isnull().any()]",
              "    for col in cols_with_missing:",
              "        column_mode = df[col].mode()[0]",
              "        df[col].fillna(column_mode, inplace=True)",
              "    ",
              "    y = df[\"survived\"]",
              "    X = df.drop(\"survived\", axis=1)",
              "    (X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=0.3, random_state=71)",
              "    ",
              "    # 標準化",
              "    X_train, X_test = standarized(X_train, X_test)",
              "    ",
              "    # データは標準化などのスケールを揃える前処理が行われているものとする",
              "    X_train, X_test = run_pca(X_train, X_test)",
              "    display(X_train)",
            ]
          },
          {
            'name': 'NMF',
            'snippet': [
              "from sklearn.decomposition import *",
              "from sklearn.preprocessing import StandardScaler, MinMaxScaler",
              "from sklearn.model_selection import train_test_split",
              "",
              "def minmax_scaled(train_x, test_x):",
              "    \"\"\"MinMaxスケーリングを行った学習データとテストデータを返す関数\"\"\"",
              "    scaler = MinMaxScaler()",
              "    scaler.fit(pd.concat([train_x, test_x], axis=0))",
              "    train_x = scaler.transform(train_x)",
              "    test_x = scaler.transform(test_x)",
              "    return pd.DataFrame(train_x), pd.DataFrame(test_x)",
              "",
              "def run_nmf(train_x, test_x, n_components=5):",
              "    \"\"\"NMF実行。行数*n_components次元のデータフレームを返す。非負の行列に近似する\"\"\"",
              "    nmf = NMF(n_components=n_components, init='random', random_state =71)",
              "    nmf.fit(train_x)",
              "    train_x = nmf.transform(train_x)",
              "    test_x = nmf.transform(test_x)",
              "    return pd.DataFrame(train_x), pd.DataFrame(test_x)",
              "",
              "",
              "if __name__ == '__main__':",
              "    import pandas as pd",
              "    import seaborn as sns",
              "    df = sns.load_dataset(\"titanic\")",
              "    df = df.drop([\"alive\"], axis=1)",
              "    for col in [\"sex\", \"embarked\", \"who\", \"embark_town\", \"class\", \"adult_male\", \"alone\", \"deck\"]:",
              "        df[col], uni = pd.factorize(df[col])",
              "        ",
              "    # 最頻値で欠損補完",
              "    cols_with_missing = [col for col in df.columns if df[col].isnull().any()]",
              "    for col in cols_with_missing:",
              "        column_mode = df[col].mode()[0]",
              "        df[col].fillna(column_mode, inplace=True)",
              "    ",
              "    y = df[\"survived\"]",
              "    X = df.drop(\"survived\", axis=1)",
              "    (X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=0.3, random_state=71)",
              "    ",
              "    # MinMaxスケーリング",
              "    X_train, X_test = minmax_scaled(X_train, X_test)",
              "    ",
              "    # 非負の値とするため、MinMaxスケーリングを行ったデータを用いる",
              "    X_train, X_test = run_nmf(X_train, X_test)",
              "    display(X_train)",
            ]
          },
          {
            'name': 'tsne',
            'snippet': [
              "from sklearn.manifold import TSNE",
              "from sklearn.preprocessing import StandardScaler, MinMaxScaler",
              "from sklearn.model_selection import train_test_split",
              "",
              "def standarized(train_x, test_x):",
              "    \"\"\"標準化を行った学習データとテストデータを返す関数\"\"\"",
              "    scaler = StandardScaler()",
              "    scaler.fit(train_x)",
              "    train_x = scaler.transform(train_x)",
              "    test_x = scaler.transform(test_x)",
              "    return pd.DataFrame(train_x), pd.DataFrame(test_x)",
              "",
              "def run_tsne(data, n_components=2):",
              "    \"\"\"tsne実行。行数*n_components次元のデータフレームを返す\"\"\"",
              "    tsne = TSNE(n_components=n_components, random_state =71)",
              "    return pd.DataFrame(tsne.fit_transform(data))",
              "",
              "",
              "if __name__ == '__main__':",
              "    import pandas as pd",
              "    import seaborn as sns",
              "    df = sns.load_dataset(\"titanic\")",
              "    df = df.drop([\"alive\"], axis=1)",
              "    for col in [\"sex\", \"embarked\", \"who\", \"embark_town\", \"class\", \"adult_male\", \"alone\", \"deck\"]:",
              "        df[col], uni = pd.factorize(df[col])",
              "        ",
              "    # 最頻値で欠損補完",
              "    cols_with_missing = [col for col in df.columns if df[col].isnull().any()]",
              "    for col in cols_with_missing:",
              "        column_mode = df[col].mode()[0]",
              "        df[col].fillna(column_mode, inplace=True)",
              "    ",
              "    y = df[\"survived\"]",
              "    X = df.drop(\"survived\", axis=1)",
              "    (X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=0.3, random_state=71)",
              "    ",
              "    # 標準化",
              "    X_train, X_test = standarized(X_train, X_test)",
              "    ",
              "    # データは標準化などのスケールを揃える前処理が行われているものとする",
              "    data = pd.concat([X_train, X_test])",
              "    data = run_tsne(data)",
              "    display(data)"
            ]
          },
          {
            'name': 'umap_tsne_scatter',
            'snippet': [
              "def umap_tsne_scatter(",
              "    x_array,",
              "    y=None,",
              "    out_png=\"umap_scatter.png\",",
              "    random_state=42,",
              "    is_umap=True,",
              "    point_size=None,",
              "    is_axis_off=True,",
              "    is_show=True,",
              "    n_neighbors=15,",
              "    min_dist=0.1,",
              "    perplexity=30.0,",
              "):",
              "    \"\"\"",
              "    umap/tsneで次元削減した画像出力",
              "    Args:",
              "        x_array: np.array型のn次元の特徴量",
              "        y: 特徴量のラベル",
              "        out_png: umap/tsneの出力画像のパス",
              "        random_state: 乱数シード",
              "        is_umap: Trueならumapで次元削減。Falseならtsneで次元削減",
              "        point_size: plot点の大きさ",
              "        is_axis_off: Trueなら画像のx,y軸表示させない",
              "        is_show: Trueなら次元削減した画像plt.show()しない",
              "        n_neighbors: umapのパラメータ。大きくするとマクロな、小さくするとミクロな構造を振る舞いを反映させる。t-SNEでいうところのperplexityのような値だと思われる。2~100が推奨されている。",
              "        min_dist: umapのパラメータ。同一クラスタとして判定する距離。値大きいとplot点広がる。0.0~0.99。",
              "        perplexity: tsneのパラメータ。投射した点の集まり方の密さを表すもの。この値は5～50が理論値で、低いほうが点が密になりやすく、高いとゆるいプロットになり",
              "    \"\"\"",
              "    import umap",
              "    from sklearn.manifold import TSNE",
              "    import matplotlib.pyplot as plt",
              "    import matplotlib.cm as cm",
              "",
              "    if is_umap == True:",
              "        embedding = umap.UMAP(",
              "            random_state=random_state, n_neighbors=n_neighbors, min_dist=min_dist",
              "        ).fit_transform(x_array)",
              "    else:",
              "        tsne_model = TSNE(",
              "            n_components=2, random_state=random_state, perplexity=perplexity",
              "        )",
              "        embedding = tsne_model.fit_transform(x_array)",
              "",
              "    if y is None:",
              "        plt.scatter(embedding[:, 0], embedding[:, 1], s=point_size)",
              "    else:",
              "        # ラベル:y指定してplot点の色変える",
              "        plt.scatter(embedding[:, 0], embedding[:, 1], c=y, s=point_size, cmap=cm.tab10)",
              "        plt.colorbar()",
              "    if is_axis_off == True:",
              "        plt.axis(\"off\")  # x,y軸表示させない",
              "    if out_png is not None:",
              "        plt.savefig(out_png)",
              "    if is_show == True:",
              "        plt.show()",
              "    plt.clf()  # plotの設定クリアにする",
              "    return embedding",
              "",
              "",
              "if __name__ == \"__main__\":",
              "    from sklearn.datasets import load_digits",
              "    digits = load_digits()",
              "",
              "    # MNIST1画像",
              "    x = digits.data[0].reshape(digits.data[0].shape[0], 1)",
              "    print(x.shape)  # (64, 1)",
              "    umap_tsne_scatter(x, out_png=\"tmp/umap_scatter_mnist.png\")",
              "",
              "    # MNIST全画像",
              "    x = digits.data",
              "    print(x.shape)  # (1797, 64)",
              "    umap_tsne_scatter(x, y=digits.target, out_png=\"tmp/umap_scatter_mnist_all.png\")",
            ]
          },
          {
            'name': 'pca+umap_scatter',
            'snippet': [
              "import os",
              "import numpy as np",
              "import pandas as pd",
              "import matplotlib.pyplot as plt",
              "from sklearn.decomposition import PCA",
              "from umap import UMAP",
              "",
              "",
              "def plot_pca_cumulative_contribution_rate(pca):",
              "    \"\"\"各主成分の寄与率（元データの情報が何%含まれているか）を算出しplot",
              "    累積寄与率80%の主成分までを使うのが良いよう\"\"\"",
              "    plt.figure(figsize=(8, 6))",
              "    plt.plot(np.cumsum(pca.explained_variance_ratio_), '-o')",
              "    plt.xlabel(\"principal components\")",
              "    plt.ylabel(\"cumulative contribution rate\")",
              "    plt.grid()",
              "    plt.show()",
              "    plt.clf()",
              "    plt.close()",
              "",
              "    ",
              "def plot_pca_umap(x: np.ndarray, y: np.ndarray, colormap=plt.cm.Paired, pca_n_comp=None, umap_n_comp=2):",
              "    '''PCAで次元削減してからUMAP実行してplot",
              "    高次元データの場合pcaで次元減らしたほうが処理早いし綺麗にクラスタ分割できるらしい",
              "    https://cpp-learning.com/pca-umap/#8220PCA_UMAP8221",
              "    '''",
              "    pca_n_comp = x.shape[1] if pca_n_comp is None else pca_n_comp",
              "    ",
              "    # pca + umap",
              "    pca = PCA(n_components=pca_n_comp, random_state=0)",
              "    umap = UMAP(n_components=umap_n_comp, random_state=0, n_neighbors=5)",
              "    pca_x = pca.fit_transform(x)",
              "    x_embedded = umap.fit_transform(pca_x)",
              "    ",
              "    # pcaの寄与度plot",
              "    plot_pca_cumulative_contribution_rate(pca)",
              "    ",
              "    plt.figure(figsize=(8, 6))",
              "    plt.scatter(x_embedded[:, 0], x_embedded[:, 1], c=y, cmap=colormap)",
              "    plt.colorbar()",
              "    plt.title(f\"Embedding Space with PCA n={pca_n_comp} and UMAP\")",
              "    plt.show()",
              "    plt.clf()",
              "    plt.close()",
              "    ",
              "    ",
              "# MNIST",
              "from sklearn.datasets import load_digits",
              "digits = load_digits()",
              "X = digits.data",
              "y = digits.target",
              "",
              "# Visualize features",
              "plot_pca_umap(X, y)  # PCAなしでUMAPのみ",
              "plot_pca_umap(X, y, pca_n_comp=20, umap_n_comp=2)  # PCAでXを20次元に削減してからUMAP",
            ]
          },
        ],
      },
      {
        'name': 'fbprophet',
        'sub-menu': [
          {
            'name': 'Prophet_sample',
            'snippet': [
              "import numpy as np",
              "import pandas as pd",
              "",
              "",
              "def sample_prophet(df, periods=48, output_dir=\"tmp\", is_Agg=True):",
              "    \"\"\"",
              "    prophetで時系列データを予測する",
              "        prophet使い方参考: https://qiita.com/japanesebonobo/items/6f3234adb592f73bafbf",
              "    Args:",
              "        df: Prophetモデルの学習データ。ds, yのカラムのみを持つデータフレーム。dsはdatestamp型、yは数値型",
              "        periods: 予測する期間。デフォルトは日数。単位はmodel.make_future_dataframe(*, freq=*)のfreqに依存する。freq=\"m\"なら月数予測",
              "        output_dir: 予測結果のプロット画像出力ディレクトリ",
              "        is_Agg: matplotlib.use(\"Agg\")にするか",
              "    Return:",
              "        forecast: 予測値列追加したデータフレーム",
              "    \"\"\"",
              "    import os",
              "    import matplotlib.pyplot as plt",
              "    from fbprophet import Prophet",
              "    from fbprophet.plot import add_changepoints_to_plot",
              "",
              "    if is_Agg:",
              "        import matplotlib",
              "",
              "        matplotlib.use(\"Agg\")",
              "",
              "    os.makedirs(output_dir, exist_ok=True)",
              "",
              "    for col in df.columns:",
              "        assert col in [\"ds\", \"y\"], \"ds, yのカラムを持つデータフレームにしてください。dsはdatestamp型、yは数値型\"",
              "",
              "    # 飽和値がある場合、cap, floorで指定できる",
              "    # df['cap'] = 6  # 最大値",
              "    df[\"floor\"] = 0  # 最小値",
              "",
              "    \"\"\" 学習",
              "    Prophet()の引数でトレンドの変換点や休日なども設定できる。以下に使いそうな引数のメモを残す。詳細は help(Prophet) で確認",
              "        - growth=\"linear\": 線形モデル（右肩上がり直線）で予測。非線形にする場合は Prophet(growth='logistic')。",
              "        - changepoint_range=0.9: 時系列データの前から90%のデータを用いて、潜在的な変化点（トレンドの変換点）を推測。デフォルトは0.8",
              "        - changepoint_prior_scale=0.05: トレンドの柔軟性を弱める（＝過学習を抑える）や柔軟性が足りてない（=学習しきれていない）のを調整する。",
              "            デフォルトは0.05。この値を増加させると、トレンドはさらに柔軟になる（=よりデータに適合させる）",
              "        - changepoints: トレンドの変換点指定。changepoints=['2014-01-01']とすると、2014-01-01が転換点として扱われる",
              "        - holiday: 祭日や特別なイベントをデータフレームで指定できる。具体例は https://qiita.com/japanesebonobo/items/e674359618a879a49c05 がわかりやすい",
              "        - yearly_seasonality: フーリエ級数を使うことで周期性を推定するパラメータ。デフォルトは10。",
              "            四季ごとの周期性にしたいなら、yearly_seasonality=4 にする",
              "            これは年単位の周期性だが、weekly_seasonality、daily_seasonalityでも指定可能",
              "            フーリエ級数のパラメータを増加させるとより短いサイクルで変化する周期性に対応できるが、過学習になりやすい",
              "            周期性をモデリングする変数2N個に対して、N個のフーリエ級数の項数を指定することをおすすめします",
              "        - seasonality_mode='multiplicative': 増加し続ける周期性の場合は'multiplicative'を指定する。",
              "            デフォルトは'additive'でただの周期性になる",
              "        - interval_width: 予測の誤差の範囲の広さ(デフォルト設定では80%信頼区間)。interval_width=0.95なら95%信頼区間？",
              "        - 外れ値についてはyの値をNone(欠損)のレコードにすれば影響受けずにモデリングしてくれる。",
              "            外れ値が多い場合は欠損にする処理必須",
              "    \"\"\"",
              "    model = Prophet(",
              "        growth=\"linear\",",
              "        changepoint_range=0.9,",
              "        changepoint_prior_scale=0.05,",
              "        seasonality_mode=\"multiplicative\",",
              "        interval_width=0.95,",
              "    )",
              "    model.fit(df)",
              "",
              "    \"\"\" 予測期間指定",
              "    - 引数 periods: 予測する期間。デフォルトは日数。単位はmodel.make_future_dataframe(*, freq=*)のfreqに依存する。freq=\"m\"なら月数予測",
              "    - freq でどの時期（年、月、日、時間）を基準に周期性とるか指定できる",
              "        freq=\"M\" なら月単位、freq=\"H\" なら時間単位、の周期性にデータを当てはめる",
              "    \"\"\"",
              "    future = model.make_future_dataframe(periods=periods, freq=\"m\")",
              "",
              "    # 予測値に飽和値がある場合も、cap, floorで指定できる",
              "    # future['cap'] = 6  # 最大値",
              "    future[\"floor\"] = 0  # 最小値",
              "",
              "    # 予測値を示すyhatの列を含むデータフレーム。YHAT_LOWER:予測値の下限信頼区間、YHAT_UPPER:測値の上限信頼区間",
              "    forecast = model.predict(future)",
              "",
              "    # 予測をプロット",
              "    fig = model.plot(forecast)",
              "    a = add_changepoints_to_plot(fig.gca(), model, forecast)  # トレンドの変換点も点線で描画",
              "    fig.savefig(f\"{output_dir}/prophet_forecast.png\")",
              "    fig.show()",
              "    print(f\"[INFO] save plot: {output_dir}/prophet_forecast.png\")",
              "",
              "    # 予測の構成要素をプロット。トレンドに加え、時系列データの年・週単位の周期性のグラフ",
              "    model.plot_components(forecast).savefig(",
              "        f\"{output_dir}/prophet_forecast_components.png\"",
              "    )",
              "    print(f\"[INFO] save plot: {output_dir}/prophet_forecast_components.png\")",
              "",
              "    # 最小値のみ0に収める",
              "    forecast[\"yhat\"] = np.clip(forecast[\"yhat\"], 0, None)",
              "",
              "    return forecast[[\"ds\", \"yhat\"]]",
              "",
              "",
              "if __name__ == \"__main__\":",
              "    # 飛行機の利用者数データ",
              "    df = pd.read_csv(",
              "        \"https://raw.githubusercontent.com/facebook/prophet/master/examples/example_air_passengers.csv\"",
              "    )",
              "    # df[\"y\"] = np.log(df[\"y\"] + 1)  # 対数化する場合",
              "    display(df.head(3))",
              "    display(df.tail(3))",
              "",
              "    # 予測",
              "    forecast = sample_prophet(df, is_Agg=False)",
              "    display(forecast.head(3))",
              "    display(forecast.tail(3))",
            ]
          },
          {
            'name': 'Prophet_Bike_Sharing',
            'snippet': [
              "import numpy as np",
              "import pandas as pd",
              "import matplotlib.pyplot as plt",
              "",
              "",
              "# Bike Sharingのデータ(https://www.kaggle.com/c/bikesharing-for-education)でトレンド予測",
              "def estimate_trend_and_seasonality(df, col):",
              "    from fbprophet import Prophet",
              "    from fbprophet.plot import add_changepoints_to_plot",
              "",
              "    holidays = pd.DataFrame(",
              "        {",
              "            \"holiday\": \"holiday\",",
              "            \"ds\": df[df[\"holiday\"] == 1]",
              "            .groupby(pd.Grouper(freq=\"D\"))",
              "            .head(1)",
              "            .index.tolist(),",
              "            \"lower_window\": 0,",
              "            \"upper_window\": 1,",
              "        }",
              "    )",
              "",
              "    pfdf = (",
              "        df[col]",
              "        .resample(\"H\")",
              "        .asfreq()",
              "        .reset_index()",
              "        .rename({\"datetime\": \"ds\", col: \"y\"}, axis=1)",
              "    )",
              "    m = Prophet(",
              "        holidays=holidays,",
              "        changepoint_prior_scale=0.001,",
              "        seasonality_mode=\"multiplicative\",",
              "        interval_width=0.95,",
              "        yearly_seasonality=4,",
              "    )",
              "    m.fit(pfdf)",
              "",
              "    forecast = m.predict(pfdf)",
              "",
              "    print(\"timeseries components\")",
              "    m.plot_components(forecast)",
              "",
              "    print(\"predict\")",
              "    fig = m.plot(forecast)",
              "    ax = fig.gca()",
              "    a = add_changepoints_to_plot(ax, m, forecast)",
              "    fig.show()",
              "",
              "    df[\"trend_\" + col] = np.clip(forecast.set_index(\"ds\")[\"yhat\"], 0, None)",
              "",
              "    return df",
              "",
              "",
              "if __name__ == \"__main__\":",
              "    csv = r\"C:\\Users\\yokoi.shingo\\my_task\\Bike_Shareing\\data\\orig_InClass\\bikesharing-for-education\\train.csv\"",
              "    df = pd.read_csv(csv, parse_dates=[\"datetime\"])",
              "    display(df.head(3))",
              "",
              "    df = df.set_index(\"datetime\", drop=False)",
              "    ",
              "    ########### 時系列モデルのトレンド特徴量追加 ###########",
              "    # ---------- casualについて ---------- #",
              "    y_col = \"casual\"",
              "    df = estimate_trend_and_seasonality(df, y_col)",
              "    ",
              "    df[\"casual\"].plot(figsize=(80, 5))",
              "    plt.show()",
              "    df[\"trend_casual\"].plot(figsize=(80, 5))",
              "    plt.show()",
              "    ",
              "    # ---------- registeredについて ----------- #",
              "    y_col = \"registered\"",
              "    df = estimate_trend_and_seasonality(df, y_col)",
              "    ",
              "    df[\"registered\"].plot(figsize=(80, 5))",
              "    plt.show()",
              "    df[\"trend_registered\"].plot(figsize=(80, 5))",
              "    plt.show()",
              "    ##################################################",
              "    ",
              "    ################ 残差の特徴量も追加 ################",
              "    # ---------- casualについて ---------- #",
              "    df[\"residual_casual\"] = df[\"casual\"] - df[\"trend_casual\"]",
              "    df[\"residual_casual\"].plot(figsize=(80, 5))",
              "    plt.show()",
              "    ",
              "    # ---------- registeredについて ----------- #",
              "    df[\"residual_registered\"] = df[\"registered\"] - df[\"trend_registered\"]",
              "    df[\"residual_registered\"].plot(figsize=(80, 5))",
              "    plt.show()",
              "    #################################################",
            ]
          },
          {
            'name': 'Prophet_coronavirus_trend',
            'snippet': [
              "import os",
              "import numpy as np",
              "import pandas as pd",
              "import matplotlib.pyplot as plt",
              "from fbprophet import Prophet",
              "from fbprophet.plot import add_changepoints_to_plot",
              "",
              "pd.set_option(\"display.max_rows\", 500)",
              "",
              "",
              "def load_data():",
              "    # webから最新の情報取ってくる",
              "    data = pd.read_csv(",
              "        \"https://stopcovid19.metro.tokyo.lg.jp/data/130001_tokyo_covid19_patients.csv\",",
              "        parse_dates=[\"公表_年月日\"],",
              "        index_col=\"公表_年月日\",",
              "    )",
              "    return data",
              "",
              "",
              "def region_cross_data(df):",
              "    # 公表_年月日、患者_居住地でクロス集計",
              "    df_cross = pd.crosstab(df.index, df[\"患者_居住地\"])",
              "    # 日本語表記除去",
              "    df_cross = df_cross.rename(",
              "        columns={",
              "            \"湖北省武漢市\": \"Wuhan, Hubei Province\",",
              "            \"都内\": \"Inside Tokyo\",",
              "            \"湖南省長沙市\": \"Changsha City, Hunan Province\",",
              "            \"都外\": \"Outside Tokyo\",",
              "            \"調査中\": \"Under investigation\",",
              "            \"―\": \"-\",",
              "        }",
              "    )",
              "    df_cross.index.names = [\"Date\"]",
              "    df_cross.columns.name = \"Region\"",
              "    sort_cols = sorted(df_cross.columns.to_list())",
              "    df_cross = df_cross[sort_cols]",
              "    return df_cross  # .rolling(7).mean()",
              "",
              "",
              "def plot_model_changepoints(model, pfdf):",
              "    \"\"\"",
              "    変化点候補の変化量を可視化",
              "    ※Prophet(changepoint_prior_scale)を指定しておかないとエラーになる",
              "    https://data.gunosy.io/entry/change-point-detection-prophet",
              "    \"\"\"",
              "    # 変化点候補の変化量を可視化",
              "    # https://data.gunosy.io/entry/change-point-detection-prophet",
              "",
              "    import seaborn as sns",
              "",
              "    # add change rates to changepoints",
              "    df_changepoints = pfdf.loc[model.changepoints.index]",
              "    df_changepoints[\"delta\"] = model.params[\"delta\"].ravel()",
              "    #display(df_changepoints.sort_values(by=\"ds\"))",
              "",
              "    # get changepoints",
              "    df_changepoints[\"ds\"] = df_changepoints[\"ds\"].astype(str)",
              "    df_changepoints[\"delta\"] = df_changepoints[\"delta\"]  # .round(2)",
              "    df_selection = df_changepoints[df_changepoints[\"delta\"] != 0]",
              "    date_changepoints = (",
              "        df_selection[\"ds\"].astype(\"datetime64[ns]\").reset_index(drop=True)",
              "    )",
              "",
              "    # plot",
              "    sns.set(style=\"whitegrid\")",
              "    ax = sns.factorplot(",
              "        x=\"ds\",",
              "        y=\"delta\",",
              "        data=df_changepoints,",
              "        kind=\"bar\",",
              "        color=\"royalblue\",",
              "        size=4,",
              "        aspect=2,",
              "    )",
              "    ax.set_xticklabels(rotation=90)",
              "",
              "",
              "def fit_prophet(pfdf):",
              "    pfdf[\"cap\"] = np.max(pfdf[\"y\"])  # 最大値",
              "    # display(pfdf)",
              "",
              "    display(pfdf.sort_values(by=\"y\", ascending=False).head())",
              "",
              "    model = Prophet(",
              "        # growth=\"linear\",",
              "        growth=\"logistic\",",
              "        changepoint_range=0.8,",
              "        changepoint_prior_scale=0.9,",
              "        # seasonality_mode=\"multiplicative\",",
              "        interval_width=0.95,",
              "        # changepoints=[\"2020-03-25\", \"2020-04-10\", \"2020-05-03\", \"2020-07-02\", \"2020-08-07\",]  # 変曲点指定",
              "    )",
              "    model.fit(pfdf)",
              "",
              "    future = model.make_future_dataframe(periods=10, freq=\"d\")",
              "    future[\"cap\"] = np.max(pfdf[\"y\"])  # 最大値",
              "    forecast = model.predict(future)",
              "",
              "    fig = model.plot(forecast)",
              "    a = add_changepoints_to_plot(fig.gca(), model, forecast)  # トレンドの変換点も点線で描画",
              "    fig.show()",
              "",
              "    # 予測の構成要素をプロット。トレンドに加え、時系列データの年・週単位の周期性のグラフ",
              "    model.plot_components(forecast)",
              "    plt.show()",
              "",
              "    return model, forecast",
              "",
              "",
              "if __name__ == \"__main__\":",
              "    # 日ごとの東京のコロナ感染者数",
              "    df = load_data()",
              "    df = region_cross_data(df)",
              "    display(df.head(2))",
              "",
              "    col = \"Inside Tokyo\"",
              "    df[col].plot(figsize=(12, 6), title=col)",
              "    plt.show()",
              "    ",
              "    # 列準備",
              "    pfdf = (",
              "        df[col]",
              "        .resample(\"D\")  # 日ごとに集計",
              "        .asfreq()",
              "        .reset_index()",
              "        .rename({\"Date\": \"ds\", col: \"y\"}, axis=1)",
              "    )",
              "    ",
              "    # 予測",
              "    model, forecast = fit_prophet(pfdf)",
              "    display(forecast[[\"ds\", \"yhat\"]].tail())",
              "",
              "    # 変曲点可視化（なんか変）",
              "    plot_model_changepoints(model, pfdf)",
            ]
          },
        ]
      },
      {
        'name': 'shap',
        'sub-menu': [
          {
            'name': 'lightGBM+shap',
            'snippet': [
              "import lightgbm as lgb",
              "import seaborn as sns",
              "from sklearn.model_selection import train_test_split",
              "from sklearn.metrics import roc_auc_score",
              "",
              "def save_plot_importance(",
              "    model_path, png_path=None, is_Agg=True, height=0.5, figsize=(8, 20),",
              "):",
              "    \"\"\"        ",
              "    lgbのモデルファイルからモデルロードしてfeature importance plot",
              "    ※lgbのデフォルトのfeature importanceの出し方はsplit(その特徴量が決定木の分岐に現れた回数)",
              "    \"\"\"",
              "    import matplotlib.pyplot as plt",
              "",
              "    if isinstance(model_path, str):",
              "        model = Util.load(model_path)",
              "    else:",
              "        model = model_path",
              "",
              "    if is_Agg:",
              "        import matplotlib",
              "",
              "        matplotlib.use(\"Agg\")",
              "",
              "    print(\"feature importance plot:\", model)",
              "    lgb.plot_importance(model, height=height, figsize=figsize)",
              "    if png_path is not None:",
              "        plt.savefig(",
              "            png_path, bbox_inches=\"tight\", pad_inches=0,",
              "        )  # bbox_inchesなどは余白削除オプション",
              "    plt.show()",
              "    plt.clf()",
              "    plt.close()",
              "",
              "if __name__ == '__main__':",
              "    df = sns.load_dataset(\"titanic\")",
              "    df = df.drop([\"sex\", \"embarked\", \"who\", \"embark_town\", \"alive\"], axis=1)",
              "",
              "    #y_col = \"survived\"",
              "    y_col = \"age\"",
              "    drop_cols = [y_col]  # 取り除きたいカラムのリスト",
              "    cols = [c for c in df.columns if c not in drop_cols]",
              "    df_x = df[cols]",
              "    X_train, X_test, y_train, y_test = train_test_split(df_x, df[y_col], random_state=42)",
              "",
              "    params = {",
              "        \"boosting_type\": \"gbdt\",",
              "        #\"objective\": \"binary\",",
              "        #\"metric\": \"binary_logloss\",",
              "        \"objective\": \"regression\",",
              "        \"metric\": \"mae\",",
              "        \"n_jobs\": -1,",
              "        \"seed\": 236,",
              "    }",
              "",
              "    # lightGBM 用のデータに変形",
              "    lgb_train = lgb.Dataset(X_train, y_train)",
              "    lgb_val = lgb.Dataset(X_test, y_test)",
              "",
              "    # モデルの作成",
              "    model_lgb = lgb.train(",
              "        params,",
              "        lgb_train,",
              "        num_boost_round=1000,",
              "        early_stopping_rounds=30,",
              "        valid_sets=[lgb_train, lgb_val],",
              "        verbose_eval=100,",
              "    )",
              "    #pred = model_lgb.predict(X_test)",
              "    #score = roc_auc_score(y_test, pred)",
              "    #print(f\"\\ntest set AUC: {round(score, 3)}\")",
              "",
              "    # feature importance",
              "    save_plot_importance(model_lgb, \"tmp/importance.png\", is_Agg=False)",
              "",
              "",
              "# shapで特徴量確認",
              "# Shap値は予測した値に対して、「それぞれの特徴変数がその予想にどのような影響を与えたか」を算出するもの",
              "# https://qiita.com/shin_mura/items/cde01198552eda9146b7",
              "# http://www.ie110704.net/2019/04/26/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92%E8%A7%A3%E9%87%88%E3%81%99%E3%82%8B%E6%8C%87%E6%A8%99shap%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/",
              "# https://takaherox.hatenablog.com/entry/2020/07/06/234337",
              "",
              "import shap",
              "shap.initjs()  # shap.force_plotはこれないとエラーになる",
              "",
              "#X = X_test",
              "#y = y_test",
              "X = X_train",
              "y = y_train",
              "",
              "# 木の数が多いとめっちゃ時間かかるので注意(n_estimater=10000なら12時間たっても終わらなかった)",
              "# 学習させたmodel渡す",
              "explainer = shap.TreeExplainer(model=model_lgb)",
              "",
              "# SHAP Valueの計算 ",
              "shap_values = explainer.shap_values(X)",
              "",
              "# 1件について",
              "print(y[0])",
              "display(shap.force_plot(explainer.expected_value, shap_values[0,:], X.iloc[0,:]))  # , X.iloc[0,:] は特徴量のカラム名渡すために書いてる",
              "",
              "# SHAPの絶対値で特徴量軸を比較してプロット",
              "shap.summary_plot(shap_values, X, plot_type=\"bar\")",
              "",
              "# Violin Plotの可視化 どの特徴量がプラス、マイナスに働いたかを確認",
              "# 色がくっきり分かれていて左右に大きく散らばっているなら目的変数と強い相関がある説明変数",
              "# 青が左側、赤が右側なら正の相関（ex.気温が高いほど目的変数の値高くなる）",
              "# 赤が左側、青が右側なら府の相関(ex.pclassが下がるほど目的変数の値高くなる)",
              "shap.summary_plot(shap_values, X)",
            ]
          },
        ]
      },
      {
        'name': 'generator',
        'sub-menu': [
          {
            'name': 'ファイル名返すgenerator',
            'snippet': [
              "def gen_file_name(dir_path):",
              "    \"\"\"ディレクトリ内のファイル名返すgenerator\"\"\"",
              "    import glob",
              "    import pathlib",
              "    for p in glob.glob(f\"{dir_path}/*\"):",
              "        yield pathlib.Path(p).stem",
              "        ",
              "gen = gen_file_name(r\"C:\\Users\\81908\\jupyter_notebook\\tf_2_work\")",
              "print(next(gen))",
              "print(list(gen))  # listに変換",
              "",
              "def lap_gen(gen1, gen2):",
              "    \"\"\"入れ子のgenerator\"\"\"",
              "    yield from gen1",
              "    yield from gen2",
              "    ",
              "gen1 = gen_file_name(r\"C:\\Users\\81908\\jupyter_notebook\")",
              "gen2 = gen_file_name(r\"C:\\Users\\81908\\jupyter_notebook\\tf_2_work\")",
              "for x in tmp(gen1, gen2):",
              "    print(x)",
            ]
          },
        ]
      },
      {
        'name': 'fuzzywuzzy',
        'sub-menu': [
          {
            'name': '指定列の文字の表記揺れを置換',
            'snippet': [
              "def replace_matches_in_column(df, column, string_to_match, min_ratio=90):",
              "    \"\"\"指定列の文字の表記揺れを置換する",
              "    指定列(column)の各文字の距離をfuzzywuzzyで測り、min_ratioより大きければstring_to_matchで置換する ",
              "    「south korea」や「southkorea 」など意味が同じ表記揺れを「southkorea」に統一したいときに使える",
              "    https://www.kaggle.com/alexisbcook/inconsistent-data-entry\"\"\"",
              "    # conda install -c conda-forge fuzzywuzzy",
              "    # fuzzywuzzyはファジーな文字列マッチングツールで、比較対象の文字列がどのくらい似ているかをレーベンシュタイン距離によって計算",
              "    import fuzzywuzzy",
              "    from fuzzywuzzy import process",
              "    ",
              "    # 一意な文字列のリストを取得",
              "    strings = df[column].unique()",
              "    ",
              "    # 入力文字列に最も近いマッチするトップ10を取得",
              "    matches = fuzzywuzzy.process.extract(string_to_match, strings, ",
              "                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)",
              "",
              "    # マッチするのは比率がmin_ratio以上の場合のみ置換",
              "    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]",
              "    print(\"close_matches:\", close_matches)",
              "",
              "    # データフレーム内のすべてのマッチした行を取得",
              "    rows_with_matches = df[column].isin(close_matches)",
              "",
              "    # 入力されたマッチに近いマッチを持つすべての行を置換",
              "    df.loc[rows_with_matches, column] = string_to_match",
              "    ",
              "    return df",
              "    ",
              "replace_matches_in_column(df=df, column='sex', string_to_match=\"male\")",
            ]
          },
        ]
      },
      {
        'name': '画像系',
        'sub-menu': [
          {
            'name': '画像表示',
            'snippet': [
              "from PIL import Image",
              "def show_file_img(img_path):",
              "    \"\"\"ファイルパスから画像データを表示させる\"\"\"",
              "    im = Image.open(img_path)",
              "    im_list = np.asarray(im)",
              "    plt.imshow(im_list)",
              "    plt.show()",
            ]
          },
          {
            'name': '画像リサイズ',
            'snippet': [
              "def resize_ndarray(x, input_shape=(380, 380, 3)):",
              "    \"\"\"tensorflow.kerasでndarray型の画像を指定サイズにリサイズする\"\"\"",
              "    import tensorflow.keras",
              "    # ndarray から PIL 形式に変換する",
              "    img = tensorflow.keras.preprocessing.image.array_to_img(x)",
              "    # 指定した大きさにリサイズする",
              "    img = img.resize((input_shape[0], input_shape[1]), resample=0)",
              "    # PIL 形式から ndarray に変換する",
              "    x = tensorflow.keras.preprocessing.image.img_to_array(img)",
              "    return x"
            ]
          },
        ]
      },
      {
        'name': 'サンプル',
        'sub-menu': [
          {
            'name': 'サンプル',
            'snippet': [
            ]
          },
        ]
      },
    ],
  };
  snippets_menu.options['menus'].push(snippets_menu.default_menus[0]);
  snippets_menu.options['menus'].push(my_favorites);
  console.log('Loaded `snippets_menu` customizations from `custom.js`');
});
